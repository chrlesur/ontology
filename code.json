[
  {
    "name": "main.go",
    "content": "// cmd/ontology/main.go\n\npackage main\n\nimport \"github.com/chrlesur/Ontology/internal/ontology\"\n\n// main is the entry point of the Ontology application.\n// It calls the Run function from the ontology package.\nfunc main() {\n\tontology.Execute()\n}\n",
    "size": 244,
    "modTime": "2024-10-21T20:24:11.756592+02:00",
    "path": "cmd\\ontology\\main.go"
  },
  {
    "name": "config.yaml",
    "content": "base_uri: \"http://www.wikidata.org/entity/\"\r\nopenai_api_url: \"https://api.openai.com/v1/chat/completions\"\r\nclaude_api_url: \"https://api.anthropic.com/v1/messages\"\r\nollama_api_url: \"http://localhost:11434/api/generate\"\r\nlog_directory: \"logs\"\r\nlog_level: \"info\"\r\nmax_tokens: 2000\r\ncontext_size: 4000\r\ndefault_llm: \"claude\"\r\ndefault_model: \"claude-3-5-sonnet-20240620\"\r\ninclude_positions: true\r\ncontext_output: false\r\ncontext_words: 30\r\nstorage:\r\n  type: \"local\"  # Peut être \"local\" ou \"s3\"\r\n  local_path: \".\"  # Chemin par défaut pour le stockage local\r\n  s3:\r\n    bucket: \"CLR-PUB\"  # Nom du bucket S3\r\n    region: \"fr1\"  # Région S3\r\n    endpoint: \"https://ctsscfabf9.s3.fr1.cloud-temple.com\"  # Endpoint S3 personnalisé (optionnel, pour les systèmes compatibles S3 comme Dell ECS)\r\n    access_key_id: \"AKIA9FEADDDCB0958EBF\"  # Clé d'accès S3 (peut être laissée vide si configurée via des variables d'environnement)\r\n    secret_access_key: \"c8aF15P8DxjlZU95NenM6Bf33NuUtE1EoqaoPCvA\"  # Clé secrète S3 (peut être laissée vide si configurée via des variables d'environnement)\r\n",
    "size": 1091,
    "modTime": "2024-11-04T18:19:23.5418292+01:00",
    "path": "config.yaml"
  },
  {
    "name": "config.go",
    "content": "package config\r\n\r\nimport (\r\n    \"fmt\"\r\n    \"io/ioutil\"\r\n    \"log\"\r\n    \"os\"\r\n    \"strconv\"\r\n    \"sync\"\r\n\r\n    \"github.com/chrlesur/Ontology/internal/i18n\"\r\n    \"gopkg.in/yaml.v2\"\r\n)\r\n\r\nvar (\r\n    once     sync.Once\r\n    instance *Config\r\n)\r\n\r\n// Config structure definition\r\ntype Config struct {\r\n    BaseURI          string        `yaml:\"base_uri\"`\r\n    OpenAIAPIURL     string        `yaml:\"openai_api_url\"`\r\n    ClaudeAPIURL     string        `yaml:\"claude_api_url\"`\r\n    OllamaAPIURL     string        `yaml:\"ollama_api_url\"`\r\n    OpenAIAPIKey     string        `yaml:\"openai_api_key\"`\r\n    ClaudeAPIKey     string        `yaml:\"claude_api_key\"`\r\n    LogDirectory     string        `yaml:\"log_directory\"`\r\n    LogLevel         string        `yaml:\"log_level\"`\r\n    MaxTokens        int           `yaml:\"max_tokens\"`\r\n    ContextSize      int           `yaml:\"context_size\"`\r\n    DefaultLLM       string        `yaml:\"default_llm\"`\r\n    DefaultModel     string        `yaml:\"default_model\"`\r\n    OntologyName     string        `yaml:\"ontology_name\"`\r\n    Input            string        `yaml:\"input\"`\r\n    IncludePositions bool          `yaml:\"include_positions\"`\r\n    ContextOutput    bool          `yaml:\"context_output\"`\r\n    ContextWords     int           `yaml:\"context_words\"`\r\n    AIYOUAPIURL      string        `yaml:\"aiyou_api_url\"`\r\n    AIYOUAssistantID string        `yaml:\"aiyou_assistant_id\"`\r\n    AIYOUEmail       string        `yaml:\"aiyou_email\"`\r\n    AIYOUPassword    string        `yaml:\"aiyou_password\"`\r\n    Storage          StorageConfig `yaml:\"storage\"`\r\n}\r\n\r\n// StorageConfig contient la configuration pour le stockage\r\ntype StorageConfig struct {\r\n    Type     string  `yaml:\"type\"`\r\n    LocalPath string `yaml:\"local_path\"`\r\n    S3       S3Config `yaml:\"s3\"`\r\n}\r\n\r\n// S3Config contient la configuration spécifique à S3\r\ntype S3Config struct {\r\n    Bucket          string `yaml:\"bucket\"`\r\n    Region          string `yaml:\"region\"`\r\n    Endpoint        string `yaml:\"endpoint\"`\r\n    AccessKeyID     string `yaml:\"access_key_id\"`\r\n    SecretAccessKey string `yaml:\"secret_access_key\"`\r\n}\r\n\r\n// GetConfig returns the singleton instance of Config\r\nfunc GetConfig() *Config {\r\n    once.Do(func() {\r\n        instance = \u0026Config{\r\n            OpenAIAPIURL:     \"https://api.openai.com/v1/chat/completions\",\r\n            ClaudeAPIURL:     \"https://api.anthropic.com/v1/messages\",\r\n            OllamaAPIURL:     \"http://localhost:11434/api/generate\",\r\n            BaseURI:          \"http://www.wikidata.org/entity/\",\r\n            LogDirectory:     \"logs\",\r\n            LogLevel:         \"info\",\r\n            MaxTokens:        1000,\r\n            ContextSize:      4000,\r\n            DefaultLLM:       \"claude\",\r\n            DefaultModel:     \"claude-3-5-sonnet-20240620\",\r\n            IncludePositions: true,\r\n            ContextOutput:    false,\r\n            ContextWords:     30,\r\n            AIYOUAssistantID: \"asst_q2YbeHKeSxBzNr43KhIESkqj\",\r\n            AIYOUAPIURL:      \"https://ai.dragonflygroup.fr/api\",\r\n            Storage: StorageConfig{\r\n                Type: \"local\",\r\n                LocalPath: \".\",\r\n            },\r\n        }\r\n        instance.loadConfigFile()\r\n        instance.loadEnvVariables()\r\n    })\r\n    return instance\r\n}\r\n\r\n// loadConfigFile loads the configuration from a YAML file\r\nfunc (c *Config) loadConfigFile() {\r\n    configPath := os.Getenv(\"ONTOLOGY_CONFIG_PATH\")\r\n    if configPath == \"\" {\r\n        configPath = \"config.yaml\"\r\n    }\r\n\r\n    data, err := ioutil.ReadFile(configPath)\r\n    if err != nil {\r\n        log.Printf(i18n.GetMessage(\"ErrReadConfigFile\"), err)\r\n        return\r\n    }\r\n\r\n    err = yaml.Unmarshal(data, c)\r\n    if err != nil {\r\n        log.Printf(i18n.GetMessage(\"ErrParseConfigFile\"), err)\r\n    }\r\n    if contextOutput := os.Getenv(\"ONTOLOGY_CONTEXT_OUTPUT\"); contextOutput == \"true\" {\r\n        c.ContextOutput = true\r\n    }\r\n    if contextWords := os.Getenv(\"ONTOLOGY_CONTEXT_WORDS\"); contextWords != \"\" {\r\n        if words, err := strconv.Atoi(contextWords); err == nil {\r\n            c.ContextWords = words\r\n        }\r\n    }\r\n\tif s3AccessKey := os.Getenv(\"S3_ACCESS_KEY_ID\"); s3AccessKey != \"\" {\r\n        c.Storage.S3.AccessKeyID = s3AccessKey\r\n    }\r\n    if s3SecretKey := os.Getenv(\"S3_SECRET_ACCESS_KEY\"); s3SecretKey != \"\" {\r\n        c.Storage.S3.SecretAccessKey = s3SecretKey\r\n    }\r\n}\r\n\r\n// loadEnvVariables loads configuration from environment variables\r\nfunc (c *Config) loadEnvVariables() {\r\n    if apiKey := os.Getenv(\"OPENAI_API_KEY\"); apiKey != \"\" {\r\n        c.OpenAIAPIKey = apiKey\r\n    }\r\n    if apiKey := os.Getenv(\"CLAUDE_API_KEY\"); apiKey != \"\" {\r\n        c.ClaudeAPIKey = apiKey\r\n    }\r\n    if s3AccessKey := os.Getenv(\"S3_ACCESS_KEY_ID\"); s3AccessKey != \"\" {\r\n        c.Storage.S3.AccessKeyID = s3AccessKey\r\n    }\r\n    if s3SecretKey := os.Getenv(\"S3_SECRET_ACCESS_KEY\"); s3SecretKey != \"\" {\r\n        c.Storage.S3.SecretAccessKey = s3SecretKey\r\n    }\r\n    if s3Endpoint := os.Getenv(\"S3_ENDPOINT\"); s3Endpoint != \"\" {\r\n        c.Storage.S3.Endpoint = s3Endpoint\r\n    }\r\n    // Add more environment variables as needed\r\n}\r\n\r\n// ValidateConfig checks if the configuration is valid\r\nfunc (c *Config) ValidateConfig() error {\r\n    if c.OpenAIAPIKey == \"\" \u0026\u0026 c.ClaudeAPIKey == \"\" {\r\n        return fmt.Errorf(i18n.GetMessage(\"ErrNoAPIKeys\"))\r\n    }\r\n    if c.Storage.Type != \"local\" \u0026\u0026 c.Storage.Type != \"s3\" {\r\n        return fmt.Errorf(\"invalid storage type: %s\", c.Storage.Type)\r\n    }\r\n    if c.Storage.Type == \"s3\" {\r\n        if c.Storage.S3.Bucket == \"\" {\r\n            return fmt.Errorf(\"S3 bucket is required when using S3 storage\")\r\n        }\r\n        if c.Storage.S3.Region == \"\" {\r\n            return fmt.Errorf(\"S3 region is required when using S3 storage\")\r\n        }\r\n        // Nous ne validons pas l'endpoint ici car il peut être optionnel (utilisation de S3 standard)\r\n        // mais nous pourrions ajouter un avertissement si l'endpoint n'est pas défini\r\n        if c.Storage.S3.Endpoint == \"\" {\r\n            log.Println(\"Warning: S3 endpoint is not set. Using default S3 endpoint.\")\r\n        }\r\n    }\r\n    // Add more validation checks as needed\r\n    return nil\r\n}\r\n\r\n// Reload reloads the configuration from file and environment variables\r\nfunc (c *Config) Reload() error {\r\n    c.loadConfigFile()\r\n    c.loadEnvVariables()\r\n    return c.ValidateConfig()\r\n}",
    "size": 6370,
    "modTime": "2024-11-04T18:07:11.361585+01:00",
    "path": "internal\\config\\config.go"
  },
  {
    "name": "convert.go",
    "content": "package converter\r\n\r\nimport (\r\n\t\"bufio\"\r\n\t\"bytes\"\r\n\t\"fmt\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\n// Convert converts a document segment into QuickStatement format\r\nfunc (qsc *QuickStatementConverter) Convert(segment []byte, context string, ontology string) (string, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ConvertStarted\"))\r\n\tlog.Debug(\"Input segment:\\n%s\", string(segment))\r\n\r\n\tstatements, err := qsc.parseSegment(segment)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.GetMessage(\"FailedToParseSegment\"), err)\r\n\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"FailedToParseSegment\"), err)\r\n\t}\r\n\r\n\tlog.Debug(\"Parsed %d statements\", len(statements))\r\n\r\n\tvar tsvBuilder strings.Builder\r\n\tfor _, stmt := range statements {\r\n\t\t// Vérifier si l'objet contient des positions\r\n\t\tobjectParts := strings.Split(stmt.Object.(string), \"@\")\r\n\t\tobject := objectParts[0]\r\n\t\tpositions := \"\"\r\n\t\tif len(objectParts) \u003e 1 {\r\n\t\t\tpositions = \"@\" + objectParts[1]\r\n\t\t}\r\n\r\n\t\t// Écrire la ligne TSV avec les positions si elles existent\r\n\t\ttsvLine := fmt.Sprintf(\"%s\\t%s\\t%s%s\\n\", stmt.Subject.ID, stmt.Property.ID, object, positions)\r\n\t\ttsvBuilder.WriteString(tsvLine)\r\n\t}\r\n\r\n\tresult := tsvBuilder.String()\r\n\tlog.Debug(\"Generated TSV output:\\n%s\", result)\r\n\treturn result, nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) cleanAndNormalizeInput(input string) string {\r\n\tlines := strings.Split(input, \"\\n\")\r\n\tvar cleanedLines []string\r\n\tfor _, line := range lines {\r\n\t\ttrimmedLine := strings.TrimSpace(line)\r\n\t\tif trimmedLine != \"\" {\r\n\t\t\tcleanedLines = append(cleanedLines, trimmedLine)\r\n\t\t}\r\n\t}\r\n\treturn strings.Join(cleanedLines, \"\\n\")\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) parseSegment(segment []byte) ([]Statement, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParsingSegment\"))\r\n\tvar statements []Statement\r\n\tscanner := bufio.NewScanner(bytes.NewReader(segment))\r\n\tfor scanner.Scan() {\r\n\t\tline := scanner.Text()\r\n\t\t// Remplacer les doubles backslashes par un caractère temporaire\r\n\t\tline = strings.ReplaceAll(line, \"\\\\\\\\\", \"\\uFFFD\")\r\n\t\t// Remplacer les \\t par des tabulations réelles\r\n\t\tline = strings.ReplaceAll(line, \"\\\\t\", \"\\t\")\r\n\t\t// Restaurer les doubles backslashes\r\n\t\tline = strings.ReplaceAll(line, \"\\uFFFD\", \"\\\\\")\r\n\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tif len(parts) \u003c 3 {\r\n\t\t\tlog.Debug(\"Skipping invalid line: %s\", line)\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tstatement := Statement{\r\n\t\t\tSubject:  Entity{ID: strings.TrimSpace(parts[0])},\r\n\t\t\tProperty: Property{ID: strings.TrimSpace(parts[1])},\r\n\t\t\tObject:   strings.TrimSpace(strings.Join(parts[2:], \"\\t\")),\r\n\t\t}\r\n\t\tstatements = append(statements, statement)\r\n\t}\r\n\tif err := scanner.Err(); err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorScanningSegment\"), err)\r\n\t}\r\n\tif len(statements) == 0 {\r\n\t\treturn nil, fmt.Errorf(i18n.GetMessage(\"NoValidStatementsFound\"))\r\n\t}\r\n\treturn statements, nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) applyContextAndOntology(statements []Statement, context string, ontology string) ([]Statement, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ApplyingContextAndOntology\"))\r\n\t// This is a placeholder implementation. In a real-world scenario, this function would\r\n\t// use the context and ontology to enrich the statements.\r\n\tfor i := range statements {\r\n\t\tstatements[i].Subject.Label = fmt.Sprintf(\"%s (from context)\", statements[i].Subject.ID)\r\n\t}\r\n\treturn statements, nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) toQuickStatementTSV(statements []Statement) (string, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ConvertingToQuickStatementTSV\"))\r\n\tvar buffer bytes.Buffer\r\n\tfor _, stmt := range statements {\r\n\t\tline := fmt.Sprintf(\"%s\\t%s\\t%v\\n\", stmt.Subject.ID, stmt.Property.ID, stmt.Object)\r\n\t\t_, err := buffer.WriteString(line)\r\n\t\tif err != nil {\r\n\t\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorWritingQuickStatement\"), err)\r\n\t\t}\r\n\t}\r\n\treturn buffer.String(), nil\r\n}\r\n",
    "size": 3865,
    "modTime": "2024-10-22T10:32:48.7791638+02:00",
    "path": "internal\\converter\\convert.go"
  },
  {
    "name": "logger.go",
    "content": "package converter\r\n\r\nimport (\r\n    \"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()",
    "size": 116,
    "modTime": "2024-10-20T14:40:18.8000271+02:00",
    "path": "internal\\converter\\logger.go"
  },
  {
    "name": "parse.go",
    "content": "package converter\r\n\r\nimport (\r\n\t\"encoding/xml\"\r\n\t\"fmt\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/knakk/rdf\"\r\n)\r\n\r\n// ParseOntology parses an ontology string and returns a structured representation\r\nfunc ParseOntology(ontology string) (map[string]interface{}, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParseOntologyStarted\"))\r\n\r\n\tformat := detectOntologyFormat(ontology)\r\n\tvar result map[string]interface{}\r\n\tvar err error\r\n\r\n\tswitch format {\r\n\tcase \"QuickStatement\":\r\n\t\tresult, err = parseQuickStatementOntology(ontology)\r\n\tcase \"RDF\":\r\n\t\tresult, err = parseRDFOntology(ontology)\r\n\tcase \"OWL\":\r\n\t\tresult, err = parseOWLOntology(ontology)\r\n\tdefault:\r\n\t\treturn nil, fmt.Errorf(i18n.GetMessage(\"UnknownOntologyFormat\"))\r\n\t}\r\n\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"FailedToParseOntology\"), err)\r\n\t}\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ParseOntologyFinished\"))\r\n\treturn result, nil\r\n}\r\n\r\nfunc detectOntologyFormat(ontology string) string {\r\n\tif strings.Contains(ontology, \"Q\") \u0026\u0026 strings.Contains(ontology, \"P\") \u0026\u0026 strings.Contains(ontology, \"\\t\") {\r\n\t\treturn \"QuickStatement\"\r\n\t}\r\n\tif strings.Contains(ontology, \"\u003crdf:RDF\") {\r\n\t\treturn \"RDF\"\r\n\t}\r\n\tif strings.Contains(ontology, \"\u003cOntology\") {\r\n\t\treturn \"OWL\"\r\n\t}\r\n\treturn \"Unknown\"\r\n}\r\n\r\nfunc parseQuickStatementOntology(ontology string) (map[string]interface{}, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParseQuickStatementOntologyStarted\"))\r\n\r\n\tresult := make(map[string]interface{})\r\n\tentities := make(map[string]map[string]interface{})\r\n\r\n\tlines := strings.Split(ontology, \"\\n\")\r\n\tfor _, line := range lines {\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tif len(parts) != 3 {\r\n\t\t\treturn nil, fmt.Errorf(i18n.GetMessage(\"InvalidQuickStatementLine\"))\r\n\t\t}\r\n\r\n\t\tsubject, predicate, object := parts[0], parts[1], parts[2]\r\n\t\tif _, exists := entities[subject]; !exists {\r\n\t\t\tentities[subject] = make(map[string]interface{})\r\n\t\t}\r\n\t\tentities[subject][predicate] = object\r\n\t}\r\n\r\n\tresult[\"entities\"] = entities\r\n\tlog.Debug(i18n.GetMessage(\"ParseQuickStatementOntologyFinished\"))\r\n\treturn result, nil\r\n}\r\n\r\nfunc parseRDFOntology(ontology string) (map[string]interface{}, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParseRDFOntologyStarted\"))\r\n\r\n\tresult := make(map[string]interface{})\r\n\tentities := make(map[string]map[string]interface{})\r\n\r\n\tdec := rdf.NewTripleDecoder(strings.NewReader(ontology), rdf.RDFXML)\r\n\tfor {\r\n\t\ttriple, err := dec.Decode()\r\n\t\tif err != nil {\r\n\t\t\tif err.Error() == \"EOF\" {\r\n\t\t\t\tbreak\r\n\t\t\t}\r\n\t\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorDecodingRDF\"), err)\r\n\t\t}\r\n\r\n\t\tsubject := triple.Subj.String()\r\n\t\tpredicate := triple.Pred.String()\r\n\t\tobject := triple.Obj.String()\r\n\r\n\t\tif _, exists := entities[subject]; !exists {\r\n\t\t\tentities[subject] = make(map[string]interface{})\r\n\t\t}\r\n\t\tentities[subject][predicate] = object\r\n\t}\r\n\r\n\tresult[\"entities\"] = entities\r\n\tlog.Debug(i18n.GetMessage(\"ParseRDFOntologyFinished\"))\r\n\treturn result, nil\r\n}\r\n\r\nfunc parseOWLOntology(ontology string) (map[string]interface{}, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParseOWLOntologyStarted\"))\r\n\r\n\tresult := make(map[string]interface{})\r\n\r\n\t// Simplified OWL parsing\r\n\tvar owlData struct {\r\n\t\tXMLName xml.Name `xml:\"Ontology\"`\r\n\t\tClasses []struct {\r\n\t\t\tIRI   string `xml:\"IRI,attr\"`\r\n\t\t\tLabel string `xml:\"label,attr\"`\r\n\t\t} `xml:\"Declaration\u003eClass\"`\r\n\t}\r\n\r\n\terr := xml.Unmarshal([]byte(ontology), \u0026owlData)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorParsingOWL\"), err)\r\n\t}\r\n\r\n\tclasses := make(map[string]interface{})\r\n\tfor _, class := range owlData.Classes {\r\n\t\tclasses[class.IRI] = map[string]interface{}{\r\n\t\t\t\"label\": class.Label,\r\n\t\t}\r\n\t}\r\n\tresult[\"classes\"] = classes\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ParseOWLOntologyFinished\"))\r\n\treturn result, nil\r\n}\r\n",
    "size": 3787,
    "modTime": "2024-10-20T14:19:06.7515729+02:00",
    "path": "internal\\converter\\parse.go"
  },
  {
    "name": "quickstatement.go",
    "content": "// Package converter provides functionality for converting document segments\r\n// into QuickStatement format and other ontology representations.\r\npackage converter\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\n// Converter defines the interface for QuickStatement conversion\r\ntype Converter interface {\r\n\tConvert(segment []byte, context string, ontology string) (string, error)\r\n}\r\n\r\n// QuickStatementConverter implements the Converter interface\r\ntype QuickStatementConverter struct {\r\n\tlogger           *logger.Logger\r\n\tincludePositions bool\r\n}\r\n\r\n// NewQuickStatementConverter creates a new QuickStatementConverter\r\nfunc NewQuickStatementConverter(log *logger.Logger, includePositions bool) *QuickStatementConverter {\r\n\treturn \u0026QuickStatementConverter{\r\n\t\tlogger:           log,\r\n\t\tincludePositions: includePositions,\r\n\t}\r\n}\r\n\r\n// Entity represents a Wikibase entity\r\ntype Entity struct {\r\n\tID    string\r\n\tLabel string\r\n}\r\n\r\n// Property represents a Wikibase property\r\ntype Property struct {\r\n\tID       string\r\n\tDataType string\r\n}\r\n\r\n// Statement represents a complete QuickStatement\r\ntype Statement struct {\r\n\tSubject  Entity\r\n\tProperty Property\r\n\tObject   interface{}\r\n}\r\n",
    "size": 1196,
    "modTime": "2024-11-04T09:26:58.2842666+01:00",
    "path": "internal\\converter\\quickstatement.go"
  },
  {
    "name": "utils.go",
    "content": "package converter\r\n\r\nimport (\r\n\t\"crypto/rand\"\r\n\t\"fmt\"\r\n\t\"net/url\"\r\n\t\"strings\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\n// EscapeString escapes special characters in a string for safe use in output formats\r\nfunc EscapeString(s string) string {\r\n\treturn strings.NewReplacer(\r\n\t\t`\"`, `\\\"`,\r\n\t\t`\\`, `\\\\`,\r\n\t\t`/`, `\\/`,\r\n\t\t\"\\b\", `\\b`,\r\n\t\t\"\\f\", `\\f`,\r\n\t\t\"\\n\", `\\n`,\r\n\t\t\"\\r\", `\\r`,\r\n\t\t\"\\t\", `\\t`,\r\n\t).Replace(s)\r\n}\r\n\r\n// IsValidURI checks if a string is a valid URI\r\nfunc IsValidURI(uri string) bool {\r\n\t_, err := url.ParseRequestURI(uri)\r\n\tif err != nil {\r\n\t\tlog.Debug(i18n.GetMessage(\"InvalidURI\")) // Cette ligne est correcte car log.Debug n'attend qu'un seul argument\r\n\t\treturn false\r\n\t}\r\n\treturn true\r\n}\r\n\r\n// FormatDate formats a date string to a standard format (ISO 8601)\r\nfunc FormatDate(date string) (string, error) {\r\n\tparsedDate, err := time.Parse(time.RFC3339, date)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidDateFormat\"), err) // Ajout de l'erreur comme second argument\r\n\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorParsingDate\"), err)\r\n\t}\r\n\treturn parsedDate.Format(time.RFC3339), nil\r\n}\r\n\r\n// GenerateUniqueID generates a unique identifier for entities without one\r\nfunc GenerateUniqueID() string {\r\n\tb := make([]byte, 16)\r\n\t_, err := rand.Read(b)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.GetMessage(\"ErrorGeneratingUniqueID\"), err)\r\n\t\treturn \"\"\r\n\t}\r\n\treturn fmt.Sprintf(\"%x\", b)\r\n}\r\n\r\n// SplitIntoChunks divides a large dataset into smaller chunks for batch processing\r\nfunc SplitIntoChunks(data []byte, chunkSize int) [][]byte {\r\n\tvar chunks [][]byte\r\n\tfor i := 0; i \u003c len(data); i += chunkSize {\r\n\t\tend := i + chunkSize\r\n\t\tif end \u003e len(data) {\r\n\t\t\tend = len(data)\r\n\t\t}\r\n\t\tchunks = append(chunks, data[i:end])\r\n\t}\r\n\treturn chunks\r\n}\r\n\r\n// MergeMaps merges multiple maps into a single map\r\nfunc MergeMaps(maps ...map[string]interface{}) map[string]interface{} {\r\n\tresult := make(map[string]interface{})\r\n\tfor _, m := range maps {\r\n\t\tfor k, v := range m {\r\n\t\t\tresult[k] = v\r\n\t\t}\r\n\t}\r\n\treturn result\r\n}\r\n\r\n// TruncateString truncates a string to a specified length, adding an ellipsis if truncated\r\nfunc TruncateString(s string, maxLength int) string {\r\n\tif len(s) \u003c= maxLength {\r\n\t\treturn s\r\n\t}\r\n\treturn s[:maxLength-3] + \"...\"\r\n}\r\n\r\n// NormalizeWhitespace removes extra whitespace from a string\r\nfunc NormalizeWhitespace(s string) string {\r\n\treturn strings.Join(strings.Fields(s), \" \")\r\n}\r\n\r\n// IsNumeric checks if a string contains only numeric characters\r\nfunc IsNumeric(s string) bool {\r\n\tfor _, r := range s {\r\n\t\tif r \u003c '0' || r \u003e '9' {\r\n\t\t\treturn false\r\n\t\t}\r\n\t}\r\n\treturn true\r\n}\r\n",
    "size": 2639,
    "modTime": "2024-10-20T14:44:56.9267011+02:00",
    "path": "internal\\converter\\utils.go"
  },
  {
    "name": "validate.go",
    "content": "package converter\r\n\r\nimport (\r\n\t\"bufio\"\r\n\t\"fmt\"\r\n\t\"regexp\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\n// ValidateQuickStatement validates a QuickStatement string\r\nfunc ValidateQuickStatement(statement string) bool {\r\n\tlog.Debug(i18n.GetMessage(\"ValidateQuickStatementStarted\"))\r\n\r\n\terr := validateQuickStatementSyntax(statement)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidQuickStatementSyntax\"), err)\r\n\t\treturn false\r\n\t}\r\n\r\n\terr = checkEntityReferences(statement)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidEntityReferences\"), err)\r\n\t\treturn false\r\n\t}\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ValidateQuickStatementFinished\"))\r\n\treturn true\r\n}\r\n\r\n// ValidateRDF validates an RDF string\r\nfunc ValidateRDF(rdf string) bool {\r\n\tlog.Debug(i18n.GetMessage(\"ValidateRDFStarted\"))\r\n\r\n\terr := validateRDFSyntax(rdf)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidRDFSyntax\"), err)\r\n\t\treturn false\r\n\t}\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ValidateRDFFinished\"))\r\n\treturn true\r\n}\r\n\r\n// ValidateOWL validates an OWL string\r\nfunc ValidateOWL(owl string) bool {\r\n\tlog.Debug(i18n.GetMessage(\"ValidateOWLStarted\"))\r\n\r\n\terr := validateOWLSyntax(owl)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidOWLSyntax\"), err)\r\n\t\treturn false\r\n\t}\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ValidateOWLFinished\"))\r\n\treturn true\r\n}\r\n\r\nfunc validateQuickStatementSyntax(statement string) error {\r\n\tscanner := bufio.NewScanner(strings.NewReader(statement))\r\n\tlineNum := 0\r\n\tfor scanner.Scan() {\r\n\t\tlineNum++\r\n\t\tline := scanner.Text()\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tif len(parts) != 3 {\r\n\t\t\treturn fmt.Errorf(i18n.GetMessage(\"InvalidQuickStatementLine\"), lineNum)\r\n\t\t}\r\n\t\tif !isValidEntity(parts[0]) || !isValidProperty(parts[1]) {\r\n\t\t\treturn fmt.Errorf(i18n.GetMessage(\"InvalidEntityOrProperty\"), lineNum)\r\n\t\t}\r\n\t}\r\n\treturn scanner.Err()\r\n}\r\n\r\nfunc validateRDFSyntax(rdf string) error {\r\n\tif !strings.Contains(rdf, \"\u003crdf:RDF\") || !strings.Contains(rdf, \"\u003c/rdf:RDF\u003e\") {\r\n\t\treturn fmt.Errorf(i18n.GetMessage(\"MissingRDFTags\"))\r\n\t}\r\n\tif !strings.Contains(rdf, \"xmlns:rdf=\\\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\\\"\") {\r\n\t\treturn fmt.Errorf(i18n.GetMessage(\"MissingRDFNamespace\"))\r\n\t}\r\n\treturn nil\r\n}\r\n\r\nfunc validateOWLSyntax(owl string) error {\r\n\tif !strings.Contains(owl, \"\u003cowl:Ontology\") || !strings.Contains(owl, \"\u003c/owl:Ontology\u003e\") {\r\n\t\treturn fmt.Errorf(i18n.GetMessage(\"MissingOWLTags\"))\r\n\t}\r\n\tif !strings.Contains(owl, \"xmlns:owl=\\\"http://www.w3.org/2002/07/owl#\\\"\") {\r\n\t\treturn fmt.Errorf(i18n.GetMessage(\"MissingOWLNamespace\"))\r\n\t}\r\n\treturn nil\r\n}\r\n\r\nfunc checkEntityReferences(statement string) error {\r\n\tscanner := bufio.NewScanner(strings.NewReader(statement))\r\n\tlineNum := 0\r\n\tfor scanner.Scan() {\r\n\t\tlineNum++\r\n\t\tline := scanner.Text()\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tif !isValidEntity(parts[0]) {\r\n\t\t\treturn fmt.Errorf(i18n.GetMessage(\"InvalidEntityReference\"), parts[0], lineNum)\r\n\t\t}\r\n\t}\r\n\treturn scanner.Err()\r\n}\r\n\r\nfunc isValidEntity(entity string) bool {\r\n\treturn regexp.MustCompile(`^Q\\d+$`).MatchString(entity)\r\n}\r\n\r\nfunc isValidProperty(property string) bool {\r\n\treturn regexp.MustCompile(`^P\\d+$`).MatchString(property)\r\n}\r\n\r\nfunc isValidURI(uri string) bool {\r\n\t// This is a simplified URI validation. In a real-world scenario, you might want to use a more comprehensive validation.\r\n\treturn regexp.MustCompile(`^(http|https)://`).MatchString(uri)\r\n}\r\n",
    "size": 3409,
    "modTime": "2024-10-20T14:43:44.4003488+02:00",
    "path": "internal\\converter\\validate.go"
  },
  {
    "name": "messages.go",
    "content": "package i18n\r\n\r\nimport (\r\n\t\"reflect\"\r\n\t\"strings\"\r\n)\r\n\r\n// Messages contient tous les messages de l'application\r\nvar Messages = struct {\r\n\tRootCmdShortDesc                  string\r\n\tRootCmdLongDesc                   string\r\n\tEnrichCmdShortDesc                string\r\n\tEnrichCmdLongDesc                 string\r\n\tConfigFlagUsage                   string\r\n\tDebugFlagUsage                    string\r\n\tSilentFlagUsage                   string\r\n\tInputFlagUsage                    string\r\n\tOutputFlagUsage                   string\r\n\tFormatFlagUsage                   string\r\n\tLLMFlagUsage                      string\r\n\tLLMModelFlagUsage                 string\r\n\tPassesFlagUsage                   string\r\n\r\n\tRecursiveFlagUsage                string\r\n\tInitializingApplication           string\r\n\tStartingEnrichProcess             string\r\n\tEnrichProcessCompleted            string\r\n\tExecutingPipeline                 string\r\n\tErrorExecutingRootCmd             string\r\n\tErrorExecutingPipeline            string\r\n\tErrorCreatingPipeline             string\r\n\tErrUnsupportedModel               string\r\n\tErrAPIKeyMissing                  string\r\n\tErrTranslationFailed              string\r\n\tErrInvalidLLMType                 string\r\n\tErrContextTooLong                 string\r\n\tTranslationStarted                string\r\n\tTranslationRetry                  string\r\n\tTranslationCompleted              string\r\n\tErrCreateLogDir                   string\r\n\tErrOpenLogFile                    string\r\n\tParseStarted                      string\r\n\tParseFailed                       string\r\n\tParseCompleted                    string\r\n\tMetadataExtractionFailed          string\r\n\tPageParseFailed                   string\r\n\tTextExtractionFailed              string\r\n\tErrInvalidContent                 string\r\n\tErrTokenization                   string\r\n\tErrReadingContent                 string\r\n\tErrTokenizerInitialization        string\r\n\tErrTokenCounting                  string\r\n\tLogSegmentationStarted            string\r\n\tLogSegmentationCompleted          string\r\n\tLogContextGeneration              string\r\n\tLogMergingSegments                string\r\n\tErrReadConfigFile                 string\r\n\tErrParseConfigFile                string\r\n\tErrNoAPIKeys                      string\r\n\tStartingQuickStatementConversion  string\r\n\tQuickStatementConversionCompleted string\r\n\tStartingRDFConversion             string\r\n\tRDFConversionCompleted            string\r\n\tStartingOWLConversion             string\r\n\tOWLConversionCompleted            string\r\n\tExistingOntologyFlagUsage         string\r\n\tErrAccessInput                    string\r\n\tErrProcessingPass                 string\r\n\tErrNoInputSpecified               string\r\n\tTranslationFailed                 string\r\n\tRateLimitExceeded                 string\r\n\tStartingPipeline                  string\r\n\tStartingPass                      string\r\n\tPipelineCompleted                 string\r\n\tSegmentProcessingError            string\r\n\tErrLoadExistingOntology           string\r\n\tErrSavingResult                   string\r\n\tErrSegmentContent                 string\r\n\tIncludePositionsFlagUsage         string\r\n\tContextOutputFlagUsage            string\r\n\tContextWordsFlagUsage             string\r\n}{\r\n\tRootCmdShortDesc: \"Ontology enrichment tool\",\r\n\tRootCmdLongDesc: `Ontology is a command-line tool for enriching ontologies from various document formats.\r\nIt supports multiple input formats and can utilize different language models for analysis.`,\r\n\tEnrichCmdShortDesc: \"Enrich an ontology from input documents\",\r\n\tEnrichCmdLongDesc: `The enrich command processes input documents to create or update an ontology.\r\nIt can handle various input formats and use different language models for analysis.`,\r\n\tConfigFlagUsage:                   \"config file (default is $HOME/.ontology.yaml)\",\r\n\tDebugFlagUsage:                    \"enable debug mode\",\r\n\tSilentFlagUsage:                   \"silent mode, only show errors\",\r\n\tInputFlagUsage:                    \"input file or directory\",\r\n\tOutputFlagUsage:                   \"output file for the enriched ontology\",\r\n\tFormatFlagUsage:                   \"input format (auto-detected if not specified)\",\r\n\tLLMFlagUsage:                      \"language model to use for analysis\",\r\n\tLLMModelFlagUsage:                 \"specific model for the chosen LLM\",\r\n\tPassesFlagUsage:                   \"number of passes for ontology enrichment\",\r\n\tRecursiveFlagUsage:                \"process input directory recursively\",\r\n\tInitializingApplication:           \"Initializing Ontology application\",\r\n\tStartingEnrichProcess:             \"Starting ontology enrichment process\",\r\n\tEnrichProcessCompleted:            \"Ontology enrichment process completed\",\r\n\tExecutingPipeline:                 \"Executing ontology enrichment pipeline\",\r\n\tErrorExecutingRootCmd:             \"Error executing root command\",\r\n\tErrorExecutingPipeline:            \"Error executing pipeline\",\r\n\tErrorCreatingPipeline:             \"Error creating pipeline\",\r\n\tErrUnsupportedModel:               \"unsupported model\",\r\n\tErrAPIKeyMissing:                  \"API key is missing\",\r\n\tErrTranslationFailed:              \"translation failed\",\r\n\tErrInvalidLLMType:                 \"invalid LLM type\",\r\n\tErrContextTooLong:                 \"context is too long\",\r\n\tTranslationStarted:                \"Translation started\",\r\n\tTranslationRetry:                  \"Translation retry\",\r\n\tTranslationCompleted:              \"Translation completed\",\r\n\tErrCreateLogDir:                   \"Failed to create log directory\",\r\n\tErrOpenLogFile:                    \"Failed to open log file\",\r\n\tParseStarted:                      \"Parsing started\",\r\n\tParseFailed:                       \"Parsing failed\",\r\n\tParseCompleted:                    \"Parsing completed\",\r\n\tMetadataExtractionFailed:          \"Metadata extraction failed\",\r\n\tPageParseFailed:                   \"Failed to parse page\",\r\n\tTextExtractionFailed:              \"Failed to extract text from page\",\r\n\tErrInvalidContent:                 \"invalid content\",\r\n\tErrTokenization:                   \"tokenization error\",\r\n\tErrReadingContent:                 \"error reading content\",\r\n\tErrTokenizerInitialization:        \"error initializing tokenizer\",\r\n\tErrTokenCounting:                  \"error counting tokens\",\r\n\tLogSegmentationStarted:            \"Segmentation started\",\r\n\tLogSegmentationCompleted:          \"Segmentation completed: %d segments\",\r\n\tLogContextGeneration:              \"Generating context\",\r\n\tLogMergingSegments:                \"Merging segments\",\r\n\tErrReadConfigFile:                 \"Failed to read config file: %v\",\r\n\tErrParseConfigFile:                \"Failed to parse config file: %v\",\r\n\tErrNoAPIKeys:                      \"No API keys provided for any LLM service\",\r\n\tStartingQuickStatementConversion:  \"Starting conversion to QuickStatement format\",\r\n\tQuickStatementConversionCompleted: \"QuickStatement conversion completed\",\r\n\tStartingRDFConversion:             \"Starting conversion to RDF format\",\r\n\tRDFConversionCompleted:            \"RDF conversion completed\",\r\n\tStartingOWLConversion:             \"Starting conversion to OWL format\",\r\n\tOWLConversionCompleted:            \"OWL conversion completed\",\r\n\tExistingOntologyFlagUsage:         \"path to an existing ontology file to enrich\",\r\n\tErrAccessInput:                    \"Failed to access input: %v\",\r\n\tErrProcessingPass:                 \"Error processing pass: %v\",\r\n\tErrNoInputSpecified:               \"No input specified. Please use --input flag to specify an input file or directory.\",\r\n\tTranslationFailed:                 \"Translation failed after maximum retries: %v\",\r\n\tRateLimitExceeded:                 \"Rate limit exceeded. Waiting %v before retrying.\",\r\n\tStartingPipeline:                  \"Starting pipeline execution\",\r\n\tStartingPass:                      \"Starting pass %d\",\r\n\tPipelineCompleted:                 \"Pipeline execution completed successfully\",\r\n\tSegmentProcessingError:            \"Error processing segment %d: %v\",\r\n\tErrLoadExistingOntology:           \"Failed to load existing ontology\",\r\n\tErrSavingResult:                   \"Error saving result\",\r\n\tErrSegmentContent:                 \"Failed to segment content\",\r\n\tIncludePositionsFlagUsage:         \"Active to not include position information in the ontology\",\r\n\tContextOutputFlagUsage:            \"Enable context output in JSON format\",\r\n\tContextWordsFlagUsage:             \"Number of context words before and after each position\",\r\n}\r\n\r\n// GetMessage retourne le message correspondant à la clé donnée\r\nfunc GetMessage(key string) string {\r\n\tv := reflect.ValueOf(Messages)\r\n\tf := v.FieldByName(key)\r\n\tif !f.IsValid() {\r\n\t\treturn key\r\n\t}\r\n\treturn strings.TrimSpace(f.String())\r\n}\r\n",
    "size": 8667,
    "modTime": "2024-11-04T09:27:51.1177263+01:00",
    "path": "internal\\i18n\\messages.go"
  },
  {
    "name": "aiyou.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"net/http\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n)\r\n\r\nconst AIYOUAPIURL = \"https://ai.dragonflygroup.fr/api\"\r\n\r\ntype APICaller interface {\r\n\tCall(endpoint, method string, data interface{}, response interface{}) error\r\n\tSetToken(token string)\r\n}\r\n\r\ntype AIYOUClient struct {\r\n\tapiCaller   APICaller\r\n\tAssistantID string\r\n\tTimeout     time.Duration\r\n\tconfig      *config.Config\r\n\tlogger      *logger.Logger\r\n}\r\n\r\ntype HTTPAPICaller struct {\r\n\tbaseURL    string\r\n\thttpClient *http.Client\r\n\ttoken      string\r\n}\r\n\r\ntype Run struct {\r\n\tID       string `json:\"id\"`\r\n\tStatus   string `json:\"status\"`\r\n\tResponse string `json:\"response\"`\r\n}\r\n\r\nfunc NewAIYOUClient(assistantID string, email string, password string) (*AIYOUClient, error) {\r\n\tcfg := config.GetConfig()\r\n\tlog := logger.GetLogger()\r\n\r\n\tif assistantID == \"\" {\r\n\t\treturn nil, fmt.Errorf(\"AI.YOU assistant ID is required\")\r\n\t}\r\n\r\n\tclient := \u0026AIYOUClient{\r\n\t\tapiCaller:   NewHTTPAPICaller(AIYOUAPIURL),\r\n\t\tAssistantID: assistantID,\r\n\t\tTimeout:     120 * time.Second,\r\n\t\tconfig:      cfg,\r\n\t\tlogger:      log,\r\n\t}\r\n\r\n\terr := client.Login(email, password)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to login to AI.YOU: %w\", err)\r\n\t}\r\n\r\n\treturn client, nil\r\n}\r\n\r\nfunc (c *AIYOUClient) Login(email, password string) error {\r\n\tc.logger.Info(\"Attempting to log in to AI.YOU\")\r\n\tloginData := map[string]string{\r\n\t\t\"email\":    email,\r\n\t\t\"password\": password,\r\n\t}\r\n\r\n\tvar loginResp struct {\r\n\t\tToken     string `json:\"token\"`\r\n\t\tExpiresAt string `json:\"expires_at\"`\r\n\t}\r\n\r\n\terr := c.apiCaller.Call(\"/login\", \"POST\", loginData, \u0026loginResp)\r\n\tif err != nil {\r\n\t\tc.logger.Error(fmt.Sprintf(\"Login failed: %v\", err))\r\n\t\treturn fmt.Errorf(\"login failed: %w\", err)\r\n\t}\r\n\r\n\tc.apiCaller.SetToken(loginResp.Token)\r\n\tc.logger.Info(\"Successfully logged in to AI.YOU\")\r\n\treturn nil\r\n}\r\n\r\nfunc (c *AIYOUClient) Translate(prompt string, context string) (string, error) {\r\n\tc.logger.Debug(\"Starting AI.YOU translation\")\r\n\r\n\tthreadID, err := c.CreateThread()\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"failed to create thread: %w\", err)\r\n\t}\r\n\r\n\tfullPrompt := fmt.Sprintf(\"%s\\n\\nContext: %s\", prompt, context)\r\n\tresponse, err := c.ChatInThread(threadID, fullPrompt)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"error during chat: %w\", err)\r\n\t}\r\n\r\n\tc.logger.Debug(\"AI.YOU translation completed\")\r\n\treturn response, nil\r\n}\r\n\r\nfunc (c *AIYOUClient) ProcessWithPrompt(promptTemplate *prompt.PromptTemplate, values map[string]string) (string, error) {\r\n\tc.logger.Debug(\"Processing with prompt using AI.YOU\")\r\n\r\n\tformattedPrompt := promptTemplate.Format(values)\r\n\treturn c.Translate(formattedPrompt, \"\")\r\n}\r\n\r\nfunc (c *AIYOUClient) CreateThread() (string, error) {\r\n\tc.logger.Debug(\"Creating new AI.YOU thread\")\r\n\r\n\tvar threadResp struct {\r\n\t\tID string `json:\"id\"`\r\n\t}\r\n\r\n\terr := c.apiCaller.Call(\"/v1/threads\", \"POST\", map[string]string{}, \u0026threadResp)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"error creating thread: %w\", err)\r\n\t}\r\n\r\n\tif threadResp.ID == \"\" {\r\n\t\treturn \"\", fmt.Errorf(\"thread ID is empty in response\")\r\n\t}\r\n\r\n\tc.logger.Debug(fmt.Sprintf(\"Thread created with ID: %s\", threadResp.ID))\r\n\treturn threadResp.ID, nil\r\n}\r\n\r\nfunc (c *AIYOUClient) ChatInThread(threadID, input string) (string, error) {\r\n\tc.logger.Debug(fmt.Sprintf(\"Chatting in thread %s\", threadID))\r\n\r\n\terr := c.addMessage(threadID, input)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"failed to add message: %w\", err)\r\n\t}\r\n\r\n\trunID, err := c.createRun(threadID)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"failed to create run: %w\", err)\r\n\t}\r\n\r\n\tcompletedRun, err := c.waitForCompletion(threadID, runID)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"run failed: %w\", err)\r\n\t}\r\n\r\n\treturn completedRun.Response, nil\r\n}\r\n\r\nfunc (c *AIYOUClient) addMessage(threadID, content string) error {\r\n\tc.logger.Debug(fmt.Sprintf(\"Adding message to thread %s\", threadID))\r\n\r\n\tmessageData := map[string]string{\r\n\t\t\"role\":    \"user\",\r\n\t\t\"content\": content,\r\n\t}\r\n\r\n\tvar response interface{}\r\n\terr := c.apiCaller.Call(fmt.Sprintf(\"/v1/threads/%s/messages\", threadID), \"POST\", messageData, \u0026response)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"error adding message: %w\", err)\r\n\t}\r\n\r\n\tc.logger.Debug(\"Message added successfully\")\r\n\treturn nil\r\n}\r\n\r\nfunc (c *AIYOUClient) createRun(threadID string) (string, error) {\r\n\tc.logger.Debug(fmt.Sprintf(\"Creating run for thread %s\", threadID))\r\n\r\n\trunData := map[string]string{\r\n\t\t\"assistantId\": c.AssistantID,\r\n\t}\r\n\r\n\tvar runResp struct {\r\n\t\tID string `json:\"id\"`\r\n\t}\r\n\terr := c.apiCaller.Call(fmt.Sprintf(\"/v1/threads/%s/runs\", threadID), \"POST\", runData, \u0026runResp)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"error creating run: %w\", err)\r\n\t}\r\n\r\n\tc.logger.Debug(fmt.Sprintf(\"Run created with ID: %s\", runResp.ID))\r\n\treturn runResp.ID, nil\r\n}\r\n\r\nfunc (c *AIYOUClient) waitForCompletion(threadID, runID string) (*Run, error) {\r\n\tmaxAttempts := 30\r\n\tdelayBetweenAttempts := 2 * time.Second\r\n\r\n\tfor i := 0; i \u003c maxAttempts; i++ {\r\n\t\tc.logger.Debug(fmt.Sprintf(\"Attempt %d to retrieve run status\", i+1))\r\n\t\trun, err := c.retrieveRun(threadID, runID)\r\n\t\tif err != nil {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\r\n\t\tswitch run.Status {\r\n\t\tcase \"completed\":\r\n\t\t\tc.logger.Debug(\"Run completed successfully\")\r\n\t\t\treturn run, nil\r\n\t\tcase \"failed\", \"cancelled\":\r\n\t\t\treturn nil, fmt.Errorf(\"run failed with status: %s\", run.Status)\r\n\t\tdefault:\r\n\t\t\tc.logger.Debug(fmt.Sprintf(\"Waiting for run completion. Pausing for %v\", delayBetweenAttempts))\r\n\t\t\ttime.Sleep(delayBetweenAttempts)\r\n\t\t}\r\n\t}\r\n\r\n\treturn nil, fmt.Errorf(\"timeout waiting for run completion\")\r\n}\r\n\r\nfunc (c *AIYOUClient) retrieveRun(threadID, runID string) (*Run, error) {\r\n\tc.logger.Debug(fmt.Sprintf(\"Retrieving run %s for thread %s\", runID, threadID))\r\n\r\n\tvar runStatus Run\r\n\terr := c.apiCaller.Call(fmt.Sprintf(\"/v1/threads/%s/runs/%s\", threadID, runID), \"POST\", map[string]string{}, \u0026runStatus)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"error retrieving run: %w\", err)\r\n\t}\r\n\r\n\tc.logger.Debug(fmt.Sprintf(\"Run status retrieved: %v\", runStatus))\r\n\treturn \u0026runStatus, nil\r\n}\r\n\r\nfunc NewHTTPAPICaller(baseURL string) APICaller {\r\n\treturn \u0026HTTPAPICaller{\r\n\t\tbaseURL: baseURL,\r\n\t\thttpClient: \u0026http.Client{\r\n\t\t\tTimeout: 120 * time.Second,\r\n\t\t},\r\n\t}\r\n}\r\n\r\nfunc (c *HTTPAPICaller) Call(endpoint, method string, data interface{}, response interface{}) error {\r\n\turl := c.baseURL + endpoint\r\n\tvar req *http.Request\r\n\tvar err error\r\n\r\n\tif data != nil {\r\n\t\tjsonData, err := json.Marshal(data)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"error marshaling request data: %w\", err)\r\n\t\t}\r\n\t\treq, err = http.NewRequest(method, url, bytes.NewBuffer(jsonData))\r\n\t} else {\r\n\t\treq, err = http.NewRequest(method, url, nil)\r\n\t}\r\n\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"error creating request: %w\", err)\r\n\t}\r\n\r\n\treq.Header.Set(\"Content-Type\", \"application/json\")\r\n\tif c.token != \"\" {\r\n\t\treq.Header.Set(\"Authorization\", \"Bearer \"+c.token)\r\n\t}\r\n\r\n\tresp, err := c.httpClient.Do(req)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"error sending request: %w\", err)\r\n\t}\r\n\tdefer resp.Body.Close()\r\n\r\n\tbody, err := ioutil.ReadAll(resp.Body)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"error reading response body: %w\", err)\r\n\t}\r\n\r\n\tif resp.StatusCode != http.StatusOK \u0026\u0026 resp.StatusCode != http.StatusCreated {\r\n\t\treturn fmt.Errorf(\"API error: status code %d, body: %s\", resp.StatusCode, string(body))\r\n\t}\r\n\r\n\terr = json.Unmarshal(body, response)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"error unmarshaling response: %w\", err)\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\nfunc (c *HTTPAPICaller) SetToken(token string) {\r\n\tc.token = token\r\n}\r\n",
    "size": 7742,
    "modTime": "2024-10-30T22:49:14.3112954+01:00",
    "path": "internal\\llm\\aiyou.go"
  },
  {
    "name": "claude.go",
    "content": "// internal/llm/claude.go\r\n\r\npackage llm\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"net/http\"\r\n\t\"strings\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n)\r\n\r\n// ClaudeClient implements the Client interface for Claude\r\ntype ClaudeClient struct {\r\n\tapiKey string\r\n\tmodel  string\r\n\tclient *http.Client\r\n\tconfig *config.Config\r\n}\r\n\r\n// supportedClaudeModels defines the list of supported Claude models\r\nvar supportedClaudeModels = map[string]bool{\r\n\t\"claude-3-5-sonnet-20240620\": true,\r\n\t\"claude-3-opus-20240229\":     true,\r\n\t\"claude-3-haiku-20240307\":    true,\r\n}\r\n\r\n// NewClaudeClient creates a new Claude client\r\nfunc NewClaudeClient(apiKey string, model string) (*ClaudeClient, error) {\r\n\tlog.Debug(\"Creating new Claude client with model: %s\", model)\r\n\tif apiKey == \"\" {\r\n\t\tlog.Error(\"API key is missing for Claude client\")\r\n\t\treturn nil, ErrAPIKeyMissing\r\n\t}\r\n\r\n\tif !supportedClaudeModels[model] {\r\n\t\tlog.Error(\"Unsupported Claude model: %s\", model)\r\n\t\treturn nil, fmt.Errorf(\"%w: %s\", ErrUnsupportedModel, model)\r\n\t}\r\n\r\n\treturn \u0026ClaudeClient{\r\n\t\tapiKey: apiKey,\r\n\t\tmodel:  model,\r\n\t\tclient: \u0026http.Client{Timeout: 60 * time.Second},\r\n\t\tconfig: config.GetConfig(),\r\n\t}, nil\r\n}\r\n\r\n// Translate sends a prompt to the Claude API and returns the response\r\nfunc (c *ClaudeClient) Translate(prompt string, context string) (string, error) {\r\n\tlog.Debug(i18n.Messages.TranslationStarted, \"Claude\", c.model)\r\n\tlog.Debug(\"Starting Translate. Prompt length: %d, Context length: %d\", len(prompt), len(context))\r\n\r\n\tvar result string\r\n\tvar err error\r\n\tmaxRetries := 5\r\n\tbaseDelay := time.Second * 10\r\n\tmaxDelay := time.Minute * 2\r\n\r\n\tfor attempt := 0; attempt \u003c maxRetries; attempt++ {\r\n\t\tlog.Debug(\"Attempt %d of %d\", attempt+1, maxRetries)\r\n\t\tresult, err = c.makeRequest(prompt, context)\r\n\t\tif err == nil {\r\n\t\t\tlog.Debug(i18n.Messages.TranslationCompleted, \"Claude\", c.model)\r\n\t\t\treturn result, nil\r\n\t\t}\r\n\r\n\t\tif !isRateLimitError(err) {\r\n\t\t\tlog.Warning(i18n.Messages.TranslationRetry, attempt+1, err)\r\n\t\t\ttime.Sleep(time.Duration(attempt+1) * time.Second)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tdelay := baseDelay * time.Duration(1\u003c\u003cuint(attempt))\r\n\t\tif delay \u003e maxDelay {\r\n\t\t\tdelay = maxDelay\r\n\t\t}\r\n\t\tlog.Warning(i18n.Messages.RateLimitExceeded, delay)\r\n\t\ttime.Sleep(delay)\r\n\t}\r\n\r\n\tlog.Error(i18n.Messages.TranslationFailed, err)\r\n\tlog.Debug(\"Translation completed. Result length: %d\", len(result))\r\n\r\n\treturn \"\", fmt.Errorf(\"%w: %v\", ErrTranslationFailed, err)\r\n}\r\n\r\nfunc isRateLimitError(err error) bool {\r\n\treturn strings.Contains(err.Error(), \"rate_limit_error\")\r\n}\r\n\r\nfunc (c *ClaudeClient) makeRequest(prompt string, context string) (string, error) {\r\n\tlog.Debug(\"Making request to Claude API\")\r\n\turl := config.GetConfig().ClaudeAPIURL\r\n\r\n\trequestBody, err := json.Marshal(map[string]interface{}{\r\n\t\t\"model\": c.model,\r\n\t\t\"messages\": []map[string]string{\r\n\t\t\t{\"role\": \"user\", \"content\": prompt},\r\n\t\t},\r\n\t\t\"system\":     context,\r\n\t\t\"max_tokens\": c.config.MaxTokens,\r\n\t})\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error marshalling request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error marshalling request: %w\", err)\r\n\t}\r\n\tlog.Debug(\"Claude API Request Body : %s\", requestBody)\r\n\treq, err := http.NewRequest(\"POST\", url, bytes.NewBuffer(requestBody))\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error creating request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error creating request: %w\", err)\r\n\t}\r\n\r\n\treq.Header.Set(\"Content-Type\", \"application/json\")\r\n\treq.Header.Set(\"x-api-key\", c.apiKey)\r\n\treq.Header.Set(\"anthropic-version\", \"2023-06-01\")\r\n\r\n\tlog.Debug(\"Sending request to Claude API : %s\", prompt)\r\n\tresp, err := c.client.Do(req)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error sending request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error sending request: %w\", err)\r\n\t}\r\n\tdefer resp.Body.Close()\r\n\r\n\tbody, err := ioutil.ReadAll(resp.Body)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error reading response: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error reading response: %w\", err)\r\n\t}\r\n\r\n\tif resp.StatusCode != http.StatusOK {\r\n\t\tlog.Error(\"API request failed with status code %d: %s\", resp.StatusCode, string(body))\r\n\t\treturn \"\", fmt.Errorf(\"API request failed with status code %d: %s\", resp.StatusCode, string(body))\r\n\t}\r\n\r\n\tvar response struct {\r\n\t\tContent []struct {\r\n\t\t\tText string `json:\"text\"`\r\n\t\t} `json:\"content\"`\r\n\t}\r\n\r\n\terr = json.Unmarshal(body, \u0026response)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error unmarshalling response: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error unmarshalling response: %w\", err)\r\n\t}\r\n\r\n\tif len(response.Content) == 0 {\r\n\t\tlog.Error(\"No content in response\")\r\n\t\treturn \"\", fmt.Errorf(\"no content in response\")\r\n\t}\r\n\tlog.Debug(\"Claude API Response : %s\", response.Content)\r\n\tlog.Debug(\"Successfully received and parsed response from Claude API.\")\r\n\treturn response.Content[0].Text, nil\r\n}\r\n\r\n// ProcessWithPrompt processes a prompt template with the given values and sends it to the Claude API\r\nfunc (c *ClaudeClient) ProcessWithPrompt(promptTemplate *prompt.PromptTemplate, values map[string]string) (string, error) {\r\n\tlog.Debug(\"Processing prompt with Claude\")\r\n\tformattedPrompt := promptTemplate.Format(values)\r\n\r\n\t// Utilisez la méthode Translate existante pour envoyer le prompt formatté\r\n\treturn c.Translate(formattedPrompt, \"\")\r\n}\r\n",
    "size": 5326,
    "modTime": "2024-11-06T00:12:42.8304112+01:00",
    "path": "internal\\llm\\claude.go"
  },
  {
    "name": "client.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n)\r\n\r\n// Client defines the interface for LLM clients\r\ntype Client interface {\r\n\t// Translate takes a prompt and context, and returns the LLM's response\r\n\tTranslate(prompt string, context string) (string, error)\r\n\tProcessWithPrompt(promptTemplate *prompt.PromptTemplate, values map[string]string) (string, error)\r\n}\r\n",
    "size": 390,
    "modTime": "2024-10-21T00:09:55.4348073+02:00",
    "path": "internal\\llm\\client.go"
  },
  {
    "name": "constants.go",
    "content": "// internal/llm/constants.go\r\n\r\npackage llm\r\n\r\nimport \"time\"\r\n\r\n\r\nconst (\r\n    MaxRetries        = 5\r\n    InitialRetryDelay = 1 * time.Second\r\n    MaxRetryDelay     = 32 * time.Second\r\n)\r\n\r\nvar ModelContextLimits = map[string]int{\r\n    \"GPT-4o\":                 8192,\r\n    \"GPT-4o mini\":            4096,\r\n    \"o1-preview\":             16384,\r\n    \"o1-mini\":                2048,\r\n    \"claude-3-5-sonnet-20240620\": 200000,\r\n    \"claude-3-opus-20240229\":     200000,\r\n    \"claude-3-haiku-20240307\":    200000,\r\n    \"llama3.2:3B\":            4096,\r\n    \"llama3.1:8B\":            4096,\r\n    \"mistral-nemo:12B\":       8192,\r\n    \"mixtral:7B\":             32768,\r\n    \"mistral:7B\":             8192,\r\n    \"mistral-small:22B\":      16384,\r\n}",
    "size": 735,
    "modTime": "2024-10-20T15:18:13.4507413+02:00",
    "path": "internal\\llm\\constants.go"
  },
  {
    "name": "errors.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"errors\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\nvar (\r\n\tErrUnsupportedModel  = errors.New(i18n.Messages.ErrUnsupportedModel)\r\n\tErrAPIKeyMissing     = errors.New(i18n.Messages.ErrAPIKeyMissing)\r\n\tErrTranslationFailed = errors.New(i18n.Messages.ErrTranslationFailed)\r\n\tErrInvalidLLMType    = errors.New(i18n.Messages.ErrInvalidLLMType)\r\n\tErrContextTooLong    = errors.New(i18n.Messages.ErrContextTooLong)\r\n)\r\n",
    "size": 449,
    "modTime": "2024-10-20T20:00:43.9609939+02:00",
    "path": "internal\\llm\\errors.go"
  },
  {
    "name": "factory.go",
    "content": "// internal/llm/factory.go\r\n\r\npackage llm\r\n\r\nimport (\r\n\t\"fmt\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n)\r\n\r\nfunc GetClient(llmType string, model string) (Client, error) {\r\n\tcfg := config.GetConfig()\r\n\r\n\tswitch llmType {\r\n\tcase \"openai\":\r\n\t\treturn NewOpenAIClient(cfg.OpenAIAPIKey, model)\r\n\tcase \"claude\":\r\n\t\treturn NewClaudeClient(cfg.ClaudeAPIKey, model)\r\n\tcase \"ollama\":\r\n\t\treturn NewOllamaClient(model)\r\n\tcase \"aiyou\":\r\n        return NewAIYOUClient(cfg.AIYOUAssistantID, cfg.AIYOUEmail, cfg.AIYOUPassword)\r\n\tdefault:\r\n\t\treturn nil, fmt.Errorf(\"%w: %s\", ErrInvalidLLMType, llmType)\r\n\t}\r\n}\r\n\r\ntype contextCheckingClient struct {\r\n\tbaseClient Client\r\n\tmodel      string\r\n}\r\n\r\nfunc (c *contextCheckingClient) Translate(prompt string, context string) (string, error) {\r\n\tif err := CheckContextLength(c.model, context); err != nil {\r\n\t\treturn \"\", err\r\n\t}\r\n\treturn c.baseClient.Translate(prompt, context)\r\n}\r\n",
    "size": 917,
    "modTime": "2024-10-30T22:49:14.3158037+01:00",
    "path": "internal\\llm\\factory.go"
  },
  {
    "name": "logger.go",
    "content": "// internal/llm/logger.go\r\n\r\npackage llm\r\n\r\nimport (\r\n    \"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()",
    "size": 139,
    "modTime": "2024-10-20T15:51:39.514526+02:00",
    "path": "internal\\llm\\logger.go"
  },
  {
    "name": "ollama.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"net/http\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n)\r\n\r\ntype OllamaClient struct {\r\n\tmodel  string\r\n\tclient *http.Client\r\n\tconfig *config.Config\r\n}\r\n\r\nvar supportedOllamaModels = map[string]bool{\r\n\t\"llama3.2\":      true,\r\n\t\"llama3.1\":      true,\r\n\t\"mistral-nemo\":  true,\r\n\t\"mixtral\":       true,\r\n\t\"mistral\":       true,\r\n\t\"mistral-small\": true,\r\n}\r\n\r\nfunc NewOllamaClient(model string) (*OllamaClient, error) {\r\n\tlog.Debug(\"Creating new Ollama client with model: %s\", model)\r\n\tif !supportedOllamaModels[model] {\r\n\t\tlog.Error(\"Unsupported Ollama model: %s\", model)\r\n\t\treturn nil, fmt.Errorf(\"%w: %s\", ErrUnsupportedModel, model)\r\n\t}\r\n\r\n\treturn \u0026OllamaClient{\r\n\t\tmodel:  model,\r\n\t\tclient: \u0026http.Client{Timeout: 60 * time.Second},\r\n\t\tconfig: config.GetConfig(),\r\n\t}, nil\r\n}\r\n\r\nfunc (c *OllamaClient) Translate(prompt string, context string) (string, error) {\r\n\tlog.Debug(i18n.Messages.TranslationStarted, \"Ollama\", c.model)\r\n\tlog.Debug(\"Starting Translate. Prompt length: %d, Context length: %d\", len(prompt), len(context))\r\n\r\n\tvar result string\r\n\tvar err error\r\n\tmaxRetries := 5\r\n\tbaseDelay := time.Second * 60\r\n\tmaxDelay := time.Minute * 5\r\n\r\n\tfor attempt := 0; attempt \u003c maxRetries; attempt++ {\r\n\t\tlog.Debug(\"Attempt %d of %d\", attempt+1, maxRetries)\r\n\t\tresult, err = c.makeRequest(prompt, context)\r\n\t\tif err == nil {\r\n\t\t\tlog.Debug(i18n.Messages.TranslationCompleted, \"Ollama\", c.model)\r\n\t\t\treturn result, nil\r\n\t\t}\r\n\r\n\t\tif !isRateLimitError(err) {\r\n\t\t\tlog.Warning(i18n.Messages.TranslationRetry, attempt+1, err)\r\n\t\t\ttime.Sleep(time.Duration(attempt+1) * time.Second)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tdelay := baseDelay * time.Duration(1\u003c\u003cuint(attempt))\r\n\t\tif delay \u003e maxDelay {\r\n\t\t\tdelay = maxDelay\r\n\t\t}\r\n\t\tlog.Warning(i18n.Messages.RateLimitExceeded, delay)\r\n\t\ttime.Sleep(delay)\r\n\t}\r\n\r\n\tlog.Error(i18n.Messages.TranslationFailed, err)\r\n\tlog.Debug(\"Translation completed. Result length: %d\", len(result))\r\n\r\n\treturn \"\", fmt.Errorf(\"%w: %v\", ErrTranslationFailed, err)\r\n}\r\n\r\nfunc (c *OllamaClient) makeRequest(prompt string, context string) (string, error) {\r\n\tlog.Debug(\"Making request to Ollama API\")\r\n\turl := c.config.OllamaAPIURL\r\n\r\n\trequestBody, err := json.Marshal(map[string]interface{}{\r\n\t\t\"model\":  c.model,\r\n\t\t\"prompt\": prompt,\r\n\t\t\"system\": context,\r\n\t\t\"stream\": false,\r\n\t})\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error marshalling request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error marshalling request: %w\", err)\r\n\t}\r\n\r\n\treq, err := http.NewRequest(\"POST\", url, bytes.NewBuffer(requestBody))\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error creating request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error creating request: %w\", err)\r\n\t}\r\n\r\n\treq.Header.Set(\"Content-Type\", \"application/json\")\r\n\r\n\tresp, err := c.client.Do(req)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error sending request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error sending request: %w\", err)\r\n\t}\r\n\tdefer resp.Body.Close()\r\n\r\n\tbody, err := ioutil.ReadAll(resp.Body)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error reading response: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error reading response: %w\", err)\r\n\t}\r\n\r\n\tif resp.StatusCode != http.StatusOK {\r\n\t\tlog.Error(\"API request failed with status code %d: %s\", resp.StatusCode, string(body))\r\n\t\treturn \"\", fmt.Errorf(\"API request failed with status code %d: %s\", resp.StatusCode, string(body))\r\n\t}\r\n\r\n\tvar response struct {\r\n\t\tResponse string `json:\"response\"`\r\n\t}\r\n\r\n\terr = json.Unmarshal(body, \u0026response)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error unmarshalling response: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error unmarshalling response: %w\", err)\r\n\t}\r\n\r\n\tlog.Debug(\"Successfully received and parsed response from Ollama API.\")\r\n\treturn response.Response, nil\r\n}\r\n\r\nfunc (c *OllamaClient) ProcessWithPrompt(promptTemplate *prompt.PromptTemplate, values map[string]string) (string, error) {\r\n\tlog.Debug(\"Processing prompt with Ollama\")\r\n\tformattedPrompt := promptTemplate.Format(values)\r\n\r\n\treturn c.Translate(formattedPrompt, \"\")\r\n}\r\n",
    "size": 4096,
    "modTime": "2024-10-30T22:49:14.3188118+01:00",
    "path": "internal\\llm\\ollama.go"
  },
  {
    "name": "openai.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"context\"\r\n\t\"fmt\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n\t\"github.com/sashabaranov/go-openai\"\r\n)\r\n\r\ntype OpenAIClient struct {\r\n\tapiKey string\r\n\tmodel  string\r\n\tclient *openai.Client\r\n\tconfig *config.Config\r\n}\r\n\r\nvar supportedModels = map[string]bool{\r\n\t\"gpt-4o\":      true,\r\n\t\"gpt-4o-mini\": true,\r\n\t\"o1-preview\":  true,\r\n\t\"o1-mini\":     true,\r\n}\r\n\r\nfunc NewOpenAIClient(apiKey string, model string) (*OpenAIClient, error) {\r\n\tlog.Debug(\"Creating new OpenAI client with model: %s\", model)\r\n\tif apiKey == \"\" {\r\n\t\tlog.Error(\"API key is missing for OpenAI client\")\r\n\t\treturn nil, ErrAPIKeyMissing\r\n\t}\r\n\r\n\tif !supportedModels[model] {\r\n\t\tlog.Error(\"Unsupported OpenAI model: %s\", model)\r\n\t\treturn nil, fmt.Errorf(\"%w: %s\", ErrUnsupportedModel, model)\r\n\t}\r\n\r\n\tclient := openai.NewClient(apiKey)\r\n\treturn \u0026OpenAIClient{\r\n\t\tapiKey: apiKey,\r\n\t\tmodel:  model,\r\n\t\tclient: client,\r\n\t\tconfig: config.GetConfig(),\r\n\t}, nil\r\n}\r\n\r\nfunc (c *OpenAIClient) Translate(prompt string, context string) (string, error) {\r\n\tlog.Debug(i18n.Messages.TranslationStarted, \"OpenAI\", c.model)\r\n\tlog.Debug(\"Starting Translate. Prompt length: %d, Context length: %d\", len(prompt), len(context))\r\n\r\n\tvar result string\r\n\tvar err error\r\n\tmaxRetries := 5\r\n\tbaseDelay := time.Second * 10\r\n\tmaxDelay := time.Minute * 2\r\n\r\n\tfor attempt := 0; attempt \u003c maxRetries; attempt++ {\r\n\t\tlog.Debug(\"Attempt %d of %d\", attempt+1, maxRetries)\r\n\t\tresult, err = c.makeRequest(prompt, context)\r\n\t\tif err == nil {\r\n\t\t\tlog.Debug(i18n.Messages.TranslationCompleted, \"OpenAI\", c.model)\r\n\t\t\treturn result, nil\r\n\t\t}\r\n\r\n\t\tif !isRateLimitError(err) {\r\n\t\t\tlog.Warning(i18n.Messages.TranslationRetry, attempt+1, err)\r\n\t\t\ttime.Sleep(time.Duration(attempt+1) * time.Second)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tdelay := baseDelay * time.Duration(1\u003c\u003cuint(attempt))\r\n\t\tif delay \u003e maxDelay {\r\n\t\t\tdelay = maxDelay\r\n\t\t}\r\n\t\tlog.Warning(i18n.Messages.RateLimitExceeded, delay)\r\n\t\ttime.Sleep(delay)\r\n\t}\r\n\r\n\tlog.Error(i18n.Messages.TranslationFailed, err)\r\n\tlog.Debug(\"Translation completed. Result length: %d\", len(result))\r\n\r\n\treturn \"\", fmt.Errorf(\"%w: %v\", ErrTranslationFailed, err)\r\n}\r\n\r\nfunc (c *OpenAIClient) makeRequest(prompt string, systemContext string) (string, error) {\r\n\tlog.Debug(\"Making request to OpenAI API\")\r\n\r\n\tmessages := []openai.ChatCompletionMessage{\r\n\t\t{\r\n\t\t\tRole:    openai.ChatMessageRoleSystem,\r\n\t\t\tContent: systemContext,\r\n\t\t},\r\n\t\t{\r\n\t\t\tRole:    openai.ChatMessageRoleUser,\r\n\t\t\tContent: prompt,\r\n\t\t},\r\n\t}\r\n\r\n\tresp, err := c.client.CreateChatCompletion(\r\n\t\tcontext.Background(),\r\n\t\topenai.ChatCompletionRequest{\r\n\t\t\tModel:       c.model,\r\n\t\t\tMessages:    messages,\r\n\t\t\tMaxTokens:   c.config.MaxTokens,\r\n\t\t\tTemperature: 0.7,\r\n\t\t},\r\n\t)\r\n\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error creating chat completion: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error creating chat completion: %w\", err)\r\n\t}\r\n\r\n\tif len(resp.Choices) == 0 {\r\n\t\tlog.Error(\"No content in response\")\r\n\t\treturn \"\", fmt.Errorf(\"no content in response\")\r\n\t}\r\n\r\n\tlog.Debug(\"Successfully received and parsed response from OpenAI API.\")\r\n\treturn resp.Choices[0].Message.Content, nil\r\n}\r\n\r\nfunc (c *OpenAIClient) ProcessWithPrompt(promptTemplate *prompt.PromptTemplate, values map[string]string) (string, error) {\r\n\tlog.Debug(\"Processing prompt with OpenAI\")\r\n\tformattedPrompt := promptTemplate.Format(values)\r\n\r\n\treturn c.Translate(formattedPrompt, \"\")\r\n}\r\n",
    "size": 3489,
    "modTime": "2024-10-30T22:49:14.3208109+01:00",
    "path": "internal\\llm\\openai.go"
  },
  {
    "name": "utils.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/tokenizer\"\r\n)\r\n\r\nfunc CheckContextLength(model string, context string) error {\r\n\tlimit, ok := ModelContextLimits[model]\r\n\tif !ok {\r\n\t\treturn ErrUnsupportedModel\r\n\t}\r\n\r\n\ttokenCount, err := tokenizer.CountTokens(context)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\tif tokenCount \u003e limit {\r\n\t\treturn ErrContextTooLong\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n",
    "size": 399,
    "modTime": "2024-10-20T15:16:37.2625502+02:00",
    "path": "internal\\llm\\utils.go"
  },
  {
    "name": "logger.go",
    "content": "// internal/logger/logger.go\r\n\r\npackage logger\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"log\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"runtime\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\n// LogLevel represents the severity of a log message\r\ntype LogLevel int\r\n\r\nconst (\r\n\t// DebugLevel is used for detailed system operations\r\n\tDebugLevel LogLevel = iota\r\n\t// InfoLevel is used for general operational entries\r\n\tInfoLevel\r\n\t// WarningLevel is used for non-critical issues\r\n\tWarningLevel\r\n\t// ErrorLevel is used for errors that need attention\r\n\tErrorLevel\r\n)\r\n\r\nvar (\r\n\tinstance *Logger\r\n\tonce     sync.Once\r\n)\r\n\r\n// Logger handles all logging operations\r\ntype Logger struct {\r\n\tlevel  LogLevel\r\n\tlogger *log.Logger\r\n\tfile   *os.File\r\n\tmu     sync.Mutex\r\n}\r\n\r\n// GetLogger returns the singleton instance of Logger\r\nfunc GetLogger() *Logger {\r\n\tonce.Do(func() {\r\n\t\tinstance = \u0026Logger{\r\n\t\t\tlevel:  InfoLevel,\r\n\t\t\tlogger: log.New(os.Stdout, \"\", log.Ldate|log.Ltime),\r\n\t\t}\r\n\t\tinstance.setupLogFile()\r\n\t})\r\n\treturn instance\r\n}\r\n\r\nfunc (l *Logger) setupLogFile() {\r\n\tlogDir := config.GetConfig().LogDirectory\r\n\tif logDir == \"\" {\r\n\t\tlogDir = \"logs\"\r\n\t}\r\n\r\n\tif err := os.MkdirAll(logDir, 0755); err != nil {\r\n\t\tl.Error(i18n.GetMessage(\"ErrCreateLogDir\"), err)\r\n\t\treturn\r\n\t}\r\n\r\n\tlogFile := filepath.Join(logDir, fmt.Sprintf(\"ontology_%s.log\", time.Now().Format(\"2006-01-02\")))\r\n\tfile, err := os.OpenFile(logFile, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644)\r\n\tif err != nil {\r\n\t\tl.Error(i18n.GetMessage(\"ErrOpenLogFile\"), err)\r\n\t\treturn\r\n\t}\r\n\r\n\tl.file = file\r\n\tl.logger.SetOutput(io.MultiWriter(os.Stdout, file))\r\n}\r\n\r\n// SetLevel sets the current log level\r\nfunc (l *Logger) SetLevel(level LogLevel) {\r\n\tl.mu.Lock()\r\n\tdefer l.mu.Unlock()\r\n\tl.level = level\r\n}\r\n\r\nfunc (l *Logger) log(level LogLevel, message string, args ...interface{}) {\r\n\tif level \u003c l.level {\r\n\t\treturn\r\n\t}\r\n\r\n\tl.mu.Lock()\r\n\tdefer l.mu.Unlock()\r\n\r\n\t_, file, line, _ := runtime.Caller(2)\r\n\tlevelStr := [...]string{\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"}[level]\r\n\tlogMessage := fmt.Sprintf(\"[%s] %s:%d - %s\", levelStr, filepath.Base(file), line, fmt.Sprintf(message, args...))\r\n\tl.logger.Println(logMessage)\r\n}\r\n\r\n// Debug logs a message at DebugLevel\r\nfunc (l *Logger) Debug(format string, args ...interface{}) {\r\n    if l.GetLevel() \u003c= DebugLevel {\r\n        l.log(DebugLevel, format, args...)\r\n    }\r\n}\r\n\r\n// Info logs a message at InfoLevel\r\nfunc (l *Logger) Info(message string, args ...interface{}) {\r\n\tl.log(InfoLevel, message, args...)\r\n}\r\n\r\n// Warning logs a message at WarningLevel\r\nfunc (l *Logger) Warning(message string, args ...interface{}) {\r\n\tl.log(WarningLevel, message, args...)\r\n}\r\n\r\n// Error logs a message at ErrorLevel\r\nfunc (l *Logger) Error(message string, args ...interface{}) {\r\n\tl.log(ErrorLevel, message, args...)\r\n}\r\n\r\n// Close closes the log file\r\nfunc (l *Logger) Close() {\r\n\tif l.file != nil {\r\n\t\tl.file.Close()\r\n\t}\r\n}\r\n\r\n// UpdateProgress updates the progress on the console\r\nfunc (l *Logger) UpdateProgress(current, total int) {\r\n\tfmt.Printf(\"\\rProgress: %d/%d\", current, total)\r\n}\r\n\r\n// RotateLogs archives old log files\r\nfunc (l *Logger) RotateLogs() error {\r\n\t// Implementation of log rotation\r\n\t// This is a placeholder and should be implemented based on specific requirements\r\n\treturn nil\r\n}\r\n\r\n// ParseLevel converts a string level to LogLevel\r\nfunc ParseLevel(level string) LogLevel {\r\n\tswitch strings.ToLower(level) {\r\n\tcase \"debug\":\r\n\t\treturn DebugLevel\r\n\tcase \"info\":\r\n\t\treturn InfoLevel\r\n\tcase \"warning\":\r\n\t\treturn WarningLevel\r\n\tcase \"error\":\r\n\t\treturn ErrorLevel\r\n\tdefault:\r\n\t\treturn InfoLevel\r\n\t}\r\n}\r\n\r\n// Ajoutez cette méthode\r\nfunc (l *Logger) GetLevel() LogLevel {\r\n\tl.mu.Lock()\r\n\tdefer l.mu.Unlock()\r\n\treturn l.level\r\n}\r\n",
    "size": 3790,
    "modTime": "2024-10-20T23:45:56.7432409+02:00",
    "path": "internal\\logger\\logger.go"
  },
  {
    "name": "metadata.go",
    "content": "// metadata/metadata.go\r\n\r\npackage metadata\r\n\r\nimport (\r\n\t\"crypto/sha256\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"net/url\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/chrlesur/Ontology/internal/storage\"\r\n)\r\n\r\nvar log = logger.GetLogger()\r\n\r\n// FileMetadata représente les métadonnées d'un fichier source\r\ntype FileMetadata struct {\r\n\tSourceFile     string            `json:\"source_file\"`\r\n\tDirectory      string            `json:\"directory\"`\r\n\tFileDate       time.Time         `json:\"file_date\"`\r\n\tSHA256Hash     string            `json:\"sha256_hash\"`\r\n\tOntologyFile   string            `json:\"ontology_file\"`\r\n\tContextFile    string            `json:\"context_file,omitempty\"`\r\n\tProcessingDate time.Time         `json:\"processing_date\"`\r\n\tFormatMetadata map[string]string `json:\"format_metadata,omitempty\"`\r\n}\r\n\r\ntype s3FileInfo struct {\r\n\tname    string\r\n\tsize    int64\r\n\tmodTime time.Time\r\n}\r\n\r\nfunc (fi *s3FileInfo) Name() string       { return fi.name }\r\nfunc (fi *s3FileInfo) Size() int64        { return fi.size }\r\nfunc (fi *s3FileInfo) Mode() os.FileMode  { return 0 }\r\nfunc (fi *s3FileInfo) ModTime() time.Time { return fi.modTime }\r\nfunc (fi *s3FileInfo) IsDir() bool        { return false }\r\nfunc (fi *s3FileInfo) Sys() interface{}   { return nil }\r\n\r\n// Generator gère la génération des métadonnées\r\ntype Generator struct {\r\n\tlogger  *logger.Logger\r\n\tstorage storage.Storage\r\n}\r\n\r\n// NewGenerator crée une nouvelle instance de Generator\r\nfunc NewGenerator(s storage.Storage) *Generator {\r\n\treturn \u0026Generator{\r\n\t\tlogger:  logger.GetLogger(),\r\n\t\tstorage: s,\r\n\t}\r\n}\r\n\r\n// GenerateMetadata crée les métadonnées pour un fichier source\r\nfunc (g *Generator) GenerateMetadata(sourcePath, ontologyFile, contextFile string) (*FileMetadata, error) {\r\n\tg.logger.Debug(\"Generating metadata for source file: %s\", sourcePath)\r\n\r\n\tisS3 := strings.HasPrefix(strings.ToLower(sourcePath), \"s3://\")\r\n\r\n\tvar fileInfo os.FileInfo\r\n\tvar err error\r\n\tvar directory string\r\n\tvar isDirectory bool\r\n\r\n\tif isS3 {\r\n\t\ts3Uri, err := url.Parse(sourcePath)\r\n\t\tif err != nil {\r\n\t\t\treturn nil, fmt.Errorf(\"failed to parse S3 URI: %w\", err)\r\n\t\t}\r\n\t\tdirectory = filepath.Dir(s3Uri.Path)\r\n\t\tisDirectory, err = g.storage.IsDirectory(sourcePath)\r\n\t\tif err != nil {\r\n\t\t\treturn nil, fmt.Errorf(\"failed to check if S3 path is directory: %w\", err)\r\n\t\t}\r\n\t\tfileInfo = \u0026s3FileInfo{\r\n\t\t\tname:    filepath.Base(s3Uri.Path),\r\n\t\t\tsize:    0, // Taille inconnue\r\n\t\t\tmodTime: time.Now(),\r\n\t\t}\r\n\t} else {\r\n\t\tfileInfo, err = os.Stat(sourcePath)\r\n\t\tif err != nil {\r\n\t\t\tg.logger.Error(\"Failed to get file info: %v\", err)\r\n\t\t\treturn nil, fmt.Errorf(\"failed to get file info: %w\", err)\r\n\t\t}\r\n\t\tdirectory = filepath.Dir(sourcePath)\r\n\t\tisDirectory = fileInfo.IsDir()\r\n\t}\r\n\r\n\tvar hash string\r\n\tif !isDirectory {\r\n\t\t// Calculer le hash SHA256 seulement si ce n'est pas un répertoire\r\n\t\thash, err = g.calculateSHA256(sourcePath)\r\n\t\tif err != nil {\r\n\t\t\tg.logger.Error(\"Failed to calculate SHA256: %v\", err)\r\n\t\t\treturn nil, fmt.Errorf(\"failed to calculate SHA256: %w\", err)\r\n\t\t}\r\n\t}\r\n\r\n\tmetadata := \u0026FileMetadata{\r\n\t\tSourceFile:     filepath.Base(sourcePath),\r\n\t\tDirectory:      directory,\r\n\t\tFileDate:       fileInfo.ModTime(),\r\n\t\tSHA256Hash:     hash,\r\n\t\tOntologyFile:   ontologyFile,\r\n\t\tContextFile:    contextFile,\r\n\t\tProcessingDate: time.Now(),\r\n\t\tFormatMetadata: make(map[string]string),\r\n\t}\r\n\r\n\tg.logger.Debug(\"Generated metadata: %+v\", metadata)\r\n\treturn metadata, nil\r\n}\r\n\r\n// SaveMetadata sauvegarde les métadonnées dans un fichier JSON\r\nfunc (g *Generator) SaveMetadata(metadata *FileMetadata, outputPath string) error {\r\n\tg.logger.Debug(\"Saving metadata to file: %s\", outputPath)\r\n\r\n\t// Créer le contenu JSON avec une indentation pour la lisibilité\r\n\tjsonData, err := json.MarshalIndent(metadata, \"\", \"  \")\r\n\tif err != nil {\r\n\t\tg.logger.Error(\"Failed to marshal metadata: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to marshal metadata: %w\", err)\r\n\t}\r\n\r\n\t// Écrire dans le fichier (utiliser le client de stockage approprié)\r\n\terr = g.storage.Write(outputPath, jsonData)\r\n\tif err != nil {\r\n\t\tg.logger.Error(\"Failed to write metadata file: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to write metadata file: %w\", err)\r\n\t}\r\n\r\n\tg.logger.Info(\"Metadata saved successfully to: %s\", outputPath)\r\n\treturn nil\r\n}\r\n\r\n// calculateSHA256 calcule le hash SHA256 d'un fichier\r\nfunc (g *Generator) calculateSHA256(filePath string) (string, error) {\r\n\tisS3 := strings.HasPrefix(strings.ToLower(filePath), \"s3://\")\r\n\r\n\tif isS3 {\r\n\t\tisDir, err := g.storage.IsDirectory(filePath)\r\n\t\tif err != nil {\r\n\t\t\treturn \"\", fmt.Errorf(\"failed to check if S3 path is directory: %w\", err)\r\n\t\t}\r\n\t\tif isDir {\r\n\t\t\treturn \"\", nil // Retourner une chaîne vide pour les répertoires S3\r\n\t\t}\r\n\t} else {\r\n\t\tfileInfo, err := os.Stat(filePath)\r\n\t\tif err != nil {\r\n\t\t\treturn \"\", fmt.Errorf(\"failed to get file info: %w\", err)\r\n\t\t}\r\n\t\tif fileInfo.IsDir() {\r\n\t\t\treturn \"\", nil // Retourner une chaîne vide pour les répertoires locaux\r\n\t\t}\r\n\t}\r\n\r\n\t// Utilisez l'interface Storage pour obtenir un lecteur\r\n\treader, err := g.storage.GetReader(filePath)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"failed to get reader for file: %w\", err)\r\n\t}\r\n\tdefer reader.Close()\r\n\r\n\thash := sha256.New()\r\n\tif _, err := io.Copy(hash, reader); err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"failed to calculate hash: %w\", err)\r\n\t}\r\n\r\n\treturn fmt.Sprintf(\"%x\", hash.Sum(nil)), nil\r\n}\r\n\r\n// GetMetadataFilename génère le nom du fichier de métadonnées\r\nfunc (g *Generator) GetMetadataFilename(sourcePath string) string {\r\n\tbaseFileName := filepath.Base(sourcePath)\r\n\text := filepath.Ext(baseFileName)\r\n\tnameWithoutExt := baseFileName[:len(baseFileName)-len(ext)]\r\n\treturn nameWithoutExt + \"_meta.json\"\r\n}\r\n",
    "size": 5769,
    "modTime": "2024-11-06T00:20:32.5509914+01:00",
    "path": "internal\\metadata\\metadata.go"
  },
  {
    "name": "ontology.go",
    "content": "package model\r\n\r\n// OntologyElement représente un élément unique dans l'ontologie\r\ntype OntologyElement struct {\r\n\tName      string // Nom de l'élément\r\n\tType      string // Type de l'élément\r\n\tPositions []int  // Positions de l'élément dans le document source\r\n\tDescription string \r\n\r\n}\r\n\r\n// Relation représente une relation entre deux éléments de l'ontologie\r\ntype Relation struct {\r\n\tSource      string\r\n\tType        string\r\n\tTarget      string\r\n\tDescription string\r\n}\r\n\r\n// Ontology représente une collection d'éléments d'ontologie\r\ntype Ontology struct {\r\n\tElements  []*OntologyElement // Liste des éléments de l'ontologie\r\n\tRelations []*Relation\r\n}\r\n\r\n// SetPositions définit les positions de l'élément\r\nfunc (e *OntologyElement) SetPositions(positions []int) {\r\n    e.Positions = positions\r\n}\r\n\r\n// NewOntologyElement crée un nouvel élément d'ontologie\r\nfunc NewOntologyElement(name, elementType string) *OntologyElement {\r\n\treturn \u0026OntologyElement{\r\n\t\tName:      name,\r\n\t\tType:      elementType,\r\n\t\tPositions: []int{}, // Initialise un slice vide pour les positions\r\n\t}\r\n}\r\n\r\n// NewOntology crée une nouvelle instance d'Ontology\r\nfunc NewOntology() *Ontology {\r\n\treturn \u0026Ontology{\r\n\t\tElements: []*OntologyElement{}, // Initialise un slice vide pour les éléments\r\n\t}\r\n}\r\n\r\n// AddElement ajoute un nouvel élément à l'ontologie\r\nfunc (o *Ontology) AddElement(element *OntologyElement) {\r\n\to.Elements = append(o.Elements, element)\r\n}\r\n\r\n// GetElementByName recherche un élément par son nom\r\nfunc (o *Ontology) GetElementByName(name string) *OntologyElement {\r\n\tfor _, element := range o.Elements {\r\n\t\tif element.Name == name {\r\n\t\t\treturn element\r\n\t\t}\r\n\t}\r\n\treturn nil // Retourne nil si l'élément n'est pas trouvé\r\n}\r\n\r\n// AddRelation ajoute une nouvelle relation à l'ontologie\r\nfunc (o *Ontology) AddRelation(relation *Relation) {\r\n\to.Relations = append(o.Relations, relation)\r\n}\r\n",
    "size": 1922,
    "modTime": "2024-10-30T22:49:14.3248112+01:00",
    "path": "internal\\model\\ontology.go"
  },
  {
    "name": "element.go",
    "content": "package ontology\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"strconv\"\r\n\t\"strings\"\r\n)\r\n\r\n// OntologyElement représente un élément unique dans l'ontologie\r\ntype OntologyElement struct {\r\n\tName      string\r\n\tType      string\r\n\tPositions []int\r\n}\r\n\r\n// NewOntologyElement crée un nouvel OntologyElement\r\nfunc NewOntologyElement(name, elementType string) *OntologyElement {\r\n\treturn \u0026OntologyElement{\r\n\t\tName:      name,\r\n\t\tType:      elementType,\r\n\t\tPositions: []int{},\r\n\t}\r\n}\r\n\r\n// AddPosition ajoute une nouvelle position à l'élément\r\nfunc (e *OntologyElement) AddPosition(position int) {\r\n\te.Positions = append(e.Positions, position)\r\n}\r\n\r\n// SetPositions remplace toutes les positions existantes par un nouveau slice\r\nfunc (e *OntologyElement) SetPositions(positions []int) {\r\n\te.Positions = positions\r\n}\r\n\r\n// String retourne une représentation en chaîne de caractères de l'élément\r\nfunc (e *OntologyElement) String() string {\r\n\tposStr := strings.Trim(strings.Join(strings.Fields(fmt.Sprint(e.Positions)), \",\"), \"[]\")\r\n\treturn fmt.Sprintf(\"%s|%s@%s\", e.Name, e.Type, posStr)\r\n}\r\n\r\n// Ontology représente une collection d'éléments d'ontologie\r\ntype Ontology struct {\r\n\tElements []*OntologyElement\r\n}\r\n\r\n// NewOntology crée une nouvelle instance d'Ontology\r\nfunc NewOntology() *Ontology {\r\n\treturn \u0026Ontology{\r\n\t\tElements: []*OntologyElement{},\r\n\t}\r\n}\r\n\r\n// AddElement ajoute un nouvel élément à l'ontologie ou met à jour un élément existant\r\nfunc (o *Ontology) AddElement(element *OntologyElement) {\r\n\texistingElement := o.GetElementByName(element.Name)\r\n\tif existingElement != nil {\r\n\t\t// Mettre à jour l'élément existant\r\n\t\texistingElement.Type = element.Type\r\n\t\texistingElement.Positions = append(existingElement.Positions, element.Positions...)\r\n\t\t// Supprimer les doublons dans les positions\r\n\t\texistingElement.Positions = removeDuplicates(existingElement.Positions)\r\n\t} else {\r\n\t\t// Ajouter un nouvel élément\r\n\t\to.Elements = append(o.Elements, element)\r\n\t}\r\n}\r\n\r\n// GetElementByName recherche un élément par son nom\r\nfunc (o *Ontology) GetElementByName(name string) *OntologyElement {\r\n\tfor _, element := range o.Elements {\r\n\t\tif element.Name == name {\r\n\t\t\treturn element\r\n\t\t}\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// LoadFromString charge une ontologie à partir d'une chaîne de caractères au format QuickStatement\r\nfunc (o *Ontology) LoadFromString(content string) error {\r\n\tlines := strings.Split(content, \"\\n\")\r\n\tfor _, line := range lines {\r\n\t\tparts := strings.Split(line, \"|\")\r\n\t\tif len(parts) != 2 {\r\n\t\t\tcontinue // Ignorer les lignes mal formatées\r\n\t\t}\r\n\t\tname := parts[0]\r\n\t\ttypeParts := strings.Split(parts[1], \"@\")\r\n\t\tif len(typeParts) != 2 {\r\n\t\t\tcontinue // Ignorer les éléments sans positions\r\n\t\t}\r\n\t\telementType := typeParts[0]\r\n\t\tpositionsStr := strings.Split(typeParts[1], \",\")\r\n\r\n\t\telement := NewOntologyElement(name, elementType)\r\n\t\tfor _, posStr := range positionsStr {\r\n\t\t\tpos, err := strconv.Atoi(posStr)\r\n\t\t\tif err != nil {\r\n\t\t\t\tcontinue // Ignorer les positions non valides\r\n\t\t\t}\r\n\t\t\telement.AddPosition(pos)\r\n\t\t}\r\n\t\to.AddElement(element)\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// ToString convertit l'ontologie en chaîne de caractères au format QuickStatement\r\nfunc (o *Ontology) ToString() string {\r\n\tvar builder strings.Builder\r\n\tfor _, element := range o.Elements {\r\n\t\tposStr := strings.Trim(strings.Join(strings.Fields(fmt.Sprint(element.Positions)), \",\"), \"[]\")\r\n\t\tbuilder.WriteString(fmt.Sprintf(\"%s|%s@%s\\n\", element.Name, element.Type, posStr))\r\n\t}\r\n\treturn builder.String()\r\n}\r\n\r\n// removeDuplicates supprime les doublons dans un slice d'entiers\r\nfunc removeDuplicates(slice []int) []int {\r\n\tkeys := make(map[int]bool)\r\n\tlist := []int{}\r\n\tfor _, entry := range slice {\r\n\t\tif _, value := keys[entry]; !value {\r\n\t\t\tkeys[entry] = true\r\n\t\t\tlist = append(list, entry)\r\n\t\t}\r\n\t}\r\n\treturn list\r\n}\r\n",
    "size": 3812,
    "modTime": "2024-10-22T10:40:49.6022691+02:00",
    "path": "internal\\ontology\\element.go"
  },
  {
    "name": "enrich.go",
    "content": "package ontology\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/chrlesur/Ontology/internal/model\"\r\n\t\"github.com/chrlesur/Ontology/internal/pipeline\"\r\n\r\n\t\"github.com/spf13/cobra\"\r\n)\r\n\r\nvar (\r\n\toutput                   string\r\n\tformat                   string\r\n\tllm                      string\r\n\tllmModel                 string\r\n\tpasses                   int\r\n\trecursive                bool\r\n\texistingOntology         string\r\n\tentityExtractionPrompt   string\r\n\trelationExtractionPrompt string\r\n\tontologyEnrichmentPrompt string\r\n\tontologyMergePrompt      string\r\n)\r\n\r\n// enrichCmd represents the enrich command\r\nvar enrichCmd = \u0026cobra.Command{\r\n\tUse:   \"enrich [input]\",\r\n\tShort: i18n.Messages.EnrichCmdShortDesc,\r\n\tLong:  i18n.Messages.EnrichCmdLongDesc,\r\n\tArgs:  cobra.ExactArgs(1),\r\n\tRunE: func(cmd *cobra.Command, args []string) error {\r\n\t\tinput := args[0]\r\n\t\tlog := logger.GetLogger()\r\n\t\tlog.Info(i18n.Messages.StartingEnrichProcess)\r\n\t\taiyouAssistantID, _ := cmd.Flags().GetString(\"aiyou-assistant-id\")\r\n\t\taiyouEmail, _ := cmd.Flags().GetString(\"aiyou-email\")\r\n\t\taiyouPassword, _ := cmd.Flags().GetString(\"aiyou-password\")\r\n\r\n\t\t// Mettre à jour la configuration si les flags sont fournis\r\n\t\tcfg := config.GetConfig() // Obtenez l'instance de configuration\r\n\t\tif aiyouAssistantID != \"\" {\r\n\t\t\tcfg.AIYOUAssistantID = aiyouAssistantID\r\n\t\t}\r\n\t\tif aiyouEmail != \"\" {\r\n\t\t\tcfg.AIYOUEmail = aiyouEmail\r\n\t\t}\r\n\t\tif aiyouPassword != \"\" {\r\n\t\t\tcfg.AIYOUPassword = aiyouPassword\r\n\t\t}\r\n\r\n\t\t// Utiliser le chemin absolu pour l'entrée\r\n\t\tvar absInput string\r\n\t\tif strings.HasPrefix(strings.ToLower(input), \"s3://\") {\r\n\t\t\tabsInput = input\r\n\t\t} else {\r\n\t\t\tvar err error\r\n\t\t\tabsInput, err = filepath.Abs(input)\r\n\t\t\tif err != nil {\r\n\t\t\t\treturn fmt.Errorf(\"error getting absolute path: %w\", err)\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\t// Déterminer le nom de fichier de sortie si non spécifié\r\n\t\tif output == \"\" {\r\n\t\t\tif strings.HasPrefix(strings.ToLower(absInput), \"s3://\") {\r\n\t\t\t\t// Pour les entrées S3, conserver le format S3 pour la sortie\r\n\t\t\t\toutput = strings.TrimSuffix(absInput, filepath.Ext(absInput)) + \".tsv\"\r\n\t\t\t} else {\r\n\t\t\t\t// Pour les entrées locales, utiliser un chemin local\r\n\t\t\t\toutput = filepath.Join(filepath.Dir(absInput), filepath.Base(absInput)+\".tsv\")\r\n\t\t\t}\r\n\t\t} else if !strings.HasPrefix(strings.ToLower(output), \"s3://\") \u0026\u0026 strings.HasPrefix(strings.ToLower(absInput), \"s3://\") {\r\n\t\t\t// Si l'entrée est S3 mais pas la sortie, convertir la sortie en format S3\r\n\t\t\toutput = strings.TrimSuffix(absInput, filepath.Ext(absInput)) + \".tsv\"\r\n\t\t}\r\n\r\n\t\tp, err := pipeline.NewPipeline(includePositions, contextOutput, contextWords, entityExtractionPrompt, relationExtractionPrompt, ontologyEnrichmentPrompt, ontologyMergePrompt, llm, llmModel, absInput)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.Messages.ErrorCreatingPipeline, err)\r\n\t\t}\r\n\r\n\t\tp.SetProgressCallback(func(info pipeline.ProgressInfo) {\r\n\t\t\tswitch info.CurrentStep {\r\n\t\t\tcase \"Starting Pass\":\r\n\t\t\t\tlog.Info(\"Starting pass %d of %d\", info.CurrentPass, info.TotalPasses)\r\n\t\t\tcase \"Segmenting\":\r\n\t\t\t\tlog.Info(\"Segmenting input into %d parts\", info.TotalSegments)\r\n\t\t\tcase \"Processing Segment\":\r\n\t\t\t\tlog.Debug(\"Processing segment %d of %d\", info.ProcessedSegments, info.TotalSegments)\r\n\t\t\t}\r\n\t\t})\r\n\r\n\t\tontology := model.NewOntology() // Utiliser le nouveau package model\r\n\r\n\t\terr = p.ExecutePipeline(absInput, output, passes, existingOntology, ontology)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.Messages.ErrorExecutingPipeline, err)\r\n\t\t}\r\n\r\n\t\tlog.Info(i18n.Messages.EnrichProcessCompleted)\r\n\t\tlog.Info(\"File processed: %s, output: %s\", absInput, output)\r\n\t\treturn nil\r\n\t},\r\n}\r\n\r\nfunc init() {\r\n\trootCmd.AddCommand(enrichCmd)\r\n\r\n\tenrichCmd.Flags().StringVar(\u0026output, \"output\", \"\", i18n.Messages.OutputFlagUsage)\r\n\tenrichCmd.Flags().StringVar(\u0026format, \"format\", \"\", i18n.Messages.FormatFlagUsage)\r\n\tenrichCmd.Flags().StringVar(\u0026llm, \"llm\", \"\", i18n.Messages.LLMFlagUsage)\r\n\tenrichCmd.Flags().StringVar(\u0026llmModel, \"llm-model\", \"\", i18n.Messages.LLMModelFlagUsage)\r\n\tenrichCmd.Flags().IntVar(\u0026passes, \"passes\", 1, i18n.Messages.PassesFlagUsage)\r\n\tenrichCmd.Flags().BoolVar(\u0026recursive, \"recursive\", false, i18n.Messages.RecursiveFlagUsage)\r\n\tenrichCmd.Flags().StringVar(\u0026existingOntology, \"existing-ontology\", \"\", i18n.Messages.ExistingOntologyFlagUsage)\r\n\r\n\tenrichCmd.Flags().StringVarP(\u0026entityExtractionPrompt, \"entity-prompt\", \"e\", \"\", \"Additional prompt for entity extraction\")\r\n\tenrichCmd.Flags().StringVarP(\u0026relationExtractionPrompt, \"relation-prompt\", \"r\", \"\", \"Additional prompt for relation extraction\")\r\n\tenrichCmd.Flags().StringVarP(\u0026ontologyEnrichmentPrompt, \"enrichment-prompt\", \"n\", \"\", \"Additional prompt for ontology enrichment\")\r\n\tenrichCmd.Flags().StringVarP(\u0026ontologyMergePrompt, \"merge-prompt\", \"m\", \"\", \"Additional prompt for ontology merging\")\r\n}\r\n\r\nfunc ExecuteEnrichCommand(input, output string, passes int, existingOntology string, includePositions, contextOutput bool, contextWords int, entityPrompt, relationPrompt, enrichmentPrompt, mergePrompt string) error {\r\n\tlog := logger.GetLogger()\r\n\tlog.Info(i18n.Messages.StartingEnrichProcess)\r\n\r\n\tabsInput := input // Garder l'input tel quel s'il est déjà en format S3\r\n\r\n\tif !strings.HasPrefix(strings.ToLower(input), \"s3://\") {\r\n\t\tvar err error\r\n\t\tabsInput, err = filepath.Abs(input)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"error getting absolute path: %w\", err)\r\n\t\t}\r\n\t}\r\n\r\n\tif output == \"\" {\r\n\t\t// Générer le nom de fichier de sortie en conservant le format S3 si l'entrée est S3\r\n\t\tif strings.HasPrefix(strings.ToLower(absInput), \"s3://\") {\r\n\t\t\t// Pour les entrées S3, construire un chemin de sortie S3\r\n\t\t\toutput = strings.TrimSuffix(absInput, filepath.Ext(absInput)) + \".tsv\"\r\n\t\t} else {\r\n\t\t\t// Pour les entrées locales, utiliser le chemin local\r\n\t\t\toutput = filepath.Join(filepath.Dir(absInput), filepath.Base(absInput)+\".tsv\")\r\n\t\t}\r\n\t} else if !strings.HasPrefix(strings.ToLower(output), \"s3://\") \u0026\u0026 strings.HasPrefix(strings.ToLower(absInput), \"s3://\") {\r\n\t\t// Si l'entrée est S3 mais pas la sortie, convertir la sortie en format S3\r\n\t\toutput = \"s3://\" + strings.TrimPrefix(filepath.ToSlash(output), \"/\")\r\n\t}\r\n\r\n\tp, err := pipeline.NewPipeline(includePositions, contextOutput, contextWords, entityPrompt, relationPrompt, enrichmentPrompt, mergePrompt, llm, llmModel, absInput)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.Messages.ErrorCreatingPipeline, err)\r\n\t}\r\n\r\n\tp.SetProgressCallback(func(info pipeline.ProgressInfo) {\r\n\t\tswitch info.CurrentStep {\r\n\t\tcase \"Starting Pass\":\r\n\t\t\tlog.Info(\"Starting pass %d of %d\", info.CurrentPass, info.TotalPasses)\r\n\t\tcase \"Segmenting\":\r\n\t\t\tlog.Info(\"Segmenting input into %d parts\", info.TotalSegments)\r\n\t\tcase \"Processing Segment\":\r\n\t\t\tlog.Debug(\"Processing segment %d of %d\", info.ProcessedSegments, info.TotalSegments)\r\n\t\t}\r\n\t})\r\n\r\n\tonto := model.NewOntology()\r\n\terr = p.ExecutePipeline(absInput, output, passes, existingOntology, onto)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.Messages.ErrorExecutingPipeline, err)\r\n\t}\r\n\r\n\tlog.Info(i18n.Messages.EnrichProcessCompleted)\r\n\tlog.Info(\"File processed: %s, output: %s\", absInput, output)\r\n\treturn nil\r\n}\r\n\r\nfunc generateOutputFilename(input string) string {\r\n\tdir := filepath.Dir(input)\r\n\tbaseName := filepath.Base(input)\r\n\tbaseName = strings.TrimSuffix(baseName, filepath.Ext(baseName))\r\n\r\n\treturn filepath.Join(dir, baseName+\".tsv\")\r\n}\r\n",
    "size": 7570,
    "modTime": "2024-11-04T21:07:16.1355413+01:00",
    "path": "internal\\ontology\\enrich.go"
  },
  {
    "name": "logger.go",
    "content": "package ontology // ou le nom du package approprié\r\n\r\nimport (\r\n    \"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()",
    "size": 150,
    "modTime": "2024-10-20T17:29:08.5359463+02:00",
    "path": "internal\\ontology\\logger.go"
  },
  {
    "name": "root.go",
    "content": "package ontology\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"os\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/spf13/cobra\"\r\n)\r\n\r\nconst Version = \"0.6.0\"\r\n\r\nvar (\r\n\tcfgFile          string\r\n\tdebug            bool\r\n\tsilent           bool\r\n\tincludePositions bool\r\n\tcontextOutput    bool\r\n\tcontextWords     int\r\n)\r\n\r\n// rootCmd represents the base command when called without any subcommands\r\nvar rootCmd = \u0026cobra.Command{\r\n\tUse:     \"ontology\",\r\n\tShort:   i18n.GetMessage(\"RootCmdShortDesc\"),\r\n\tLong:    i18n.GetMessage(\"RootCmdLongDesc\"),\r\n\tVersion: Version,\r\n}\r\n\r\nvar versionCmd = \u0026cobra.Command{\r\n\tUse:   \"version\",\r\n\tShort: \"Print the version number of Ontology\",\r\n\tLong:  `All software has versions. This is Ontology's.`,\r\n\tRun: func(cmd *cobra.Command, args []string) {\r\n\t\tfmt.Printf(\"Ontology version %s\\n\", Version)\r\n\t},\r\n}\r\n\r\n// Execute adds all child commands to the root command and sets flags appropriately.\r\n// This is called by main.main(). It only needs to happen once to the rootCmd.\r\nfunc Execute() {\r\n\terr := rootCmd.Execute()\r\n\tif err != nil {\r\n\t\tos.Exit(1)\r\n\t}\r\n}\r\n\r\nfunc init() {\r\n\tcobra.OnInitialize(initConfig)\r\n\r\n\trootCmd.PersistentFlags().StringVar(\u0026cfgFile, \"config\", \"\", i18n.GetMessage(\"ConfigFlagUsage\"))\r\n\trootCmd.PersistentFlags().BoolVarP(\u0026debug, \"debug\", \"d\", false, i18n.GetMessage(\"DebugFlagUsage\"))\r\n\trootCmd.PersistentFlags().BoolVarP(\u0026silent, \"silent\", \"s\", false, i18n.GetMessage(\"SilentFlagUsage\"))\r\n\trootCmd.PersistentFlags().BoolVarP(\u0026includePositions, \"include-positions\", \"i\", true, i18n.GetMessage(\"IncludePositionsFlagUsage\"))\r\n\trootCmd.PersistentFlags().BoolVarP(\u0026contextOutput, \"context-output\", \"c\", false, i18n.GetMessage(\"ContextOutputFlagUsage\"))\r\n\trootCmd.PersistentFlags().IntVarP(\u0026contextWords, \"context-words\", \"w\", 30, i18n.GetMessage(\"ContextWordsFlagUsage\"))\r\n\trootCmd.PersistentFlags().StringP(\"aiyou-assistant-id\", \"a\", \"\", \"AI.YOU Assistant ID\")\r\n\trootCmd.PersistentFlags().String(\"aiyou-email\", \"\", \"AI.YOU Email\")\r\n\trootCmd.PersistentFlags().String(\"aiyou-password\", \"\", \"AI.YOU Password\")\r\n\trootCmd.Run = rootCmd.HelpFunc()\r\n\r\n\trootCmd.AddCommand(versionCmd)\r\n}\r\n\r\n// initConfig reads in config file and ENV variables if set.\r\nfunc initConfig() {\r\n\tif cfgFile != \"\" {\r\n\t\t// Set the config file path if specified\r\n\t\tos.Setenv(\"ONTOLOGY_CONFIG_PATH\", cfgFile)\r\n\t}\r\n\r\n\t// This will load the config from file and environment variables\r\n\tcfg := config.GetConfig()\r\n\tcfg.ContextOutput = contextOutput\r\n\tcfg.ContextWords = contextWords\r\n\r\n\t// Validate the config\r\n\tif err := cfg.ValidateConfig(); err != nil {\r\n\t\tfmt.Printf(\"Error in configuration: %v\\n\", err)\r\n\t\tos.Exit(1)\r\n\t}\r\n\r\n\tlog := logger.GetLogger()\r\n\r\n\t// Initialize logger based on debug and silent flags\r\n\tif debug {\r\n\t\tlog.SetLevel(logger.DebugLevel)\r\n\t} else if silent {\r\n\t\tlog.SetLevel(logger.ErrorLevel)\r\n\t} else {\r\n\t\tlogLevel := logger.ParseLevel(cfg.LogLevel)\r\n\t\tlog.SetLevel(logLevel)\r\n\t}\r\n\r\n\tlog.Debug(i18n.GetMessage(\"InitializingApplication\"))\r\n}\r\n",
    "size": 3070,
    "modTime": "2024-11-04T09:47:47.215993+01:00",
    "path": "internal\\ontology\\root.go"
  },
  {
    "name": "docx.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"archive/zip\"\r\n\t\"bytes\"\r\n\t\"encoding/xml\"\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"io/ioutil\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\ntype DOCXParser struct {\r\n\tmetadata map[string]string\r\n}\r\n\r\nfunc init() {\r\n\tRegisterParser(\".docx\", NewDOCXParser)\r\n}\r\n\r\nfunc NewDOCXParser() Parser {\r\n\treturn \u0026DOCXParser{\r\n\t\tmetadata: make(map[string]string),\r\n\t}\r\n}\r\n\r\nfunc (p *DOCXParser) Parse(reader io.Reader) ([]byte, error) {\r\n\tlog.Debug(i18n.Messages.ParseStarted, \"DOCX\")\r\n\r\n\t// Lire tout le contenu en mémoire\r\n\tcontent, err := ioutil.ReadAll(reader)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.Messages.ParseFailed, \"DOCX\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to read DOCX content: %w\", err)\r\n\t}\r\n\r\n\t// Créer un lecteur zip à partir du contenu\r\n\tzipReader, err := zip.NewReader(bytes.NewReader(content), int64(len(content)))\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.Messages.ParseFailed, \"DOCX\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to create zip reader: %w\", err)\r\n\t}\r\n\r\n\tvar textContent strings.Builder\r\n\r\n\tfor _, file := range zipReader.File {\r\n\t\tswitch file.Name {\r\n\t\tcase \"word/document.xml\":\r\n\t\t\tif err := p.extractContent(file, \u0026textContent); err != nil {\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\t\tcase \"docProps/core.xml\":\r\n\t\t\tif err := p.extractMetadata(file); err != nil {\r\n\t\t\t\tlog.Warning(i18n.Messages.MetadataExtractionFailed, \"DOCX\", err)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\tlog.Info(i18n.Messages.ParseCompleted, \"DOCX\")\r\n\treturn []byte(textContent.String()), nil\r\n}\r\n\r\nfunc (p *DOCXParser) extractContent(file *zip.File, textContent *strings.Builder) error {\r\n\trc, err := file.Open()\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"failed to open document.xml: %w\", err)\r\n\t}\r\n\tdefer rc.Close()\r\n\r\n\tcontent, err := ioutil.ReadAll(rc)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"failed to read document.xml: %w\", err)\r\n\t}\r\n\r\n\tvar document struct {\r\n\t\tBody struct {\r\n\t\t\tParagraphs []struct {\r\n\t\t\t\tRuns []struct {\r\n\t\t\t\t\tText string `xml:\"t\"`\r\n\t\t\t\t} `xml:\"r\"`\r\n\t\t\t} `xml:\"p\"`\r\n\t\t}\r\n\t}\r\n\r\n\terr = xml.Unmarshal(content, \u0026document)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"failed to unmarshal document.xml: %w\", err)\r\n\t}\r\n\r\n\tfor _, paragraph := range document.Body.Paragraphs {\r\n\t\tfor _, run := range paragraph.Runs {\r\n\t\t\ttextContent.WriteString(run.Text)\r\n\t\t}\r\n\t\ttextContent.WriteString(\"\\n\")\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\nfunc (p *DOCXParser) extractMetadata(file *zip.File) error {\r\n\trc, err := file.Open()\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"failed to open core.xml: %w\", err)\r\n\t}\r\n\tdefer rc.Close()\r\n\r\n\tcontent, err := ioutil.ReadAll(rc)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"failed to read core.xml: %w\", err)\r\n\t}\r\n\r\n\tvar coreProps struct {\r\n\t\tTitle          string `xml:\"title\"`\r\n\t\tSubject        string `xml:\"subject\"`\r\n\t\tCreator        string `xml:\"creator\"`\r\n\t\tKeywords       string `xml:\"keywords\"`\r\n\t\tDescription    string `xml:\"description\"`\r\n\t\tLastModifiedBy string `xml:\"lastModifiedBy\"`\r\n\t\tRevision       string `xml:\"revision\"`\r\n\t\tCreated        string `xml:\"created\"`\r\n\t\tModified       string `xml:\"modified\"`\r\n\t}\r\n\r\n\terr = xml.Unmarshal(content, \u0026coreProps)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"failed to unmarshal core.xml: %w\", err)\r\n\t}\r\n\r\n\tp.metadata[\"format\"] = \"DOCX\"\r\n\tif coreProps.Title != \"\" {\r\n\t\tp.metadata[\"title\"] = coreProps.Title\r\n\t}\r\n\tif coreProps.Subject != \"\" {\r\n\t\tp.metadata[\"subject\"] = coreProps.Subject\r\n\t}\r\n\tif coreProps.Creator != \"\" {\r\n\t\tp.metadata[\"creator\"] = coreProps.Creator\r\n\t}\r\n\tif coreProps.Keywords != \"\" {\r\n\t\tp.metadata[\"keywords\"] = coreProps.Keywords\r\n\t}\r\n\tif coreProps.Description != \"\" {\r\n\t\tp.metadata[\"description\"] = coreProps.Description\r\n\t}\r\n\tif coreProps.LastModifiedBy != \"\" {\r\n\t\tp.metadata[\"lastModifiedBy\"] = coreProps.LastModifiedBy\r\n\t}\r\n\tif coreProps.Revision != \"\" {\r\n\t\tp.metadata[\"revision\"] = coreProps.Revision\r\n\t}\r\n\tif coreProps.Created != \"\" {\r\n\t\tp.metadata[\"created\"] = coreProps.Created\r\n\t}\r\n\tif coreProps.Modified != \"\" {\r\n\t\tp.metadata[\"modified\"] = coreProps.Modified\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\nfunc (p *DOCXParser) GetFormatMetadata() map[string]string {\r\n\treturn p.metadata\r\n}\r\n",
    "size": 4047,
    "modTime": "2024-11-05T19:29:52.4737642+01:00",
    "path": "internal\\parser\\docx.go"
  },
  {
    "name": "html.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"io/ioutil\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"golang.org/x/net/html\"\r\n)\r\n\r\ntype HTMLParser struct {\r\n\tmetadata map[string]string\r\n}\r\n\r\nfunc init() {\r\n\tRegisterParser(\".html\", NewPDFParser)\r\n}\r\n\r\nfunc NewHTMLParser() Parser {\r\n\treturn \u0026HTMLParser{\r\n\t\tmetadata: make(map[string]string),\r\n\t}\r\n}\r\n\r\nfunc (p *HTMLParser) Parse(reader io.Reader) ([]byte, error) {\r\n\tlog.Debug(i18n.Messages.ParseStarted, \"HTML\")\r\n\r\n\tcontent, err := ioutil.ReadAll(reader)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.Messages.ParseFailed, \"HTML\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to read HTML content: %w\", err)\r\n\t}\r\n\r\n\tdoc, err := html.Parse(bytes.NewReader(content))\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.Messages.ParseFailed, \"HTML\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to parse HTML: %w\", err)\r\n\t}\r\n\r\n\tvar textContent strings.Builder\r\n\tp.extractContent(doc, \u0026textContent)\r\n\tp.extractMetadata(doc)\r\n\r\n\tlog.Info(i18n.Messages.ParseCompleted, \"HTML\")\r\n\treturn []byte(textContent.String()), nil\r\n}\r\n\r\nfunc (p *HTMLParser) extractContent(n *html.Node, textContent *strings.Builder) {\r\n\tif n.Type == html.TextNode {\r\n\t\ttextContent.WriteString(strings.TrimSpace(n.Data))\r\n\t\ttextContent.WriteString(\" \")\r\n\t}\r\n\tfor c := n.FirstChild; c != nil; c = c.NextSibling {\r\n\t\tp.extractContent(c, textContent)\r\n\t}\r\n}\r\n\r\nfunc (p *HTMLParser) extractMetadata(n *html.Node) {\r\n\tp.metadata[\"format\"] = \"HTML\"\r\n\r\n\tvar extractMetaTag func(*html.Node)\r\n\textractMetaTag = func(n *html.Node) {\r\n\t\tif n.Type == html.ElementNode \u0026\u0026 n.Data == \"meta\" {\r\n\t\t\tvar name, content string\r\n\t\t\tfor _, attr := range n.Attr {\r\n\t\t\t\tswitch attr.Key {\r\n\t\t\t\tcase \"name\", \"property\":\r\n\t\t\t\t\tname = attr.Val\r\n\t\t\t\tcase \"content\":\r\n\t\t\t\t\tcontent = attr.Val\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif name != \"\" \u0026\u0026 content != \"\" {\r\n\t\t\t\tp.metadata[name] = content\r\n\t\t\t}\r\n\t\t}\r\n\t\tfor c := n.FirstChild; c != nil; c = c.NextSibling {\r\n\t\t\textractMetaTag(c)\r\n\t\t}\r\n\t}\r\n\r\n\tvar extractTitle func(*html.Node)\r\n\textractTitle = func(n *html.Node) {\r\n\t\tif n.Type == html.ElementNode \u0026\u0026 n.Data == \"title\" {\r\n\t\t\tif n.FirstChild != nil \u0026\u0026 n.FirstChild.Type == html.TextNode {\r\n\t\t\t\tp.metadata[\"title\"] = n.FirstChild.Data\r\n\t\t\t}\r\n\t\t\treturn\r\n\t\t}\r\n\t\tfor c := n.FirstChild; c != nil; c = c.NextSibling {\r\n\t\t\textractTitle(c)\r\n\t\t}\r\n\t}\r\n\r\n\textractMetaTag(n)\r\n\textractTitle(n)\r\n}\r\n\r\nfunc (p *HTMLParser) GetFormatMetadata() map[string]string {\r\n\treturn p.metadata\r\n}\r\n",
    "size": 2430,
    "modTime": "2024-11-05T19:28:39.3712305+01:00",
    "path": "internal\\parser\\html.go"
  },
  {
    "name": "logger.go",
    "content": "// internal/parser/logger.go\r\n\r\npackage parser\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()\r\n",
    "size": 144,
    "modTime": "2024-10-20T15:59:02.1149669+02:00",
    "path": "internal\\parser\\logger.go"
  },
  {
    "name": "markdown.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"bufio\"\r\n\t\"bytes\"\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"gopkg.in/yaml.v2\"\r\n)\r\n\r\ntype MarkdownParser struct {\r\n\tmetadata map[string]string\r\n}\r\n\r\nfunc init() {\r\n\tRegisterParser(\".md\", NewMarkdownParser)\r\n}\r\n\r\nfunc NewMarkdownParser() Parser {\r\n\treturn \u0026MarkdownParser{\r\n\t\tmetadata: make(map[string]string),\r\n\t}\r\n}\r\n\r\nfunc (p *MarkdownParser) Parse(reader io.Reader) ([]byte, error) {\r\n\tlog.Debug(i18n.Messages.ParseStarted, \"Markdown\")\r\n\r\n\tvar content bytes.Buffer\r\n\tscanner := bufio.NewScanner(reader)\r\n\tinFrontMatter := false\r\n\tvar frontMatter strings.Builder\r\n\tlineCount, wordCount, charCount, headerCount := 0, 0, 0, 0\r\n\r\n\tfor scanner.Scan() {\r\n\t\tline := scanner.Text()\r\n\r\n\t\tif lineCount == 0 \u0026\u0026 line == \"---\" {\r\n\t\t\tinFrontMatter = true\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tif inFrontMatter {\r\n\t\t\tif line == \"---\" {\r\n\t\t\t\tinFrontMatter = false\r\n\t\t\t\tp.parseFrontMatter(frontMatter.String())\r\n\t\t\t} else {\r\n\t\t\t\tfrontMatter.WriteString(line + \"\\n\")\r\n\t\t\t}\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tcontent.WriteString(line + \"\\n\")\r\n\t\tlineCount++\r\n\t\twordCount += len(strings.Fields(line))\r\n\t\tcharCount += len(line)\r\n\r\n\t\tif strings.HasPrefix(line, \"#\") {\r\n\t\t\theaderCount++\r\n\t\t}\r\n\t}\r\n\r\n\tif err := scanner.Err(); err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\tp.metadata[\"format\"] = \"Markdown\"\r\n\tp.metadata[\"lineCount\"] = fmt.Sprintf(\"%d\", lineCount)\r\n\tp.metadata[\"wordCount\"] = fmt.Sprintf(\"%d\", wordCount)\r\n\tp.metadata[\"charCount\"] = fmt.Sprintf(\"%d\", charCount)\r\n\tp.metadata[\"headerCount\"] = fmt.Sprintf(\"%d\", headerCount)\r\n\r\n\tlog.Info(i18n.Messages.ParseCompleted, \"Markdown\")\r\n\treturn content.Bytes(), nil\r\n}\r\n\r\nfunc (p *MarkdownParser) parseFrontMatter(frontMatter string) {\r\n\tvar fm map[string]interface{}\r\n\terr := yaml.Unmarshal([]byte(frontMatter), \u0026fm)\r\n\tif err != nil {\r\n\t\tlog.Warning(\"Failed to parse YAML front matter: %v\", err)\r\n\t\treturn\r\n\t}\r\n\r\n\tfor key, value := range fm {\r\n\t\tp.metadata[key] = fmt.Sprintf(\"%v\", value)\r\n\t}\r\n}\r\n\r\nfunc (p *MarkdownParser) GetFormatMetadata() map[string]string {\r\n\treturn p.metadata\r\n}\r\n",
    "size": 2056,
    "modTime": "2024-11-05T19:29:46.9883633+01:00",
    "path": "internal\\parser\\markdown.go"
  },
  {
    "name": "parser.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/metadata\"\r\n)\r\n\r\n// Parser définit l'interface pour tous les analyseurs de documents\r\ntype Parser interface {\r\n\tParse(reader io.Reader) ([]byte, error)\r\n\tGetFormatMetadata() map[string]string\r\n}\r\n\r\nfunc init() {\r\n\tlog.Debug(\"Registered parsers: %v\", formatParsers)\r\n}\r\n\r\n// FormatParser est une fonction qui crée un Parser spécifique à un format\r\ntype FormatParser func() Parser\r\n\r\n// formatParsers stocke les fonctions de création de Parser pour chaque format supporté\r\nvar formatParsers = make(map[string]FormatParser)\r\n\r\n// RegisterParser enregistre un nouveau parser pour un format donné\r\nfunc RegisterParser(format string, parser FormatParser) {\r\n\tformatParsers[format] = parser\r\n}\r\n\r\n// GetParser retourne le parser approprié basé sur le format spécifié\r\nfunc GetParser(format string) (Parser, error) {\r\n\tformat = strings.ToLower(format)\r\n\tif !strings.HasPrefix(format, \".\") {\r\n\t\tformat = \".\" + format\r\n\t}\r\n\tparserFunc, ok := formatParsers[format]\r\n\tif !ok {\r\n\t\treturn nil, fmt.Errorf(\"unsupported format: %s\", format)\r\n\t}\r\n\treturn parserFunc(), nil\r\n}\r\n\r\n// ParseDirectory parcourt un répertoire et parse tous les fichiers supportés\r\nfunc ParseDirectory(path string, recursive bool, metadataGen *metadata.Generator) ([][]byte, []*metadata.FileMetadata, error) {\r\n\tvar results [][]byte\r\n\tvar metadataList []*metadata.FileMetadata\r\n\r\n\terr := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {\r\n\t\tif err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\r\n\t\tif info.IsDir() {\r\n\t\t\tif !recursive \u0026\u0026 filePath != path {\r\n\t\t\t\treturn filepath.SkipDir\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t}\r\n\r\n\t\text := strings.ToLower(filepath.Ext(filePath))\r\n\t\tparser, err := GetParser(ext)\r\n\t\tif err != nil {\r\n\t\t\tlog.Warning(\"Unsupported file type: %s, skipping\", filePath)\r\n\t\t\treturn nil\r\n\t\t}\r\n\r\n\t\tfile, err := os.Open(filePath)\r\n\t\tif err != nil {\r\n\t\t\tlog.Warning(\"Failed to open file: %s, error: %v\", filePath, err)\r\n\t\t\treturn nil\r\n\t\t}\r\n\t\tdefer file.Close()\r\n\r\n\t\tcontent, err := parser.Parse(file)\r\n\t\tif err != nil {\r\n\t\t\tlog.Warning(\"Failed to parse file: %s, error: %v\", filePath, err)\r\n\t\t\treturn nil\r\n\t\t}\r\n\r\n\t\tresults = append(results, content)\r\n\r\n\t\t// Generate metadata\r\n\t\tmeta, err := metadataGen.GenerateMetadata(filePath, \"\", \"\")\r\n\t\tif err != nil {\r\n\t\t\tlog.Warning(\"Failed to generate metadata for file: %s, error: %v\", filePath, err)\r\n\t\t\treturn nil\r\n\t\t}\r\n\r\n\t\t// Add format-specific metadata\r\n\t\tfor key, value := range parser.GetFormatMetadata() {\r\n\t\t\tmeta.FormatMetadata[key] = value\r\n\t\t}\r\n\r\n\t\tmetadataList = append(metadataList, meta)\r\n\r\n\t\treturn nil\r\n\t})\r\n\r\n\tif err != nil {\r\n\t\treturn nil, nil, err\r\n\t}\r\n\r\n\treturn results, metadataList, nil\r\n}\r\n",
    "size": 2768,
    "modTime": "2024-11-05T18:01:24.8121963+01:00",
    "path": "internal\\parser\\parser.go"
  },
  {
    "name": "parser_test.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"io\"\r\n\t\"io/ioutil\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n\t\"testing\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/metadata\"\r\n\t\"github.com/chrlesur/Ontology/internal/storage\"\r\n\t\"github.com/stretchr/testify/assert\"\r\n)\r\n\r\nfunc TestPDFParser(t *testing.T) {\r\n    content := `%PDF-1.5\r\n    1 0 obj\r\n    \u003c\u003c/Type/Catalog/Pages 2 0 R\u003e\u003e\r\n    endobj\r\n    2 0 obj\r\n    \u003c\u003c/Type/Pages/Kids[3 0 R]/Count 1\u003e\u003e\r\n    endobj\r\n    3 0 obj\r\n    \u003c\u003c/Type/Page/Parent 2 0 R/MediaBox[0 0 612 792]/Resources\u003c\u003c\u003e\u003e/Contents 4 0 R\u003e\u003e\r\n    endobj\r\n    4 0 obj\r\n    \u003c\u003c/Length 51\u003e\u003e\r\n    stream\r\n    BT\r\n    /F1 12 Tf\r\n    100 700 Td\r\n    (Test PDF Content) Tj\r\n    ET\r\n    endstream\r\n    endobj\r\n    xref\r\n    0 5\r\n    0000000000 65535 f\r\n    0000000009 00000 n\r\n    0000000052 00000 n\r\n    0000000101 00000 n\r\n    0000000192 00000 n\r\n    trailer\r\n    \u003c\u003c/Size 5/Root 1 0 R/Info\u003c\u003c/Title(Test PDF)/Author(Test Author)\u003e\u003e\u003e\u003e\r\n    startxref\r\n    291\r\n    %%EOF`\r\n\r\n\tparser := NewPDFParser()\r\n\tresult, err := parser.Parse(bytes.NewReader([]byte(content)))\r\n\r\n\tassert.NoError(t, err)\r\n\tassert.Contains(t, string(result), \"Test PDF\")\r\n\r\n\tmetadata := parser.GetFormatMetadata()\r\n\tassert.Equal(t, \"PDF\", metadata[\"format\"])\r\n\tassert.Equal(t, \"Test PDF\", metadata[\"title\"])\r\n\tassert.Equal(t, \"Test Author\", metadata[\"author\"])\r\n}\r\n\r\nfunc TestDOCXParser(t *testing.T) {\r\n\t// This is a simplified DOCX content for testing purposes\r\n    content := `\r\n    \u003c?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?\u003e\r\n    \u003cw:document xmlns:w=\"http://schemas.openxmlformats.org/wordprocessingml/2006/main\"\u003e\r\n      \u003cw:body\u003e\r\n        \u003cw:p\u003e\r\n          \u003cw:r\u003e\r\n            \u003cw:t\u003eThis is a test DOCX document.\u003c/w:t\u003e\r\n          \u003c/w:r\u003e\r\n        \u003c/w:p\u003e\r\n      \u003c/w:body\u003e\r\n    \u003c/w:document\u003e`\r\n\r\n\tparser := NewDOCXParser()\r\n\tresult, err := parser.Parse(bytes.NewReader([]byte(content)))\r\n\r\n\tassert.NoError(t, err)\r\n\tassert.Contains(t, string(result), \"This is a test DOCX document.\")\r\n\r\n\tmetadata := parser.GetFormatMetadata()\r\n\tassert.Equal(t, \"DOCX\", metadata[\"format\"])\r\n\tassert.Equal(t, \"Test DOCX\", metadata[\"title\"])\r\n\tassert.Equal(t, \"Test Author\", metadata[\"creator\"])\r\n}\r\n\r\nfunc TestHTMLParser(t *testing.T) {\r\n\tcontent := `\r\n\u003c!DOCTYPE html\u003e\r\n\u003chtml\u003e\r\n\u003chead\u003e\r\n    \u003ctitle\u003eTest HTML\u003c/title\u003e\r\n    \u003cmeta name=\"author\" content=\"Test Author\"\u003e\r\n\u003c/head\u003e\r\n\u003cbody\u003e\r\n    \u003ch1\u003eThis is a test HTML document.\u003c/h1\u003e\r\n\u003c/body\u003e\r\n\u003c/html\u003e`\r\n\r\n\tparser := NewHTMLParser()\r\n\tresult, err := parser.Parse(strings.NewReader(content))\r\n\r\n\tassert.NoError(t, err)\r\n\tassert.Contains(t, string(result), \"This is a test HTML document.\")\r\n\r\n\tmetadata := parser.GetFormatMetadata()\r\n\tassert.Equal(t, \"HTML\", metadata[\"format\"])\r\n\tassert.Equal(t, \"Test HTML\", metadata[\"title\"])\r\n\tassert.Equal(t, \"Test Author\", metadata[\"author\"])\r\n}\r\n\r\nfunc TestTextParser(t *testing.T) {\r\n\tcontent := \"This is a test text document.\\nIt has multiple lines.\\nAnd some words.\"\r\n\r\n\tparser := NewTextParser()\r\n\tresult, err := parser.Parse(strings.NewReader(content))\r\n\r\n\tassert.NoError(t, err)\r\n\tassert.Equal(t, content, string(result))\r\n\r\n\tmetadata := parser.GetFormatMetadata()\r\n\tassert.Equal(t, \"Text\", metadata[\"format\"])\r\n\tassert.Equal(t, \"3\", metadata[\"lineCount\"])\r\n\tassert.Equal(t, \"13\", metadata[\"wordCount\"])\r\n\tassert.Equal(t, \"69\", metadata[\"charCount\"])\r\n    assert.Equal(t, content+\"\\n\", string(result))\r\n}\r\n\r\nfunc TestMarkdownParser(t *testing.T) {\r\n\tcontent := `---\r\ntitle: Test Markdown\r\nauthor: Test Author\r\n---\r\n\r\n# This is a test Markdown document\r\n\r\nIt has some content and metadata.`\r\n\r\n\tparser := NewMarkdownParser()\r\n\tresult, err := parser.Parse(strings.NewReader(content))\r\n\r\n\tassert.NoError(t, err)\r\n\tassert.Contains(t, string(result), \"This is a test Markdown document\")\r\n\r\n\tmetadata := parser.GetFormatMetadata()\r\n\tassert.Equal(t, \"Markdown\", metadata[\"format\"])\r\n\tassert.Equal(t, \"Test Markdown\", metadata[\"title\"])\r\n\tassert.Equal(t, \"Test Author\", metadata[\"author\"])\r\n\tassert.Equal(t, \"3\", metadata[\"lineCount\"])  // Excluding front matter\r\n\tassert.Equal(t, \"11\", metadata[\"wordCount\"]) // Excluding front matter\r\n\tassert.Equal(t, \"1\", metadata[\"headerCount\"])\r\n}\r\n\r\nfunc TestParseDirectory(t *testing.T) {\r\n\t// Create a temporary directory for testing\r\n\ttempDir, err := ioutil.TempDir(\"\", \"parser_test\")\r\n\tassert.NoError(t, err)\r\n\tdefer os.RemoveAll(tempDir)\r\n\r\n\t// Create test files\r\n\tfiles := map[string]string{\r\n\t\t\"test.txt\":  \"This is a text file.\",\r\n\t\t\"test.md\":   \"# This is a Markdown file\",\r\n\t\t\"test.html\": `\u003c!DOCTYPE html\u003e\u003chtml\u003e\u003cbody\u003eThis is an HTML file\u003c/body\u003e\u003c/html\u003e`,\r\n\t}\r\n\r\n\tfor name, content := range files {\r\n\t\terr = ioutil.WriteFile(filepath.Join(tempDir, name), []byte(content), 0644)\r\n\t\tassert.NoError(t, err)\r\n\t}\r\n\r\n\t// Create a metadata generator\r\n\tstorage := \u0026mockStorage{} // You need to implement a mock storage\r\n\tmetadataGen := metadata.NewGenerator(storage)\r\n\r\n\t// Parse the directory\r\n\tresults, metadataList, err := ParseDirectory(tempDir, false, metadataGen)\r\n\tassert.NoError(t, err)\r\n\tassert.Len(t, results, 3)\r\n\tassert.Len(t, metadataList, 3)\r\n\r\n\t// Check if all files were parsed\r\n\tfor _, result := range results {\r\n\t\tassert.Contains(t, []string{\r\n\t\t\t\"This is a text file.\",\r\n\t\t\t\"# This is a Markdown file\",\r\n\t\t\t\"This is an HTML file\",\r\n\t\t}, strings.TrimSpace(string(result)))\r\n\t}\r\n\r\n\t// Check metadata\r\n\tfor _, meta := range metadataList {\r\n\t\tassert.NotEmpty(t, meta.SourceFile)\r\n\t\tassert.NotEmpty(t, meta.SHA256Hash)\r\n\t}\r\n}\r\n\r\n// mockStorage is a mock implementation of the storage.Storage interface\r\ntype mockStorage struct{}\r\n\r\nfunc (m *mockStorage) Read(path string) ([]byte, error) {\r\n\treturn ioutil.ReadFile(path)\r\n}\r\n\r\nfunc (m *mockStorage) Write(path string, data []byte) error {\r\n\treturn ioutil.WriteFile(path, data, 0644)\r\n}\r\n\r\nfunc (m *mockStorage) List(prefix string) ([]string, error) {\r\n\treturn nil, nil\r\n}\r\n\r\nfunc (m *mockStorage) Delete(path string) error {\r\n\treturn os.Remove(path)\r\n}\r\n\r\nfunc (m *mockStorage) Exists(path string) (bool, error) {\r\n\t_, err := os.Stat(path)\r\n\tif os.IsNotExist(err) {\r\n\t\treturn false, nil\r\n\t}\r\n\treturn err == nil, err\r\n}\r\n\r\nfunc (m *mockStorage) IsDirectory(path string) (bool, error) {\r\n\tinfo, err := os.Stat(path)\r\n\tif err != nil {\r\n\t\treturn false, err\r\n\t}\r\n\treturn info.IsDir(), nil\r\n}\r\n\r\nfunc (m *mockStorage) GetReader(path string) (io.ReadCloser, error) {\r\n\treturn os.Open(path)\r\n}\r\n\r\nfunc (m *mockStorage) Stat(path string) (storage.FileInfo, error) {\r\n\tinfo, err := os.Stat(path)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\treturn \u0026mockFileInfo{info}, nil\r\n}\r\n\r\ntype mockFileInfo struct {\r\n\tos.FileInfo\r\n}\r\n\r\nfunc (mfi *mockFileInfo) ETag() string {\r\n\treturn \"mock-etag\"\r\n}\r\n",
    "size": 6609,
    "modTime": "2024-11-05T17:25:30.9228413+01:00",
    "path": "internal\\parser\\parser_test.go"
  },
  {
    "name": "pdf.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"io/ioutil\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/ledongthuc/pdf\"\r\n)\r\n\r\ntype PDFParser struct {\r\n\tmetadata map[string]string\r\n}\r\n\r\nfunc init() {\r\n\tRegisterParser(\".pdf\", NewPDFParser)\r\n}\r\n\r\nfunc NewPDFParser() Parser {\r\n\treturn \u0026PDFParser{\r\n\t\tmetadata: make(map[string]string),\r\n\t}\r\n}\r\n\r\nfunc (p *PDFParser) Parse(reader io.Reader) ([]byte, error) {\r\n\tlog.Debug(i18n.Messages.ParseStarted, \"PDF\")\r\n\r\n\tcontent, err := ioutil.ReadAll(reader)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.Messages.ParseFailed, \"PDF\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to read PDF content: %w\", err)\r\n\t}\r\n\r\n\tpdfReader, err := pdf.NewReader(bytes.NewReader(content), int64(len(content)))\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.Messages.ParseFailed, \"PDF\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to create PDF reader: %w\", err)\r\n\t}\r\n\r\n\tvar textContent bytes.Buffer\r\n\tfor i := 1; i \u003c= pdfReader.NumPage(); i++ {\r\n\t\tpage := pdfReader.Page(i)\r\n\t\tif page.V.IsNull() {\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\ttext, err := page.GetPlainText(nil)\r\n\t\tif err != nil {\r\n\t\t\tlog.Warning(\"Failed to extract text from page %d: %v\", i, err)\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\ttextContent.WriteString(text)\r\n\t}\r\n\r\n\tp.extractMetadata(pdfReader)\r\n\r\n\tlog.Info(i18n.Messages.ParseCompleted, \"PDF\")\r\n\treturn textContent.Bytes(), nil\r\n}\r\n\r\nfunc (p *PDFParser) extractMetadata(pdfReader *pdf.Reader) {\r\n\tp.metadata[\"format\"] = \"PDF\"\r\n\tp.metadata[\"pageCount\"] = fmt.Sprintf(\"%d\", pdfReader.NumPage())\r\n\r\n\tinfo := pdfReader.Trailer().Key(\"Info\")\r\n\tif info.IsNull() {\r\n\t\treturn\r\n\t}\r\n\r\n\tmetadataFields := []struct {\r\n\t\tkey      string\r\n\t\tpdfField string\r\n\t}{\r\n\t\t{\"title\", \"Title\"},\r\n\t\t{\"author\", \"Author\"},\r\n\t\t{\"subject\", \"Subject\"},\r\n\t\t{\"keywords\", \"Keywords\"},\r\n\t\t{\"creator\", \"Creator\"},\r\n\t\t{\"producer\", \"Producer\"},\r\n\t\t{\"creationDate\", \"CreationDate\"},\r\n\t\t{\"modificationDate\", \"ModDate\"},\r\n\t}\r\n\r\n\tfor _, field := range metadataFields {\r\n\t\tvalue := info.Key(field.pdfField)\r\n\t\tif !value.IsNull() {\r\n\t\t\tp.metadata[field.key] = value.String()\r\n\t\t}\r\n\t}\r\n}\r\n\r\nfunc (p *PDFParser) GetFormatMetadata() map[string]string {\r\n\treturn p.metadata\r\n}\r\n",
    "size": 2131,
    "modTime": "2024-11-05T18:03:11.2154912+01:00",
    "path": "internal\\parser\\pdf.go"
  },
  {
    "name": "text.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"bufio\"\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"strings\"\r\n\t\"unicode/utf8\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\ntype TextParser struct {\r\n\tmetadata map[string]string\r\n}\r\n\r\nfunc init() {\r\n\tRegisterParser(\".txt\", NewTextParser)\r\n}\r\n\r\nfunc NewTextParser() Parser {\r\n\treturn \u0026TextParser{\r\n\t\tmetadata: make(map[string]string),\r\n\t}\r\n}\r\n\r\nfunc (p *TextParser) Parse(reader io.Reader) ([]byte, error) {\r\n\tlog.Debug(i18n.Messages.ParseStarted, \"Text\")\r\n\r\n\tvar content strings.Builder\r\n\tscanner := bufio.NewScanner(reader)\r\n\tlineCount := 0\r\n\twordCount := 0\r\n\tcharCount := 0\r\n\r\n\tfor scanner.Scan() {\r\n\t\tline := scanner.Text()\r\n\t\tcontent.WriteString(line)\r\n\t\tcontent.WriteString(\"\\n\")\r\n\r\n\t\tlineCount++\r\n\t\twords := strings.Fields(line)\r\n\t\twordCount += len(words)\r\n\t\tcharCount += utf8.RuneCountInString(line)\r\n\t}\r\n\r\n\tif err := scanner.Err(); err != nil {\r\n\t\tlog.Error(i18n.Messages.ParseFailed, \"Text\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to read text content: %w\", err)\r\n\t}\r\n\r\n\tp.extractMetadata(lineCount, wordCount, charCount)\r\n\r\n\tlog.Info(i18n.Messages.ParseCompleted, \"Text\")\r\n\treturn []byte(content.String()), nil\r\n}\r\n\r\nfunc (p *TextParser) extractMetadata(lineCount, wordCount, charCount int) {\r\n\tp.metadata[\"format\"] = \"Text\"\r\n\tp.metadata[\"lineCount\"] = fmt.Sprintf(\"%d\", lineCount)\r\n\tp.metadata[\"wordCount\"] = fmt.Sprintf(\"%d\", wordCount)\r\n\tp.metadata[\"charCount\"] = fmt.Sprintf(\"%d\", charCount)\r\n}\r\n\r\nfunc (p *TextParser) GetFormatMetadata() map[string]string {\r\n\treturn p.metadata\r\n}\r\n",
    "size": 1509,
    "modTime": "2024-11-05T19:29:59.6976535+01:00",
    "path": "internal\\parser\\text.go"
  },
  {
    "name": "context.go",
    "content": "package pipeline\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()\r\n\r\n// ContextEntry représente le contexte pour une position spécifique dans le document\r\ntype ContextEntry struct {\r\n\tPosition int      `json:\"position\"`\r\n\tBefore   []string `json:\"before\"`\r\n\tAfter    []string `json:\"after\"`\r\n\tElement  string   `json:\"element\"`\r\n\tLength   int      `json:\"length\"`\r\n}\r\n",
    "size": 417,
    "modTime": "2024-11-04T15:05:49.6047546+01:00",
    "path": "internal\\pipeline\\context.go"
  },
  {
    "name": "context_generation.go",
    "content": "// context_generation.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"strings\"\r\n)\r\n\r\n// GenerateContextJSON génère un JSON contenant le contexte pour chaque position donnée\r\nfunc GenerateContextJSON(content []byte, positions []int, contextWords int, positionRanges []PositionRange) (string, error) {\r\n\tlog.Debug(\"Starting GenerateContextJSON\")\r\n\tlog.Debug(\"Number of positions: %d, Context words: %d\", len(positions), contextWords)\r\n\r\n\twords := strings.Fields(string(content))\r\n\tlog.Debug(\"Total words in content: %d\", len(words))\r\n\r\n\tentries := make([]ContextEntry, 0, len(positions))\r\n\r\n\tfor i, pos := range positions {\r\n\t\t//log.Debug(\"Processing position: %d\", pos)\r\n\r\n\t\tif pos \u003c 0 || pos \u003e= len(words) {\r\n\t\t\tlog.Warning(\"Invalid position %d. Skipping.\", pos)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tstart := max(0, pos-contextWords)\r\n\t\tbefore := words[start:pos]\r\n\r\n\t\tend := min(len(words), pos+contextWords+1)\r\n\t\tvar after []string\r\n\t\tif pos+1 \u003c len(words) {\r\n\t\t\tafter = words[pos+1 : end]\r\n\t\t}\r\n\r\n\t\tentry := ContextEntry{\r\n\t\t\tPosition: pos,\r\n\t\t\tBefore:   before,\r\n\t\t\tAfter:    after,\r\n\t\t\tElement:  positionRanges[i].Element,\r\n\t\t\tLength:   positionRanges[i].End - positionRanges[i].Start + 1,\r\n\t\t}\r\n\r\n\t\tentries = append(entries, entry)\r\n\t}\r\n\r\n\tlog.Debug(\"Number of context entries generated: %d\", len(entries))\r\n\r\n\t// Utiliser un encoder JSON personnalisé\r\n\tvar buf strings.Builder\r\n\tencoder := json.NewEncoder(\u0026buf)\r\n\tencoder.SetEscapeHTML(false)\r\n\tencoder.SetIndent(\"\", \"  \") // Deux espaces pour l'indentation\r\n\r\n\tif err := encoder.Encode(entries); err != nil {\r\n\t\tlog.Error(\"Error marshaling context JSON: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error marshaling context JSON: %w\", err)\r\n\t}\r\n\r\n\t// Appliquer le remplacement sur le JSON généré\r\n\toutput := strings.ReplaceAll(buf.String(), \"\\n\", \"\")\r\n\toutput = strings.ReplaceAll(output, \"  \", \"\")\r\n\r\n\tlog.Debug(\"JSON data generated successfully. Length: %d bytes\", len(output))\r\n\r\n\treturn output, nil\r\n}\r\n\r\n// getContextWords récupère les mots de contexte avant et après une position donnée\r\nfunc getContextWords(words []string, position, contextSize int) ([]string, []string) {\r\n\tstart := max(0, position-contextSize)\r\n\tend := min(len(words), position+contextSize+1)\r\n\r\n\tvar before, after []string\r\n\tif position \u003e start {\r\n\t\tbefore = words[start:position]\r\n\t}\r\n\tif position+1 \u003c end {\r\n\t\tafter = words[position+1 : end]\r\n\t}\r\n\r\n\treturn before, after\r\n}\r\n\r\n// formatContextJSON formate le JSON de contexte pour une meilleure lisibilité\r\n\r\nfunc formatContextJSON(jsonString string) string {\r\n\tvar buf bytes.Buffer\r\n\terr := json.Indent(\u0026buf, []byte(jsonString), \"\", \"  \")\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to format JSON: %v\", err)\r\n\t\treturn jsonString // Retourne le JSON non formaté en cas d'erreur\r\n\t}\r\n\treturn buf.String()\r\n}\r\n",
    "size": 2808,
    "modTime": "2024-11-06T00:27:07.4765509+01:00",
    "path": "internal\\pipeline\\context_generation.go"
  },
  {
    "name": "enrichment.go",
    "content": "// enrichment.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n    \"fmt\"\r\n    \"strings\"\r\n\r\n    \"github.com/chrlesur/Ontology/internal/model\"\r\n    \"github.com/chrlesur/Ontology/internal/prompt\"\r\n)\r\n\r\n// processSegment traite un segment individuel du contenu\r\nfunc (p *Pipeline) processSegment(segment []byte, context string, previousResult string, positionIndex map[string][]int, includePositions bool) (string, error) {\r\n    log.Debug(\"Processing segment of length %d, context length %d, previous result length %d\", len(segment), len(context), len(previousResult))\r\n    log.Debug(\"Segment content preview: %s\", truncateString(string(segment), 200))\r\n    log.Debug(\"Context preview: %s\", truncateString(context, 200))\r\n\r\n    enrichmentValues := map[string]string{\r\n        \"text\":              string(segment),\r\n        \"context\":           context,\r\n        \"previous_result\":   previousResult,\r\n        \"additional_prompt\": p.ontologyEnrichmentPrompt,\r\n    }\r\n    log.Debug(\"Calling LLM with OntologyEnrichmentPrompt\")\r\n\r\n    enrichedResult, err := p.llm.ProcessWithPrompt(prompt.OntologyEnrichmentPrompt, enrichmentValues)\r\n    if err != nil {\r\n        log.Error(\"Ontology enrichment failed: %v\", err)\r\n        return \"\", fmt.Errorf(\"ontology enrichment failed: %w\", err)\r\n    }\r\n\r\n    // Normaliser le résultat enrichi\r\n    normalizedResult := normalizeTSV(enrichedResult)\r\n\r\n    log.Debug(\"Enriched result length: %d, preview: %s\", len(normalizedResult), truncateString(normalizedResult, 100))\r\n\r\n    p.enrichOntologyWithPositions(normalizedResult, positionIndex, includePositions, string(segment))\r\n\r\n    return normalizedResult, nil\r\n}\r\n\r\n// enrichOntologyWithPositions enrichit l'ontologie avec les positions des éléments\r\nfunc (p *Pipeline) enrichOntologyWithPositions(enrichedResult string, positionIndex map[string][]int, includePositions bool, content string) {\r\n    log.Debug(\"Starting enrichOntologyWithPositions\")\r\n    log.Debug(\"Include positions: %v\", includePositions)\r\n    log.Debug(\"Position index size: %d\", len(positionIndex))\r\n\r\n    lines := strings.Split(enrichedResult, \"\\n\")\r\n    log.Debug(\"Number of lines to process: %d\", len(lines))\r\n\r\n    for i, line := range lines {\r\n        log.Debug(\"Processing line %d: %s\", i, line)\r\n        parts := strings.Fields(line)\r\n        if len(parts) \u003e= 3 {\r\n            name := parts[0]\r\n            elementType := parts[1]\r\n            description := strings.Join(parts[2:], \" \")\r\n\r\n            element := p.ontology.GetElementByName(name)\r\n            if element == nil {\r\n                element = model.NewOntologyElement(name, elementType)\r\n                p.ontology.AddElement(element)\r\n                log.Debug(\"Added new element: %v\", element)\r\n            } else {\r\n                log.Debug(\"Updated existing element: %v\", element)\r\n            }\r\n            element.Description = description\r\n\r\n            if includePositions {\r\n                log.Debug(\"Searching for positions of entity: %s\", name)\r\n                allPositions := p.findPositions(name, positionIndex, content)\r\n                log.Debug(\"Found %d positions for entity %s: %v\", len(allPositions), name, allPositions)\r\n                if len(allPositions) \u003e 0 {\r\n                    uniquePos := uniquePositions(allPositions)\r\n                    element.SetPositions(uniquePos)\r\n                    log.Debug(\"Set %d unique positions for element %s: %v\", len(uniquePos), name, uniquePos)\r\n                } else {\r\n                    log.Debug(\"No positions found for element %s\", name)\r\n                }\r\n            }\r\n\r\n            if len(parts) \u003e= 4 { // C'est une relation\r\n                log.Debug(\"Processing relation: %v\", parts)\r\n                source := parts[0]\r\n                relationType := parts[1]\r\n                target := parts[2]\r\n                relationDescription := strings.Join(parts[3:], \" \")\r\n                relation := \u0026model.Relation{\r\n                    Source:      source,\r\n                    Type:        relationType,\r\n                    Target:      target,\r\n                    Description: relationDescription,\r\n                }\r\n                p.ontology.AddRelation(relation)\r\n                log.Info(\"Added new relation: %v\", relation)\r\n            }\r\n        } else {\r\n            log.Debug(\"Skipping invalid line: %s\", line)\r\n        }\r\n    }\r\n\r\n    log.Debug(\"Ontology after enrichment:\")\r\n    for _, element := range p.ontology.Elements {\r\n        log.Debug(\"Element: %s, Type: %s, Description: %s, Positions: %v\",\r\n            element.Name, element.Type, element.Description, element.Positions)\r\n    }\r\n    log.Debug(\"Final ontology state - Elements: %d, Relations: %d\",\r\n        len(p.ontology.Elements), len(p.ontology.Relations))\r\n}\r\n\r\n// uniquePositions supprime les doublons dans une slice d'entiers\r\nfunc uniquePositions(positions []int) []int {\r\n    keys := make(map[int]bool)\r\n    list := []int{}\r\n    for _, entry := range positions {\r\n        if _, value := keys[entry]; !value {\r\n            keys[entry] = true\r\n            list = append(list, entry)\r\n        }\r\n    }\r\n    return list\r\n}\r\n\r\n// normalizeTSV normalise une chaîne TSV\r\nfunc normalizeTSV(input string) string {\r\n    lines := strings.Split(input, \"\\n\")\r\n    var normalizedLines []string\r\n    for _, line := range lines {\r\n        line = strings.ReplaceAll(line, \"\\\\t\", \" \")\r\n        line = strings.ReplaceAll(line, \"\\t\", \" \")\r\n        fields := strings.Fields(line)\r\n        if len(fields) \u003e= 3 {\r\n            normalizedLine := strings.Join(fields[:2], \"\\t\") + \"\\t\" + strings.Join(fields[2:], \" \")\r\n            normalizedLines = append(normalizedLines, normalizedLine)\r\n        }\r\n    }\r\n    return strings.Join(normalizedLines, \"\\n\")\r\n}",
    "size": 5707,
    "modTime": "2024-11-06T00:09:50.6936364+01:00",
    "path": "internal\\pipeline\\enrichment.go"
  },
  {
    "name": "input_processing.go",
    "content": "// input_processing.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"fmt\"\r\n\t\"path/filepath\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/parser\"\r\n\t\"github.com/chrlesur/Ontology/internal/storage\"\r\n)\r\n\r\n// parseInput traite le fichier ou le répertoire d'entrée\r\nfunc (p *Pipeline) parseInput(input string) ([]byte, error) {\r\n\tp.logger.Debug(\"Starting parseInput for: %s\", input)\r\n\tif p.storage == nil {\r\n\t\treturn nil, fmt.Errorf(\"storage is not initialized\")\r\n\t}\r\n\r\n\tstorageType := storage.DetectStorageType(input)\r\n\tp.logger.Debug(\"Detected storage type: %s for input: %s\", storageType, input)\r\n\r\n\tswitch storageType {\r\n\tcase storage.S3StorageType:\r\n\t\ts3Storage, ok := p.storage.(*storage.S3Storage)\r\n\t\tif !ok {\r\n\t\t\treturn nil, fmt.Errorf(\"S3 storage is not initialized\")\r\n\t\t}\r\n\t\tbucket, key, err := storage.ParseS3URI(input)\r\n\t\tif err != nil {\r\n\t\t\treturn nil, fmt.Errorf(\"failed to parse S3 URI: %w\", err)\r\n\t\t}\r\n\t\treturn s3Storage.ReadFromBucket(bucket, key)\r\n\tcase storage.LocalStorageType:\r\n\t\treturn p.storage.Read(input)\r\n\tdefault:\r\n\t\treturn nil, fmt.Errorf(\"unsupported storage type: %s\", storageType)\r\n\t}\r\n}\r\n\r\n// parseDirectory traite récursivement un répertoire d'entrée\r\nfunc (p *Pipeline) parseDirectory(dir string) ([]byte, error) {\r\n\tp.logger.Debug(\"Starting parseDirectory for: %s\", dir)\r\n\r\n\tvar content []byte\r\n\tfiles, err := p.storage.List(dir)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Error listing directory contents: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrParseDirectory\"), err)\r\n\t}\r\n\r\n\tfor _, file := range files {\r\n\t\tfilePath := filepath.Join(dir, file)\r\n\t\tisDir, err := p.storage.IsDirectory(filePath)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Warning(\"Error checking if path is directory: %v\", err)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tif !isDir {\r\n\t\t\text := filepath.Ext(file)\r\n\t\t\tparser, err := parser.GetParser(ext)\r\n\t\t\tif err != nil {\r\n\t\t\t\tp.logger.Warning(\"Unsupported file format for %s: %v\", file, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\r\n\t\t\tfileContent, err := p.storage.Read(filePath)\r\n\t\t\tif err != nil {\r\n\t\t\t\tp.logger.Warning(\"Error reading file %s: %v\", file, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\r\n\t\t\t// Créer un io.Reader à partir du contenu du fichier\r\n\t\t\treader := bytes.NewReader(fileContent)\r\n\r\n\t\t\tparsed, err := parser.Parse(reader)\r\n\t\t\tif err != nil {\r\n\t\t\t\tp.logger.Warning(\"Error parsing file %s: %v\", file, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\r\n\t\t\tcontent = append(content, parsed...)\r\n\t\t\tp.logger.Debug(\"Parsed file %s, content length: %d bytes\", file, len(parsed))\r\n\t\t}\r\n\t}\r\n\r\n\tp.logger.Debug(\"Finished parsing directory, total content length: %d bytes\", len(content))\r\n\treturn content, nil\r\n}\r\n\r\n// loadExistingOntology charge une ontologie existante à partir d'un fichier\r\nfunc (p *Pipeline) loadExistingOntology(path string) (string, error) {\r\n\tp.logger.Debug(\"Loading existing ontology from: %s\", path)\r\n\r\n\tcontent, err := p.storage.Read(path)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Error reading existing ontology: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrReadExistingOntology\"), err)\r\n\t}\r\n\r\n\tp.logger.Debug(\"Successfully loaded existing ontology, content length: %d bytes\", len(content))\r\n\treturn string(content), nil\r\n}\r\n",
    "size": 3211,
    "modTime": "2024-11-05T17:30:20.4339004+01:00",
    "path": "internal\\pipeline\\input_processing.go"
  },
  {
    "name": "output_generation.go",
    "content": "// output_generation.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/metadata\"\r\n)\r\n\r\n// saveResult sauvegarde les résultats de l'ontologie et génère les fichiers de sortie\r\nfunc (p *Pipeline) saveResult(result string, outputPath string, newContent []byte) error {\r\n\tp.logger.Debug(\"Starting saveResult\")\r\n\tp.logger.Info(\"Number of elements in ontology: %d\", len(p.ontology.Elements))\r\n\tp.logger.Info(\"Number of relations in ontology: %d\", len(p.ontology.Relations))\r\n\tp.logger.Debug(\"Writing TSV to: %s\", outputPath)\r\n\r\n\t// Générer le contenu TSV\r\n\ttsvContent := p.generateTSVContent()\r\n\r\n\t// Sauvegarder le fichier TSV\r\n\terr := p.storage.Write(outputPath, []byte(tsvContent))\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to write TSV file: %v\", err)\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrWriteOutput\"), err)\r\n\t}\r\n\tp.logger.Debug(\"TSV file written: %s\", outputPath)\r\n\r\n\tif p.contextOutput {\r\n\t\tcontextFile := strings.TrimSuffix(outputPath, filepath.Ext(outputPath)) + \"_context.json\"\r\n\r\n\t\twords := strings.Fields(string(newContent))\r\n\t\tpositionRanges := p.getAllPositionsFromNewContent(words)\r\n\t\tmergedPositions := mergeOverlappingPositions(positionRanges)\r\n\r\n\t\tpositions := make([]int, len(mergedPositions))\r\n\t\tfor i, pr := range mergedPositions {\r\n\t\t\tpositions[i] = pr.Start\r\n\t\t}\r\n\r\n\t\tcontextJSON, err := GenerateContextJSON(newContent, positions, p.contextWords, mergedPositions)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Error(\"Failed to generate context JSON: %v\", err)\r\n\t\t\treturn fmt.Errorf(\"failed to generate context JSON: %w\", err)\r\n\t\t}\r\n\r\n\t\terr = p.storage.Write(contextFile, []byte(contextJSON))\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Error(\"Failed to write context JSON file: %v\", err)\r\n\t\t\treturn fmt.Errorf(\"failed to write context JSON file: %w\", err)\r\n\t\t}\r\n\t\tp.logger.Info(\"Context JSON saved to: %s\", contextFile)\r\n\t} else {\r\n\t\tp.logger.Debug(\"Context output is disabled. Skipping context JSON generation.\")\r\n\t}\r\n\r\n\t// Générer et sauvegarder les métadonnées\r\n\tmetadataGen := metadata.NewGenerator(p.storage)\r\n\tontologyFile := filepath.Base(outputPath)\r\n\tmeta, err := metadataGen.GenerateMetadata(p.inputPath, ontologyFile, outputPath)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to generate metadata: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to generate metadata: %w\", err)\r\n\t}\r\n\r\n\tmetaContent, err := json.MarshalIndent(meta, \"\", \"  \")\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to marshal metadata: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to marshal metadata: %w\", err)\r\n\t}\r\n\r\n\tmetaFilePath := strings.TrimSuffix(outputPath, filepath.Ext(outputPath)) + \"_meta.json\"\r\n\r\n\terr = p.storage.Write(metaFilePath, metaContent)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to save metadata: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to save metadata: %w\", err)\r\n\t}\r\n\r\n\tp.logger.Info(\"Metadata saved to: %s\", metaFilePath)\r\n\tp.logger.Debug(\"Finished saveResult\")\r\n\treturn nil\r\n}\r\n\r\n// generateTSVContent génère le contenu TSV à partir de l'ontologie\r\nfunc (p *Pipeline) generateTSVContent() string {\r\n\tvar tsvBuilder strings.Builder\r\n\r\n\tp.logger.Debug(\"Generating TSV content\")\r\n\r\n\t// Écrire les éléments\r\n\tfor _, element := range p.ontology.Elements {\r\n\t\tpositions := strings.Trim(strings.Join(strings.Fields(fmt.Sprint(element.Positions)), \",\"), \"[]\")\r\n\t\tline := fmt.Sprintf(\"%s\\t%s\\t%s\\t%s\\n\", element.Name, element.Type, element.Description, positions)\r\n\t\ttsvBuilder.WriteString(line)\r\n\t\tp.logger.Debug(\"Added element to TSV: %s\", strings.TrimSpace(line))\r\n\t}\r\n\r\n\t// Écrire les relations\r\n\tfor _, relation := range p.ontology.Relations {\r\n\t\tline := fmt.Sprintf(\"%s\\t%s\\t%s\\t%s\\n\", relation.Source, relation.Type, relation.Target, relation.Description)\r\n\t\ttsvBuilder.WriteString(line)\r\n\t\tp.logger.Debug(\"Added relation to TSV: %s\", strings.TrimSpace(line))\r\n\t}\r\n\r\n\treturn tsvBuilder.String()\r\n}\r\n\r\n// generateAndSaveContextJSON génère et sauvegarde le JSON de contexte\r\nfunc (p *Pipeline) generateAndSaveContextJSON(content []byte, dir, baseName string) (string, error) {\r\n\tp.logger.Info(\"Generating context JSON\")\r\n\twords := strings.Fields(string(content))\r\n\tp.logger.Debug(\"Total words in content: %d\", len(words))\r\n\r\n\tpositionRanges := p.getAllPositionsFromNewContent(words)\r\n\tp.logger.Info(\"Total position ranges collected: %d\", len(positionRanges))\r\n\r\n\tmergedPositions := mergeOverlappingPositions(positionRanges)\r\n\tp.logger.Info(\"Merged position ranges: %d\", len(mergedPositions))\r\n\r\n\tvalidPositions := make([]int, len(mergedPositions))\r\n\tfor i, pr := range mergedPositions {\r\n\t\tvalidPositions[i] = pr.Start\r\n\t}\r\n\r\n\tcontextJSON, err := GenerateContextJSON(content, validPositions, p.contextWords, mergedPositions)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to generate context JSON: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"failed to generate context JSON: %w\", err)\r\n\t}\r\n\tp.logger.Debug(\"Context JSON generated successfully. Length: %d bytes\", len(contextJSON))\r\n\r\n\tcontextFile := filepath.Join(dir, strings.TrimSuffix(baseName, filepath.Ext(baseName))+\"_context.json\")\r\n\terr = p.storage.Write(contextFile, []byte(contextJSON))\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to write context JSON file: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"failed to write context JSON file: %w\", err)\r\n\t}\r\n\tp.logger.Info(\"Context JSON saved to: %s\", contextFile)\r\n\r\n\treturn contextFile, nil\r\n}\r\n\r\n// generateAndSaveMetadata génère et sauvegarde les métadonnées\r\nfunc (p *Pipeline) generateAndSaveMetadata(outputPath, contextFile string) error {\r\n\tp.logger.Debug(\"Generating and saving metadata\")\r\n\tmetadataGen := metadata.NewGenerator(p.storage) // Passez p.storage ici\r\n\r\n\tontologyFile := filepath.Base(outputPath)\r\n\tmeta, err := metadataGen.GenerateMetadata(p.inputPath, ontologyFile, contextFile)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to generate metadata: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to generate metadata: %w\", err)\r\n\t}\r\n\r\n\tmetaContent, err := json.MarshalIndent(meta, \"\", \"  \")\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to marshal metadata: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to marshal metadata: %w\", err)\r\n\t}\r\n\r\n\tmetaFilePath := filepath.Join(filepath.Dir(outputPath), metadataGen.GetMetadataFilename(p.inputPath))\r\n\terr = p.storage.Write(metaFilePath, metaContent)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to save metadata: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to save metadata: %w\", err)\r\n\t}\r\n\r\n\tp.logger.Info(\"Metadata saved to: %s\", metaFilePath)\r\n\treturn nil\r\n}\r\n",
    "size": 6541,
    "modTime": "2024-11-05T18:20:22.4613777+01:00",
    "path": "internal\\pipeline\\output_generation.go"
  },
  {
    "name": "pipeline.go",
    "content": "// pipeline.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/llm\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/chrlesur/Ontology/internal/model\"\r\n\t\"github.com/chrlesur/Ontology/internal/parser\"\r\n\t\"github.com/chrlesur/Ontology/internal/storage\"\r\n\r\n\t\"github.com/pkoukk/tiktoken-go\"\r\n)\r\n\r\n// ProgressInfo contient les informations sur la progression du traitement\r\ntype ProgressInfo struct {\r\n\tCurrentPass       int\r\n\tTotalPasses       int\r\n\tCurrentStep       string\r\n\tTotalSegments     int\r\n\tProcessedSegments int\r\n}\r\n\r\n// ProgressCallback est une fonction de rappel pour mettre à jour la progression\r\ntype ProgressCallback func(ProgressInfo)\r\n\r\n// Pipeline représente le pipeline de traitement principal\r\ntype Pipeline struct {\r\n\tconfig                   *config.Config\r\n\tlogger                   *logger.Logger\r\n\tllm                      llm.Client\r\n\tprogressCallback         ProgressCallback\r\n\tontology                 *model.Ontology\r\n\tincludePositions         bool\r\n\tcontextOutput            bool\r\n\tcontextWords             int\r\n\tinputPath                string\r\n\tentityExtractionPrompt   string\r\n\trelationExtractionPrompt string\r\n\tontologyEnrichmentPrompt string\r\n\tontologyMergePrompt      string\r\n\tstorage                  storage.Storage\r\n}\r\n\r\n// NewPipeline crée une nouvelle instance du pipeline de traitement\r\nfunc NewPipeline(includePositions bool, contextOutput bool, contextWords int, entityPrompt, relationPrompt, enrichmentPrompt, mergePrompt, llmType, llmModel, inputPath string) (*Pipeline, error) {\r\n\tcfg := config.GetConfig()\r\n\tlog := logger.GetLogger()\r\n\r\n\tlog.Debug(\"Creating new pipeline instance\")\r\n\r\n\t// Sélection du LLM\r\n\tselectedLLM := cfg.DefaultLLM\r\n\tselectedModel := cfg.DefaultModel\r\n\tif llmType != \"\" {\r\n\t\tselectedLLM = llmType\r\n\t}\r\n\tif llmModel != \"\" {\r\n\t\tselectedModel = llmModel\r\n\t}\r\n\r\n\tlog.Debug(\"Selected LLM: %s, Model: %s\", selectedLLM, selectedModel)\r\n\r\n\t// Initialisation du client LLM\r\n\tclient, err := llm.GetClient(selectedLLM, selectedModel)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to initialize LLM client: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrInitLLMClient\"), err)\r\n\t}\r\n\r\n\t// Initialisation du stockage\r\n\tstorageInstance, err := storage.NewStorage(cfg, inputPath)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to initialize storage: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to initialize storage: %w\", err)\r\n\t}\r\n\r\n\tlog.Info(\"Pipeline instance created successfully\")\r\n\r\n\treturn \u0026Pipeline{\r\n\t\tconfig:                   cfg,\r\n\t\tlogger:                   log,\r\n\t\tllm:                      client,\r\n\t\tontology:                 model.NewOntology(),\r\n\t\tincludePositions:         includePositions,\r\n\t\tcontextOutput:            contextOutput,\r\n\t\tcontextWords:             contextWords,\r\n\t\tentityExtractionPrompt:   entityPrompt,\r\n\t\trelationExtractionPrompt: relationPrompt,\r\n\t\tontologyEnrichmentPrompt: enrichmentPrompt,\r\n\t\tontologyMergePrompt:      mergePrompt,\r\n\t\tstorage:                  storageInstance,\r\n\t}, nil\r\n}\r\n\r\n// SetProgressCallback définit la fonction de rappel pour les mises à jour de progression\r\nfunc (p *Pipeline) SetProgressCallback(callback ProgressCallback) {\r\n\tp.logger.Debug(\"Setting progress callback\")\r\n\tp.progressCallback = callback\r\n}\r\n\r\n// ExecutePipeline orchestre l'ensemble du flux de travail\r\nfunc (p *Pipeline) ExecutePipeline(input string, output string, passes int, existingOntology string, ontology *model.Ontology) error {\r\n\tp.inputPath = input\r\n\tp.logger.Info(i18n.GetMessage(\"StartingPipeline\"))\r\n\tp.logger.Debug(\"Input: %s, Output: %s, Passes: %d, Existing Ontology: %s\", input, output, passes, existingOntology)\r\n\r\n\tvar result string\r\n\tvar err error\r\n\tvar finalContent []byte\r\n\r\n\t// Initialiser le tokenizer\r\n\ttke, err := tiktoken.GetEncoding(\"cl100k_base\")\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to initialize tokenizer: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to initialize tokenizer: %w\", err)\r\n\t}\r\n\r\n\t// Charger l'ontologie existante si spécifiée\r\n\tif existingOntology != \"\" {\r\n\t\tcontent, err := p.storage.Read(existingOntology)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Error(\"Failed to read existing ontology: %v\", err)\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrLoadExistingOntology\"), err)\r\n\t\t}\r\n\t\tresult = string(content)\r\n\t\ttokenCount := len(tke.Encode(result, nil, nil))\r\n\t\tp.logger.Debug(\"Loaded existing ontology, token count: %d\", tokenCount)\r\n\t}\r\n\r\n\t// Effectuer les passes de traitement\r\n\tfor i := 0; i \u003c passes; i++ {\r\n\t\tif p.progressCallback != nil {\r\n\t\t\tp.progressCallback(ProgressInfo{\r\n\t\t\t\tCurrentPass: i + 1,\r\n\t\t\t\tTotalPasses: passes,\r\n\t\t\t\tCurrentStep: \"Starting Pass\",\r\n\t\t\t})\r\n\t\t}\r\n\t\tinitialTokenCount := len(tke.Encode(result, nil, nil))\r\n\t\tp.logger.Info(\"Starting pass %d with initial result token count: %d\", i+1, initialTokenCount)\r\n\r\n\t\tresult, finalContent, err = p.processSinglePass(input, result, p.includePositions)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Error(i18n.GetMessage(\"ErrProcessingPass\"), i+1, err)\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrProcessingPass\"), err)\r\n\t\t}\r\n\r\n\t\tnewTokenCount := len(tke.Encode(result, nil, nil))\r\n\t\tp.logger.Info(\"Completed pass %d, new result token count: %d\", i+1, newTokenCount)\r\n\t\tp.logger.Info(\"Token count change in pass %d: %d\", i+1, newTokenCount-initialTokenCount)\r\n\t}\r\n\r\n\t// Sauvegarder les résultats\r\n\terr = p.saveResult(result, output, finalContent)\r\n\tif err != nil {\r\n\t\tp.logger.Error(i18n.GetMessage(\"ErrSavingResult\"), err)\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrSavingResult\"), err)\r\n\t}\r\n\r\n\tp.logger.Info(\"Pipeline execution completed successfully\")\r\n\treturn nil\r\n}\r\n\r\nfunc (p *Pipeline) getParser(input string) (parser.Parser, error) {\r\n    ext := strings.ToLower(filepath.Ext(input))\r\n    if ext == \"\" {\r\n        return nil, fmt.Errorf(\"file has no extension: %s\", input)\r\n    }\r\n    p.logger.Debug(\"Getting parser for file extension: %s\", ext)\r\n    \r\n    return parser.GetParser(ext)\r\n}",
    "size": 6085,
    "modTime": "2024-11-05T18:01:08.5127305+01:00",
    "path": "internal\\pipeline\\pipeline.go"
  },
  {
    "name": "pipeline_test.go",
    "content": "package pipeline\r\n\r\nimport (\r\n\t\"os\"\r\n\t\"testing\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/chrlesur/Ontology/internal/storage\"\r\n\t\"github.com/stretchr/testify/assert\"\r\n)\r\n\r\n// MockStorage implémente l'interface Storage pour les tests\r\ntype MockStorage struct {\r\n\tIsDirectoryFunc func(string) (bool, error)\r\n\tStatFunc        func(string) (storage.FileInfo, error)\r\n\tReadFunc        func(string) ([]byte, error)\r\n\tWriteFunc       func(string, []byte) error\r\n\tListFunc        func(string) ([]string, error)\r\n\tDeleteFunc      func(string) error\r\n\tExistsFunc      func(string) (bool, error)\r\n}\r\n\r\ntype MockFileInfo struct {\r\n\tNameValue    string\r\n\tSizeValue    int64\r\n\tModeValue    os.FileMode\r\n\tModTimeValue time.Time\r\n\tIsDirValue   bool\r\n}\r\n\r\ntype FileInfo interface {\r\n\tName() string\r\n\tSize() int64\r\n\tMode() os.FileMode\r\n\tModTime() time.Time\r\n\tIsDir() bool\r\n\tSys() interface{}\r\n}\r\n\r\nfunc (m *MockStorage) IsDirectory(path string) (bool, error) {\r\n\tif m.IsDirectoryFunc != nil {\r\n\t\treturn m.IsDirectoryFunc(path)\r\n\t}\r\n\treturn false, nil\r\n}\r\n\r\nfunc (m *MockStorage) Stat(path string) (storage.FileInfo, error) {\r\n\tif m.StatFunc != nil {\r\n\t\treturn m.StatFunc(path)\r\n\t}\r\n\treturn \u0026MockFileInfo{}, nil\r\n}\r\n\r\nfunc (m *MockStorage) Read(path string) ([]byte, error) {\r\n\tif m.ReadFunc != nil {\r\n\t\treturn m.ReadFunc(path)\r\n\t}\r\n\treturn []byte(\"Mock content\"), nil\r\n}\r\n\r\nfunc (m *MockStorage) Write(path string, data []byte) error {\r\n\tif m.WriteFunc != nil {\r\n\t\treturn m.WriteFunc(path, data)\r\n\t}\r\n\treturn nil\r\n}\r\n\r\nfunc (m *MockStorage) List(prefix string) ([]string, error) {\r\n\tif m.ListFunc != nil {\r\n\t\treturn m.ListFunc(prefix)\r\n\t}\r\n\treturn []string{}, nil\r\n}\r\n\r\nfunc (m *MockStorage) Delete(path string) error {\r\n\tif m.DeleteFunc != nil {\r\n\t\treturn m.DeleteFunc(path)\r\n\t}\r\n\treturn nil\r\n}\r\n\r\nfunc (m *MockStorage) Exists(path string) (bool, error) {\r\n\tif m.ExistsFunc != nil {\r\n\t\treturn m.ExistsFunc(path)\r\n\t}\r\n\treturn true, nil\r\n}\r\n\r\nfunc (mfi *MockFileInfo) Name() string       { return mfi.NameValue }\r\nfunc (mfi *MockFileInfo) Size() int64        { return mfi.SizeValue }\r\nfunc (mfi *MockFileInfo) Mode() os.FileMode  { return mfi.ModeValue }\r\nfunc (mfi *MockFileInfo) ModTime() time.Time { return mfi.ModTimeValue }\r\nfunc (mfi *MockFileInfo) IsDir() bool        { return mfi.IsDirValue }\r\nfunc (mfi *MockFileInfo) Sys() interface{}   { return nil }\r\n\r\nfunc TestNewPipeline(t *testing.T) {\r\n\tp, err := NewPipeline(true, false, 30, \"\", \"\", \"\", \"\", \"claude\", \"claude-3-5-sonnet-20240620\")\r\n\tassert.NoError(t, err)\r\n\tassert.NotNil(t, p)\r\n\tassert.True(t, p.includePositions)\r\n\tassert.False(t, p.contextOutput)\r\n\tassert.Equal(t, 30, p.contextWords)\r\n}\r\n\r\nfunc TestCreatePositionIndex(t *testing.T) {\r\n\tp := \u0026Pipeline{\r\n\t\tlogger: logger.GetLogger(),\r\n\t}\r\n\tcontent := []byte(\"This is a test. This is another test with some_underscore.\")\r\n\tindex := p.createPositionIndex(content)\r\n\r\n\tassert.Equal(t, []int{0}, index[\"this\"])\r\n\tassert.Equal(t, []int{1, 5}, index[\"is\"])\r\n\tassert.Equal(t, []int{3, 7}, index[\"test\"])\r\n\tassert.Equal(t, []int{9}, index[\"another\"])\r\n\tassert.Equal(t, []int{11}, index[\"some_underscore\"])\r\n\tassert.Equal(t, []int{11}, index[\"some underscore\"])\r\n}\r\n\r\nfunc TestFindPositions(t *testing.T) {\r\n\tp := \u0026Pipeline{\r\n\t\tlogger: logger.GetLogger(),\r\n\t}\r\n\tcontent := \"This is a test. This is another test with some_underscore.\"\r\n\tindex := p.createPositionIndex([]byte(content))\r\n\r\n\ttests := []struct {\r\n\t\tword     string\r\n\t\texpected []int\r\n\t}{\r\n\t\t{\"test\", []int{3, 7}},\r\n\t\t{\"This\", []int{0, 4}},\r\n\t\t{\"is\", []int{1, 5}},\r\n\t\t{\"some_underscore\", []int{11}},\r\n\t\t{\"nonexistent\", []int{}},\r\n\t}\r\n\r\n\tfor _, tt := range tests {\r\n\t\tt.Run(tt.word, func(t *testing.T) {\r\n\t\t\tpositions := p.findPositions(tt.word, index, content)\r\n\t\t\tassert.Equal(t, tt.expected, positions)\r\n\t\t})\r\n\t}\r\n}\r\n\r\nfunc TestNormalizeWord(t *testing.T) {\r\n\ttests := []struct {\r\n\t\tinput    string\r\n\t\texpected string\r\n\t}{\r\n\t\t{\"Hello\", \"hello\"},\r\n\t\t{\"World!\", \"world\"},\r\n\t\t{\"l'exemple\", \"l'exemple\"},\r\n\t\t{\"UPPER_CASE\", \"upper case\"},\r\n\t}\r\n\r\n\tfor _, tt := range tests {\r\n\t\tassert.Equal(t, tt.expected, normalizeWord(tt.input))\r\n\t}\r\n}\r\n\r\nfunc TestGenerateArticleVariants(t *testing.T) {\r\n\ttests := []struct {\r\n\t\tinput    string\r\n\t\texpected []string\r\n\t}{\r\n\t\t{\"example\", []string{\"example\", \"l'example\", \"d'example\", \"l example\", \"d example\"}},\r\n\t\t{\"underscore_word\", []string{\r\n\t\t\t\"underscore_word\", \"l'underscore_word\", \"d'underscore_word\", \"l underscore_word\", \"d underscore_word\",\r\n\t\t\t\"underscore word\", \"l'underscore word\", \"d'underscore word\", \"l underscore word\", \"d underscore word\",\r\n\t\t}},\r\n\t\t{\"l'apple\", []string{\"l'apple\"}},\r\n\t}\r\n\r\n\tfor _, tt := range tests {\r\n\t\tt.Run(tt.input, func(t *testing.T) {\r\n\t\t\tvariants := generateArticleVariants(tt.input)\r\n\t\t\tassert.ElementsMatch(t, tt.expected, variants)\r\n\t\t})\r\n\t}\r\n}\r\n\r\nfunc TestTruncateString(t *testing.T) {\r\n\tassert.Equal(t, \"Hello...\", truncateString(\"Hello, World!\", 5))\r\n\tassert.Equal(t, \"Hello, World!\", truncateString(\"Hello, World!\", 20))\r\n}\r\n\r\nfunc TestMergeOverlappingPositions(t *testing.T) {\r\n\tpositions := []PositionRange{\r\n\t\t{Start: 0, End: 5, Element: \"A\"},\r\n\t\t{Start: 3, End: 8, Element: \"B\"},\r\n\t\t{Start: 10, End: 15, Element: \"C\"},\r\n\t}\r\n\tmerged := mergeOverlappingPositions(positions)\r\n\tassert.Equal(t, 2, len(merged))\r\n\tassert.Equal(t, PositionRange{Start: 0, End: 8, Element: \"A\"}, merged[0])\r\n}\r\n\r\nfunc TestGenerateContextJSON(t *testing.T) {\r\n\tcontent := []byte(\"This is a test sentence for context generation.\")\r\n\tpositions := []int{3, 7}\r\n\tpositionRanges := []PositionRange{\r\n\t\t{Start: 3, End: 3, Element: \"test\"},\r\n\t\t{Start: 7, End: 7, Element: \"sentence\"},\r\n\t}\r\n\r\n\tjson, err := GenerateContextJSON(content, positions, 2, positionRanges)\r\n\tassert.NoError(t, err)\r\n\tassert.Contains(t, json, \"test\")\r\n\tassert.Contains(t, json, \"sentence\")\r\n}\r\n\r\n",
    "size": 5811,
    "modTime": "2024-11-05T09:57:33.6176046+01:00",
    "path": "internal\\pipeline\\pipeline_test.go"
  },
  {
    "name": "position_utils.go",
    "content": "// position_utils.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"sort\"\r\n\t\"strings\"\r\n)\r\n\r\n// PositionRange représente une plage de positions pour un élément\r\ntype PositionRange struct {\r\n\tStart   int\r\n\tEnd     int\r\n\tElement string\r\n}\r\n\r\n// createPositionIndex crée un index des positions pour chaque mot dans le contenu\r\nfunc (p *Pipeline) createPositionIndex(content []byte) map[string][]int {\r\n\tlog.Debug(\"Starting createPositionIndex\")\r\n\tindex := make(map[string][]int)\r\n\twords := bytes.Fields(content)\r\n\r\n\tfor i, word := range words {\r\n\t\tvariants := generateArticleVariants(string(word))\r\n\t\tfor _, variant := range variants {\r\n\t\t\tnormalizedVariant := normalizeWord(variant)\r\n\t\t\tindex[normalizedVariant] = append(index[normalizedVariant], i)\r\n\t\t}\r\n\t}\r\n\r\n\t// Indexer les paires et triplets de mots\r\n\tfor i := 0; i \u003c len(words)-1; i++ {\r\n\t\tpair := normalizeWord(string(words[i])) + \" \" + normalizeWord(string(words[i+1]))\r\n\t\tindex[pair] = append(index[pair], i)\r\n\r\n\t\tif i \u003c len(words)-2 {\r\n\t\t\ttriplet := pair + \" \" + normalizeWord(string(words[i+2]))\r\n\t\t\tindex[triplet] = append(index[triplet], i)\r\n\t\t}\r\n\t}\r\n\r\n\tlog.Debug(\"Finished createPositionIndex. Total indexed terms: %d\", len(index))\r\n\treturn index\r\n}\r\n\r\n// findPositions trouve toutes les positions d'un mot ou d'une phrase dans le contenu\r\nfunc (p *Pipeline) findPositions(word string, index map[string][]int, content string) []int {\r\n\tparts := strings.Split(word, \"\\t\")\r\n\tentityName := parts[0]\r\n\r\n\tlog.Debug(\"Searching for positions of entity: %s\", entityName)\r\n\r\n\tallVariants := generateArticleVariants(entityName)\r\n\tvar allPositions []int\r\n\r\n\tfor _, variant := range allVariants {\r\n\t\tnormalizedVariant := normalizeWord(variant)\r\n\t\tlog.Debug(\"Trying variant: %s\", normalizedVariant)\r\n\r\n\t\t// Recherche exacte d'abord\r\n\t\tif positions, ok := index[normalizedVariant]; ok {\r\n\t\t\tlog.Debug(\"Found exact match positions for %s: %v\", variant, positions)\r\n\t\t\tallPositions = append(allPositions, positions...)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\t// Utiliser la recherche approximative seulement pour la variante originale\r\n\t\tif variant == entityName {\r\n\t\t\tpositions := p.findApproximatePositions(normalizedVariant, content)\r\n\t\t\tif len(positions) \u003e 0 {\r\n\t\t\t\tlog.Debug(\"Found approximate positions for %s: %v\", variant, positions)\r\n\t\t\t\tallPositions = append(allPositions, positions...)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Dédupliquer et trier les positions trouvées\r\n\tuniquePositions := uniqueIntSlice(allPositions)\r\n\tsort.Ints(uniquePositions)\r\n\r\n\tif len(uniquePositions) \u003e 0 {\r\n\t\tlog.Debug(\"Found total unique positions for %s: %v\", entityName, uniquePositions)\r\n\t} else {\r\n\t\tlog.Debug(\"No positions found for entity: %s\", entityName)\r\n\t}\r\n\r\n\treturn uniquePositions\r\n}\r\n\r\n// findApproximatePositions trouve les positions approximatives d'un mot ou d'une phrase\r\nfunc (p *Pipeline) findApproximatePositions(entityName string, content string) []int {\r\n\twords := strings.Fields(strings.ToLower(entityName))\r\n\tcontentLower := strings.ToLower(content)\r\n\tvar positions []int\r\n\r\n\t// Recherche exacte de la phrase complète\r\n\tif index := strings.Index(contentLower, strings.Join(words, \" \")); index != -1 {\r\n\t\tpositions = append(positions, index)\r\n\t\tlog.Debug(\"Found exact match for %s at position %d\", entityName, index)\r\n\t\treturn positions\r\n\t}\r\n\r\n\t// Recherche approximative\r\n\tcontentWords := strings.Fields(contentLower)\r\n\tmaxDistance := 5 // Nombre maximum de mots entre les termes recherchés\r\n\r\n\tfor i := 0; i \u003c len(contentWords); i++ {\r\n\t\tif matchFound, endPos := p.checkApproximateMatch(words, contentWords[i:], maxDistance); matchFound {\r\n\t\t\tpositions = append(positions, i)\r\n\t\t\tmatchedPhrase := strings.Join(contentWords[i:i+endPos+1], \" \")\r\n\t\t\tlog.Debug(\"Found approximate match for %s at position %d: %s\", entityName, i, matchedPhrase)\r\n\t\t}\r\n\t}\r\n\r\n\treturn positions\r\n}\r\n\r\n// checkApproximateMatch vérifie si une correspondance approximative est trouvée\r\nfunc (p *Pipeline) checkApproximateMatch(searchWords, contentWords []string, maxDistance int) (bool, int) {\r\n\twordIndex := 0\r\n\tdistanceCount := 0\r\n\tfor i, word := range contentWords {\r\n\t\tif strings.Contains(word, searchWords[wordIndex]) {\r\n\t\t\twordIndex++\r\n\t\t\tdistanceCount = 0\r\n\t\t\tif wordIndex == len(searchWords) {\r\n\t\t\t\treturn true, i\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\tdistanceCount++\r\n\t\t\tif distanceCount \u003e maxDistance {\r\n\t\t\t\treturn false, -1\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn false, -1\r\n}\r\n\r\n// getAllPositionsFromNewContent récupère toutes les positions des éléments dans le nouveau contenu\r\nfunc (p *Pipeline) getAllPositionsFromNewContent(words []string) []PositionRange {\r\n\tvar allPositions []PositionRange\r\n\tfor _, element := range p.ontology.Elements {\r\n\t\telementWords := strings.Fields(strings.ToLower(element.Name))\r\n\t\t// Utiliser les positions déjà connues dans l'ontologie\r\n\t\tfor _, pos := range element.Positions {\r\n\t\t\tif pos \u003e= 0 \u0026\u0026 pos \u003c len(words) {\r\n\t\t\t\tend := min(pos+len(elementWords), len(words)) - 1\r\n\t\t\t\tallPositions = append(allPositions, PositionRange{\r\n\t\t\t\t\tStart:   pos,\r\n\t\t\t\t\tEnd:     end,\r\n\t\t\t\t\tElement: element.Name,\r\n\t\t\t\t})\r\n\t\t\t}\r\n\t\t}\r\n\t\t// Rechercher également de nouvelles occurrences\r\n\t\tfor i := 0; i \u003c= len(words)-len(elementWords); i++ {\r\n\t\t\tmatch := true\r\n\t\t\tfor j, ew := range elementWords {\r\n\t\t\t\tif strings.ToLower(words[i+j]) != ew {\r\n\t\t\t\t\tmatch = false\r\n\t\t\t\t\tbreak\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif match {\r\n\t\t\t\tallPositions = append(allPositions, PositionRange{\r\n\t\t\t\t\tStart:   i,\r\n\t\t\t\t\tEnd:     i + len(elementWords) - 1,\r\n\t\t\t\t\tElement: element.Name,\r\n\t\t\t\t})\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\tlog.Debug(\"Total position ranges collected from new content: %d\", len(allPositions))\r\n\treturn allPositions\r\n}\r\n\r\n// mergeOverlappingPositions fusionne les plages de positions qui se chevauchent\r\nfunc mergeOverlappingPositions(positions []PositionRange) []PositionRange {\r\n\tif len(positions) == 0 {\r\n\t\treturn positions\r\n\t}\r\n\r\n\tsort.Slice(positions, func(i, j int) bool {\r\n\t\treturn positions[i].Start \u003c positions[j].Start\r\n\t})\r\n\r\n\tmerged := []PositionRange{positions[0]}\r\n\r\n\tfor _, current := range positions[1:] {\r\n\t\tlast := \u0026merged[len(merged)-1]\r\n\t\tif current.Start \u003c= last.End+1 {\r\n\t\t\tif current.End \u003e last.End {\r\n\t\t\t\tlast.End = current.End\r\n\t\t\t}\r\n\t\t\tif len(current.Element) \u003e len(last.Element) {\r\n\t\t\t\tlast.Element = current.Element\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\tmerged = append(merged, current)\r\n\t\t}\r\n\t}\r\n\r\n\treturn merged\r\n}\r\n",
    "size": 6301,
    "modTime": "2024-11-04T15:01:41.3500303+01:00",
    "path": "internal\\pipeline\\position_utils.go"
  },
  {
    "name": "segmentation.go",
    "content": "// segmentation.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/parser\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n\t\"github.com/chrlesur/Ontology/internal/segmenter\"\r\n\r\n\t\"github.com/pkoukk/tiktoken-go\"\r\n)\r\n\r\n// processSinglePass traite une seule passe de l'ensemble du contenu\r\nfunc (p *Pipeline) processSinglePass(input string, previousResult string, includePositions bool) (string, []byte, error) {\r\n\tp.logger.Debug(\"Starting processSinglePass for input: %s\", input)\r\n\r\n\tisDir, err := p.storage.IsDirectory(input)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to check if input is directory: %v\", err)\r\n\t\treturn \"\", nil, fmt.Errorf(\"failed to check if input is directory: %w\", err)\r\n\t}\r\n\r\n\tvar content []byte\r\n\tif isDir {\r\n\t\tcontent, err = p.readDirectory(input)\r\n\t} else {\r\n\t\tcontent, err = p.readFile(input)\r\n\t}\r\n\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to read input: %v\", err)\r\n\t\treturn \"\", nil, fmt.Errorf(\"failed to read input: %w\", err)\r\n\t}\r\n\r\n\tif len(content) == 0 {\r\n\t\tp.logger.Error(\"No content found in input: %s\", input)\r\n\t\treturn \"\", nil, fmt.Errorf(\"no content found in input\")\r\n\t}\r\n\r\n\t// Initialiser le tokenizer\r\n\ttke, err := tiktoken.GetEncoding(\"cl100k_base\")\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to initialize tokenizer: %v\", err)\r\n\t\treturn \"\", nil, fmt.Errorf(\"failed to initialize tokenizer: %w\", err)\r\n\t}\r\n\r\n\tcontentTokens := len(tke.Encode(string(content), nil, nil))\r\n\tp.logger.Info(\"Input content tokens: %d\", contentTokens)\r\n\r\n\tpositionIndex := p.createPositionIndex(content)\r\n\tp.logger.Debug(\"Position index created. Number of entries: %d\", len(positionIndex))\r\n\r\n\tsegments, err := segmenter.Segment(content, segmenter.SegmentConfig{\r\n\t\tMaxTokens:   p.config.MaxTokens,\r\n\t\tContextSize: p.config.ContextSize,\r\n\t\tModel:       p.config.DefaultModel,\r\n\t})\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to segment content: %v\", err)\r\n\t\treturn \"\", nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrSegmentContent\"), err)\r\n\t}\r\n\tp.logger.Info(\"Number of segments: %d\", len(segments))\r\n\r\n\tif p.progressCallback != nil {\r\n\t\tp.progressCallback(ProgressInfo{\r\n\t\t\tCurrentStep:   \"Segmenting\",\r\n\t\t\tTotalSegments: len(segments),\r\n\t\t})\r\n\t}\r\n\r\n\tresults := make([]string, len(segments))\r\n\tvar wg sync.WaitGroup\r\n\tsem := make(chan struct{}, 10) // Sémaphore pour limiter à 5 goroutines concurrentes\r\n\r\n\tfor i, segment := range segments {\r\n\t\twg.Add(1)\r\n\t\tgo func(i int, seg segmenter.SegmentInfo) {\r\n\t\t\tdefer wg.Done()\r\n\r\n\t\t\t// Acquérir une place dans le sémaphore\r\n\t\t\tsem \u003c- struct{}{}\r\n\t\t\tdefer func() { \u003c-sem }() // Libérer la place à la fin\r\n\r\n\t\t\tsegmentTokens := len(tke.Encode(string(seg.Content), nil, nil))\r\n\t\t\tp.logger.Debug(\"Processing segment %d/%d, Start: %d, End: %d, Length: %d bytes, Tokens: %d\",\r\n\t\t\t\ti+1, len(segments), seg.Start, seg.End, len(seg.Content), segmentTokens)\r\n\r\n\t\t\tcontext := segmenter.GetContext(segments, i, segmenter.SegmentConfig{\r\n\t\t\t\tMaxTokens:   p.config.MaxTokens,\r\n\t\t\t\tContextSize: p.config.ContextSize,\r\n\t\t\t\tModel:       p.config.DefaultModel,\r\n\t\t\t})\r\n\t\t\tp.logger.Debug(\"Context for segment %d/%d, Length: %d bytes\", i+1, len(segments), len(context))\r\n\r\n\t\t\tresult, err := p.processSegment(seg.Content, context, previousResult, positionIndex, includePositions)\r\n\t\t\tif err != nil {\r\n\t\t\t\tp.logger.Error(i18n.GetMessage(\"SegmentProcessingError\"), i+1, err)\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t\tresultTokens := len(tke.Encode(result, nil, nil))\r\n\t\t\tresults[i] = result\r\n\t\t\tp.logger.Info(\"Segment %d processed successfully, result tokens: %d\", i+1, resultTokens)\r\n\t\t\tif p.progressCallback != nil {\r\n\t\t\t\tp.progressCallback(ProgressInfo{\r\n\t\t\t\t\tCurrentStep:       \"Processing Segment\",\r\n\t\t\t\t\tProcessedSegments: i + 1,\r\n\t\t\t\t\tTotalSegments:     len(segments),\r\n\t\t\t\t})\r\n\t\t\t}\r\n\t\t}(i, segment)\r\n\t}\r\n\twg.Wait()\r\n\r\n\t// Fusionner les résultats de tous les segments\r\n\tmergedResult, err := p.mergeResults(previousResult, results)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to merge segment results: %v\", err)\r\n\t\treturn \"\", nil, fmt.Errorf(\"failed to merge segment results: %w\", err)\r\n\t}\r\n\r\n\tmergedResultTokens := len(tke.Encode(mergedResult, nil, nil))\r\n\tp.logger.Info(\"Merged result tokens: %d\", mergedResultTokens)\r\n\tp.logger.Debug(\"processSinglePass completed. Merged result length: %d\", len(mergedResult))\r\n\treturn mergedResult, content, nil\r\n}\r\n\r\n// mergeResults fusionne les résultats de tous les segments\r\nfunc (p *Pipeline) mergeResults(previousResult string, newResults []string) (string, error) {\r\n\tlog.Debug(\"Starting mergeResults. Previous result length: %d, Number of new results: %d\", len(previousResult), len(newResults))\r\n\r\n\t// Combiner tous les nouveaux résultats\r\n\tcombinedNewResults := strings.Join(newResults, \"\\n\")\r\n\tlog.Debug(\"Combined new results length: %d\", len(combinedNewResults))\r\n\r\n\t// Préparer les valeurs pour le prompt de fusion\r\n\tmergeValues := map[string]string{\r\n\t\t\"previous_ontology\": previousResult,\r\n\t\t\"new_ontology\":      combinedNewResults,\r\n\t\t\"additional_prompt\": p.ontologyMergePrompt,\r\n\t}\r\n\r\n\t// Utiliser le LLM pour fusionner les résultats\r\n\tlog.Debug(\"Calling LLM with OntologyMergePrompt\")\r\n\r\n\tmergedResult, err := p.llm.ProcessWithPrompt(prompt.OntologyMergePrompt, mergeValues)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Ontology merge failed: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"ontology merge failed: %w\", err)\r\n\t}\r\n\r\n\t// Normaliser le résultat fusionné\r\n\tnormalizedMergedResult := normalizeTSV(mergedResult)\r\n\r\n\tlog.Debug(\"Merged result length: %d\", len(normalizedMergedResult))\r\n\treturn normalizedMergedResult, nil\r\n}\r\n\r\nfunc (p *Pipeline) processMetadata(metadata map[string]string) {\r\n\tp.logger.Debug(\"Processing metadata\")\r\n\tfor key, value := range metadata {\r\n\t\t// Logique pour traiter chaque métadonnée\r\n\t\tp.logger.Debug(\"Metadata: %s = %s\", key, value)\r\n\t}\r\n}\r\n\r\nfunc (p *Pipeline) readDirectory(dirPath string) ([]byte, error) {\r\n\tp.logger.Debug(\"Reading directory: %s\", dirPath)\r\n\r\n\tfiles, err := p.storage.List(dirPath)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to list directory contents: %w\", err)\r\n\t}\r\n\r\n\tvar allContent []byte\r\n\tfor _, filePath := range files {\r\n\t\t// Utiliser le chemin tel quel, sans le joindre à dirPath\r\n\t\tcontent, err := p.readFile(filePath)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Warning(\"Failed to read file %s: %v\", filePath, err)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tallContent = append(allContent, content...)\r\n\t\tallContent = append(allContent, '\\n') // Add separator between files\r\n\t}\r\n\r\n\tif len(allContent) == 0 {\r\n\t\treturn nil, fmt.Errorf(\"no content found in directory: %s\", dirPath)\r\n\t}\r\n\r\n\treturn allContent, nil\r\n}\r\n\r\nfunc (p *Pipeline) readFile(filePath string) ([]byte, error) {\r\n\text := filepath.Ext(filePath)\r\n\tparser, err := parser.GetParser(ext)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to get parser for file %s: %w\", filePath, err)\r\n\t}\r\n\r\n\treader, err := p.storage.GetReader(filePath)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to get reader for file %s: %w\", filePath, err)\r\n\t}\r\n\tdefer reader.Close()\r\n\r\n\treturn parser.Parse(reader)\r\n}\r\n",
    "size": 7108,
    "modTime": "2024-11-06T00:26:36.2485878+01:00",
    "path": "internal\\pipeline\\segmentation.go"
  },
  {
    "name": "string_utils.go",
    "content": "// string_utils.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n    \"strings\"\r\n    \"unicode\"\r\n)\r\n\r\n// truncateString tronque une chaîne à une longueur maximale donnée\r\nfunc truncateString(s string, maxLength int) string {\r\n    if len(s) \u003c= maxLength {\r\n        return s\r\n    }\r\n    return s[:maxLength] + \"...\"\r\n}\r\n\r\n// normalizeWord normalise un mot en le convertissant en minuscules et en ne gardant que les lettres, les chiffres et les apostrophes\r\nfunc normalizeWord(word string) string {\r\n    return strings.TrimSpace(strings.Map(func(r rune) rune {\r\n        if unicode.IsLetter(r) || unicode.IsNumber(r) || r == '\\'' {\r\n            return unicode.ToLower(r)\r\n        }\r\n        return ' '\r\n    }, word))\r\n}\r\n\r\n// generateArticleVariants génère des variantes d'un mot avec différents articles\r\nfunc generateArticleVariants(word string) []string {\r\n    variants := []string{word}\r\n    lowercaseWord := strings.ToLower(word)\r\n\r\n    // Ajouter des variantes avec et sans apostrophe\r\n    if !strings.HasPrefix(lowercaseWord, \"l'\") \u0026\u0026 !strings.HasPrefix(lowercaseWord, \"d'\") {\r\n        variants = append(variants, \"l'\"+lowercaseWord, \"d'\"+lowercaseWord, \"l \"+lowercaseWord, \"d \"+lowercaseWord)\r\n    }\r\n\r\n    // Ajouter une variante sans underscore si le mot en contient\r\n    if strings.Contains(word, \"_\") {\r\n        spaceVariant := strings.ReplaceAll(word, \"_\", \" \")\r\n        variants = append(variants, spaceVariant)\r\n        variants = append(variants, \"l'\"+spaceVariant, \"d'\"+spaceVariant, \"l \"+spaceVariant, \"d \"+spaceVariant)\r\n    }\r\n\r\n    return variants\r\n}\r\n\r\n// uniqueIntSlice supprime les doublons dans une slice d'entiers\r\nfunc uniqueIntSlice(intSlice []int) []int {\r\n    keys := make(map[int]bool)\r\n    var list []int\r\n    for _, entry := range intSlice {\r\n        if _, value := keys[entry]; !value {\r\n            keys[entry] = true\r\n            list = append(list, entry)\r\n        }\r\n    }\r\n    return list\r\n}\r\n\r\n// min retourne le minimum entre deux entiers\r\nfunc min(a, b int) int {\r\n    if a \u003c b {\r\n        return a\r\n    }\r\n    return b\r\n}\r\n\r\n// max retourne le maximum entre deux entiers\r\nfunc max(a, b int) int {\r\n    if a \u003e b {\r\n        return a\r\n    }\r\n    return b\r\n}",
    "size": 2183,
    "modTime": "2024-11-04T15:15:05.1969855+01:00",
    "path": "internal\\pipeline\\string_utils.go"
  },
  {
    "name": "prompt.go",
    "content": "package prompt\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"strings\"\r\n)\r\n\r\n// PromptTemplate représente un template de prompt\r\ntype PromptTemplate struct {\r\n\tTemplate string\r\n}\r\n\r\n// NewPromptTemplate crée un nouveau PromptTemplate\r\nfunc NewPromptTemplate(template string) *PromptTemplate {\r\n\treturn \u0026PromptTemplate{Template: template}\r\n}\r\n\r\n// Format remplit le template avec les valeurs fournies\r\nfunc (pt *PromptTemplate) Format(values map[string]string) string {\r\n    result := pt.Template\r\n    for key, value := range values {\r\n        result = strings.Replace(result, fmt.Sprintf(\"{%s}\", key), value, -1)\r\n    }\r\n    \r\n    // Ajouter le prompt supplémentaire s'il existe\r\n    if additionalPrompt, ok := values[\"additional_prompt\"]; ok \u0026\u0026 additionalPrompt != \"\" {\r\n        result += \"\\n\\nAdditional instructions:\\n\" + additionalPrompt\r\n    }\r\n    \r\n    return result\r\n}\r\n\r\n// Définition des templates de prompts\r\nvar (\r\n\tEntityExtractionPrompt = NewPromptTemplate(`\r\nAnalyze the following text and extract key entities (e.g., people, organizations, concepts) relevant to building an ontology:\r\n\r\n{text}\r\n\r\nFor each entity, provide:\r\n1. Entity name\r\n2. Entity type (e.g., Person, Organization, Concept)\r\n3. A brief description or context\r\n\r\nFormat your response as a list of entities, one per line, using tabs to separate fields like this:\r\nEntityName\\tEntityType\\tDescription/Context\r\n\r\nEnsure that:\r\nyour extractions are relevant to creating an ontology and avoid including irrelevant or trivial information.\r\nAll spaces in entity names are replaced with underscores\r\nYour extractions are relevant to creating an ontology\r\nYou avoid including irrelevant or trivial information\r\nYou use the original document language\r\nYou provide the output silently with no additional comments\r\n`)\r\n\r\n\tRelationExtractionPrompt = NewPromptTemplate(`\r\nBased on the following text and the list of entities provided, identify relationships between these entities that would be relevant for an ontology:\r\n\r\nText:\r\n{text}\r\n\r\nEntities:\r\n{entities}\r\n\r\nFor each relationship, provide:\r\n1. Source Entity\r\n2. Relationship Type\r\n3. Target Entity\r\n4. A brief description or context of the relationship\r\n\r\nFormat your response as a list of relationships, one per line, using tabs to separate fields like this:\r\nSourceEntity\\tRelationshipType\\tTargetEntity\\tDescription/Context\r\n\r\nFocus on meaningful relationships that contribute to the structure of the ontology. Avoid trivial or overly generic relationships.\r\nEnsure that:\r\n\r\nAll spaces in entity names are replaced with underscores\r\nYour extractions are relevant to creating an ontology\r\nYou avoid including irrelevant or trivial information\r\nYou use the original document language\r\nYou provide the output silently with no additional comments\r\n`)\r\n\r\n\tOntologyEnrichmentPrompt = NewPromptTemplate(`\r\nVous êtes un expert en ontologies chargé d'enrichir et de raffiner une ontologie existante. Voici l'ontologie actuelle et de nouvelles informations à intégrer :\r\n\r\nOntologie actuelle :\r\n{previous_result}\r\n\r\nNouveau texte à analyser :\r\n{text}\r\n\r\nContexte supplémentaire :\r\n{context}\r\n\r\nVotre tâche :\r\n1. Analyser le nouveau texte et le contexte.\r\n2. Identifier les nouvelles entités et relations pertinentes.\r\n3. Intégrer ces nouvelles informations dans l'ontologie existante.\r\n4. Raffiner les entités et relations existantes si nécessaire.\r\n5. Assurer la cohérence globale de l'ontologie.\r\n\r\nFournissez l'ontologie enrichie et raffinée dans le format suivant :\r\n- Pour les entités : Nom_Entité\\tType_Entité\\tDescription\r\n- Pour les relations : Entité_Source\\tType_Relation\\tEntité_Cible\\tDescription\r\n\r\nAssurez-vous que :\r\n\r\nTous les espaces dans les noms d'entités sont remplacés par des underscores.\r\nVos extractions sont pertinentes pour la création d'une ontologie\r\nVous évitez d'inclure des informations non pertinentes ou triviales\r\nVous utilisez la langue originale du document\r\nVous fournissez le résultat silencieusement sans commentaires supplémentaires\r\n`)\r\n\r\n\tOntologyMergePrompt = NewPromptTemplate(`\r\nVous êtes un expert en fusion d'ontologies. Votre tâche est de fusionner intelligemment une ontologie existante avec de nouvelles informations pour créer une ontologie enrichie et cohérente.\r\n\r\nOntologie existante :\r\n{previous_ontology}\r\n\r\nNouvelles informations à intégrer :\r\n{new_ontology}\r\n\r\nDirectives pour la fusion :\r\n1. Intégrez toutes les nouvelles entités et relations pertinentes de la nouvelle ontologie.\r\n2. En cas de conflit ou de duplication, identifie les concepts qui sont essentiellement identiques ou très proches sémantiquement. Pour chaque groupe de concepts similaires, choisis le nom le plus approprié et représentatif.\r\nFusionne les descriptions en une seule, plus complète. Combine toutes les positions textuelles en une seule liste, sans doublons, triée par ordre croissant.\r\n3. Assurez-vous que les relations entre les entités restent cohérentes.\r\n4. Si une nouvelle information contredit une ancienne, privilégiez la nouvelle mais notez la contradiction si elle est significative.\r\n5. Maintenez la structure et le format de l'ontologie existante.\r\n6. Évitez les redondances et les informations en double.\r\n\r\nVotre tâche :\r\n- Analysez attentivement les deux ensembles d'informations.\r\n- Fusionnez-les en une ontologie unique et cohérente.\r\n- Assurez-vous que le résultat final est complet, sans perte d'information importante.\r\n\r\nFormat de sortie :\r\nPrésentez l'ontologie fusionnée dans le même format que l'ontologie existante, avec une entité ou une relation par ligne.\r\nPour les entités : Nom_Entité\\tType_Entité\\tDescription\r\nPour les relations : Entité_Source\\tType_Relation\\tEntité_Cible\\tDescription\r\n\r\nProcédez à la fusion de manière silencieuse, sans ajouter de commentaires ou d'explications supplémentaires.\r\n`)\r\n)\r\n",
    "size": 5836,
    "modTime": "2024-10-30T22:49:14.3348105+01:00",
    "path": "internal\\prompt\\prompt.go"
  },
  {
    "name": "segmenter.go",
    "content": "package segmenter\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"errors\"\r\n\t\"fmt\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/pkoukk/tiktoken-go\"\r\n)\r\n\r\nvar (\r\n\tErrInvalidContent = errors.New(i18n.Messages.ErrInvalidContent)\r\n\tErrTokenization   = errors.New(i18n.Messages.ErrTokenization)\r\n)\r\n\r\nvar log = logger.GetLogger()\r\n\r\n// SegmentConfig holds the configuration for segmentation\r\ntype SegmentConfig struct {\r\n\tMaxTokens   int\r\n\tContextSize int\r\n\tModel       string\r\n}\r\n\r\n// Segment divides the content into segments of maxTokens\r\ntype SegmentInfo struct {\r\n\tContent []byte\r\n\tStart   int\r\n\tEnd     int\r\n}\r\n\r\nfunc Segment(content []byte, cfg SegmentConfig) ([]SegmentInfo, error) {\r\n\tlog.Debug(i18n.Messages.LogSegmentationStarted)\r\n\tlog.Debug(fmt.Sprintf(\"Segmentation config: MaxTokens=%d, ContextSize=%d, Model=%s\", cfg.MaxTokens, cfg.ContextSize, cfg.Model))\r\n\tlog.Debug(fmt.Sprintf(\"Content length: %d bytes\", len(content)))\r\n\r\n\tif len(content) == 0 {\r\n\t\treturn nil, ErrInvalidContent\r\n\t}\r\n\r\n\ttokenizer, err := getTokenizer(cfg.Model)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\tvar segments []SegmentInfo\r\n\tsentences := splitIntoSentences(content)\r\n\tcurrentSegment := new(bytes.Buffer)\r\n\tcurrentTokenCount := 0\r\n\tcurrentStart := 0\r\n\r\n\tfor _, sentence := range sentences {\r\n\t\tsentenceTokens := CountTokens(sentence, tokenizer)\r\n\r\n\t\tif currentTokenCount+sentenceTokens \u003e cfg.MaxTokens {\r\n\t\t\tif currentSegment.Len() \u003e 0 {\r\n\t\t\t\tsegmentContent := content[currentStart : currentStart+currentSegment.Len()]\r\n\t\t\t\tsegments = append(segments, SegmentInfo{\r\n\t\t\t\t\tContent: segmentContent,\r\n\t\t\t\t\tStart:   currentStart,\r\n\t\t\t\t\tEnd:     currentStart + currentSegment.Len(),\r\n\t\t\t\t})\r\n\t\t\t\tlog.Debug(fmt.Sprintf(\"Segment created. Start: %d, End: %d, Length: %d bytes, Tokens: %d, Preview: %s\",\r\n\t\t\t\t\tcurrentStart, currentStart+currentSegment.Len(), len(segmentContent), currentTokenCount,\r\n\t\t\t\t\ttruncateString(string(segmentContent), 100)))\r\n\t\t\t\tcurrentStart = currentStart + currentSegment.Len()\r\n\t\t\t\tcurrentSegment.Reset()\r\n\t\t\t\tcurrentTokenCount = 0\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tcurrentSegment.Write(sentence)\r\n\t\tcurrentTokenCount += sentenceTokens\r\n\r\n\t\tif currentTokenCount \u003e= cfg.MaxTokens {\r\n\t\t\tsegmentContent := content[currentStart : currentStart+currentSegment.Len()]\r\n\t\t\tsegments = append(segments, SegmentInfo{\r\n\t\t\t\tContent: segmentContent,\r\n\t\t\t\tStart:   currentStart,\r\n\t\t\t\tEnd:     currentStart + currentSegment.Len(),\r\n\t\t\t})\r\n\t\t\tlog.Debug(fmt.Sprintf(\"Segment created. Start: %d, End: %d, Length: %d bytes, Tokens: %d, Preview: %s\",\r\n\t\t\t\tcurrentStart, currentStart+currentSegment.Len(), len(segmentContent), currentTokenCount,\r\n\t\t\t\ttruncateString(string(segmentContent), 100)))\r\n\t\t\tcurrentStart = currentStart + currentSegment.Len()\r\n\t\t\tcurrentSegment.Reset()\r\n\t\t\tcurrentTokenCount = 0\r\n\t\t}\r\n\t}\r\n\r\n\tif currentSegment.Len() \u003e 0 {\r\n\t\tsegmentContent := content[currentStart : currentStart+currentSegment.Len()]\r\n\t\tsegments = append(segments, SegmentInfo{\r\n\t\t\tContent: segmentContent,\r\n\t\t\tStart:   currentStart,\r\n\t\t\tEnd:     currentStart + currentSegment.Len(),\r\n\t\t})\r\n\t\tlog.Debug(fmt.Sprintf(\"Final segment created. Start: %d, End: %d, Length: %d bytes, Tokens: %d, Preview: %s\",\r\n\t\t\tcurrentStart, currentStart+currentSegment.Len(), len(segmentContent), currentTokenCount,\r\n\t\t\ttruncateString(string(segmentContent), 100)))\r\n\t}\r\n\r\n\tlog.Info(fmt.Sprintf(i18n.Messages.LogSegmentationCompleted, len(segments)))\r\n\treturn segments, nil\r\n}\r\nfunc splitIntoSentences(content []byte) [][]byte {\r\n\tvar sentences [][]byte\r\n\tvar currentSentence []byte\r\n\r\n\tfor _, b := range content {\r\n\t\tcurrentSentence = append(currentSentence, b)\r\n\t\tif b == '.' || b == '!' || b == '?' {\r\n\t\t\tsentences = append(sentences, currentSentence)\r\n\t\t\tcurrentSentence = []byte{}\r\n\t\t}\r\n\t}\r\n\r\n\tif len(currentSentence) \u003e 0 {\r\n\t\tsentences = append(sentences, currentSentence)\r\n\t}\r\n\r\n\treturn sentences\r\n}\r\n\r\n// CountTokens returns the number of tokens in the content\r\nfunc CountTokens(content []byte, tokenizer *tiktoken.Tiktoken) int {\r\n\ttokens := tokenizer.Encode(string(content), nil, nil)\r\n\tcount := len(tokens)\r\n\treturn count\r\n}\r\n\r\n// getTokenizer returns a tokenizer for the specified model\r\nfunc getTokenizer(model string) (*tiktoken.Tiktoken, error) {\r\n\tencoding, err := tiktoken.GetEncoding(\"cl100k_base\")\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.Messages.ErrTokenizerInitialization, err)\r\n\t}\r\n\treturn encoding, nil\r\n}\r\n\r\n// CalibrateTokenCount adjusts the token count based on the LLM model\r\nfunc CalibrateTokenCount(count int, model string) int {\r\n\tlog.Debug(\"Calibrating token count for model %s. Original count: %d\", model, count)\r\n\t// Implement model-specific calibration logic here\r\n\t// For now, we'll just return the original count\r\n\tlog.Debug(\"Calibrated count: %d\", count)\r\n\treturn count\r\n}\r\n\r\n// GetContext returns the context of previous segments\r\nfunc GetContext(segments []SegmentInfo, currentIndex int, cfg SegmentConfig) string {\r\n\tlog.Debug(\"GetContext called for segment %d/%d\", currentIndex+1, len(segments))\r\n\r\n\tvar context bytes.Buffer\r\n\ttokenCount := 0\r\n\tsegmentsUsed := 0\r\n\r\n\ttokenizer, err := getTokenizer(cfg.Model)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to get tokenizer: %v\", err)\r\n\t\treturn \"\"\r\n\t}\r\n\r\n\tfor i := currentIndex - 1; i \u003e= 0 \u0026\u0026 tokenCount \u003c cfg.ContextSize; i-- {\r\n\t\tsegmentTokens := CountTokens(segments[i].Content, tokenizer)\r\n\t\tif tokenCount+segmentTokens \u003e cfg.ContextSize {\r\n\t\t\tbreak\r\n\t\t}\r\n\t\t// Prepend this segment to the context\r\n\t\ttemp := make([]byte, len(segments[i].Content)+1)\r\n\t\tcopy(temp[1:], segments[i].Content)\r\n\t\ttemp[0] = '\\n'\r\n\t\tcontext.Write(temp)\r\n\t\ttokenCount += segmentTokens\r\n\t\tsegmentsUsed++\r\n\t}\r\n\r\n\tlog.Debug(\"Generated context for segment %d/%d. Segments used: %d, Token count: %d, Context length: %d bytes\",\r\n\t\tcurrentIndex+1, len(segments), segmentsUsed, tokenCount, context.Len())\r\n\tlog.Debug(\"Context preview for segment %d: %s\",\r\n\t\tcurrentIndex+1, truncateString(context.String(), 100))\r\n\r\n\treturn context.String()\r\n}\r\n\r\nfunc truncateString(s string, maxLength int) string {\r\n\tif len(s) \u003c= maxLength {\r\n\t\treturn s\r\n\t}\r\n\treturn s[:maxLength] + \"...\"\r\n}\r\n",
    "size": 6140,
    "modTime": "2024-10-30T22:49:14.3368111+01:00",
    "path": "internal\\segmenter\\segmenter.go"
  },
  {
    "name": "detector.go",
    "content": "// internal/storage/detector.go\r\n\r\npackage storage\r\n\r\nimport (\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n)\r\n\r\n// DetectStorageType détermine le type de stockage basé sur le chemin d'entrée\r\nfunc DetectStorageType(path string) string {\r\n\tlog.Debug(\"Detecting storage type for path: %s\", path)\r\n\r\n\t// Normaliser le chemin pour gérer les chemins Windows\r\n\tnormalizedPath := filepath.ToSlash(path)\r\n\r\n\tif strings.HasPrefix(strings.ToLower(normalizedPath), \"s3://\") {\r\n\t\tlog.Debug(\"Detected S3 storage type\")\r\n\t\treturn S3StorageType\r\n\t}\r\n\tlog.Debug(\"Detected Local storage type\")\r\n\treturn LocalStorageType\r\n}\r\n",
    "size": 598,
    "modTime": "2024-11-04T19:38:48.2068634+01:00",
    "path": "internal\\storage\\detector.go"
  },
  {
    "name": "errors.go",
    "content": "// internal/storage/errors.go\r\n\r\npackage storage\r\n\r\nimport \"errors\"\r\n\r\nvar (\r\n    // ErrInvalidS3URI est renvoyée lorsqu'une URI S3 est invalide\r\n    ErrInvalidS3URI = errors.New(\"invalid S3 URI format\")\r\n)",
    "size": 207,
    "modTime": "2024-11-04T14:09:24.9274288+01:00",
    "path": "internal\\storage\\errors.go"
  },
  {
    "name": "factory.go",
    "content": "// internal/storage/factory.go\r\n\r\npackage storage\r\n\r\nimport (\r\n\t\"fmt\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\n// NewStorage crée et retourne une instance de Storage basée sur la configuration\r\nfunc NewStorage(cfg *config.Config, inputPath string) (Storage, error) {\r\n\tlog := logger.GetLogger()\r\n\r\n\tstorageType := DetectStorageType(inputPath)\r\n\tlog.Debug(\"Creating new storage with type: %s\", storageType)\r\n\r\n\tswitch storageType {\r\n\tcase LocalStorageType:\r\n\t\tlog.Debug(\"Creating Local storage\")\r\n\t\treturn NewLocalStorage(cfg.Storage.LocalPath, logger.GetLogger()), nil\r\n\tcase S3StorageType:\r\n\t\tlog.Debug(\"Creating S3 storage\")\r\n\t\treturn NewS3Storage(\r\n\t\t\tcfg.Storage.S3.Region,\r\n\t\t\tcfg.Storage.S3.Endpoint,\r\n\t\t\tcfg.Storage.S3.AccessKeyID,\r\n\t\t\tcfg.Storage.S3.SecretAccessKey,\r\n\t\t\tlog,\r\n\t\t)\r\n\tdefault:\r\n\t\treturn nil, fmt.Errorf(\"unsupported storage type: %s\", storageType)\r\n\t}\r\n}\r\n",
    "size": 946,
    "modTime": "2024-11-05T18:50:34.7186176+01:00",
    "path": "internal\\storage\\factory.go"
  },
  {
    "name": "local.go",
    "content": "// internal/storage/local.go\r\n\r\npackage storage\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"io/ioutil\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\ntype LocalStorage struct {\r\n\tbasePath string\r\n\tlogger   *logger.Logger\r\n}\r\n\r\ntype localFileInfo struct {\r\n\tos.FileInfo\r\n}\r\n\r\nfunc NewLocalStorage(basePath string, logger *logger.Logger) *LocalStorage {\r\n\treturn \u0026LocalStorage{\r\n\t\tbasePath: basePath,\r\n\t\tlogger:   logger,\r\n\t}\r\n}\r\n\r\nfunc (ls *LocalStorage) Read(path string) ([]byte, error) {\r\n\tls.logger.Debug(\"Reading from local storage: %s\", path)\r\n\tfullPath := ls.getFullPath(path)\r\n\tls.logger.Debug(\"Full path for reading: %s\", fullPath)\r\n\r\n\tfileInfo, err := os.Stat(fullPath)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to get file info: %w\", err)\r\n\t}\r\n\r\n\tif fileInfo.IsDir() {\r\n\t\treturn ls.readDirectory(fullPath)\r\n\t}\r\n\r\n\treturn ioutil.ReadFile(fullPath)\r\n}\r\n\r\nfunc (ls *LocalStorage) readDirectory(dirPath string) ([]byte, error) {\r\n\tvar content []byte\r\n\terr := filepath.Walk(dirPath, func(path string, info os.FileInfo, err error) error {\r\n\t\tif err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t\tif !info.IsDir() {\r\n\t\t\tfileContent, err := ioutil.ReadFile(path)\r\n\t\t\tif err != nil {\r\n\t\t\t\treturn err\r\n\t\t\t}\r\n\t\t\tcontent = append(content, fileContent...)\r\n\t\t\tcontent = append(content, '\\n') // Add newline between files\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to read directory: %w\", err)\r\n\t}\r\n\treturn content, nil\r\n}\r\n\r\nfunc (ls *LocalStorage) Write(path string, data []byte) error {\r\n\tls.logger.Debug(\"Writing file: %s\", path)\r\n\tfullPath := ls.getFullPath(path)\r\n\tls.logger.Debug(\"Full path for writing: %s\", fullPath)\r\n\r\n\t// Assurez-vous que le répertoire existe\r\n\tdir := filepath.Dir(fullPath)\r\n\tif err := os.MkdirAll(dir, 0755); err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\treturn ioutil.WriteFile(fullPath, data, 0644)\r\n}\r\n\r\n// getFullPath gère la conversion des chemins relatifs en chemins absolus\r\nfunc (ls *LocalStorage) getFullPath(path string) string {\r\n\tif filepath.IsAbs(path) {\r\n\t\treturn filepath.Clean(path)\r\n\t}\r\n\treturn filepath.Clean(filepath.Join(ls.basePath, path))\r\n}\r\n\r\nfunc (ls *LocalStorage) List(prefix string) ([]string, error) {\r\n\tls.logger.Debug(\"Listing files with prefix: %s\", prefix)\r\n\r\n\tfullPath := ls.getFullPath(prefix)\r\n\tls.logger.Debug(\"Full path for listing: %s\", fullPath)\r\n\r\n\tvar files []string\r\n\terr := filepath.Walk(fullPath, func(path string, info os.FileInfo, err error) error {\r\n\t\tif err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t\tif !info.IsDir() {\r\n\t\t\t// Retourner le chemin complet au lieu du chemin relatif\r\n\t\t\tfiles = append(files, path)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to list directory contents: %w\", err)\r\n\t}\r\n\r\n\tls.logger.Debug(\"Listed %d files\", len(files))\r\n\treturn files, nil\r\n}\r\n\r\nfunc (ls *LocalStorage) Delete(path string) error {\r\n\tls.logger.Debug(\"Deleting file: %s\", path)\r\n\tfullPath := filepath.Join(ls.basePath, path)\r\n\treturn os.Remove(fullPath)\r\n}\r\n\r\nfunc (ls *LocalStorage) Exists(path string) (bool, error) {\r\n\tls.logger.Debug(\"Checking if file exists: %s\", path)\r\n\tfullPath := filepath.Join(ls.basePath, path)\r\n\t_, err := os.Stat(fullPath)\r\n\tif err == nil {\r\n\t\treturn true, nil\r\n\t}\r\n\tif os.IsNotExist(err) {\r\n\t\treturn false, nil\r\n\t}\r\n\treturn false, err\r\n}\r\n\r\nfunc (ls *LocalStorage) IsDirectory(path string) (bool, error) {\r\n\tls.logger.Debug(\"Checking if path is a directory: %s\", path)\r\n\r\n\t// Utiliser le chemin tel quel s'il est absolu, sinon le joindre au chemin de base\r\n\tfullPath := path\r\n\tif !filepath.IsAbs(path) {\r\n\t\tfullPath = filepath.Join(ls.basePath, path)\r\n\t}\r\n\r\n\tls.logger.Debug(\"Full path for directory check: %s\", fullPath)\r\n\r\n\tfileInfo, err := os.Stat(fullPath)\r\n\tif err != nil {\r\n\t\tif os.IsNotExist(err) {\r\n\t\t\tls.logger.Debug(\"Path does not exist: %s\", fullPath)\r\n\t\t\treturn false, nil\r\n\t\t}\r\n\t\tls.logger.Error(\"Error checking directory: %v\", err)\r\n\t\treturn false, err\r\n\t}\r\n\r\n\tisDir := fileInfo.IsDir()\r\n\tls.logger.Debug(\"Is directory: %v\", isDir)\r\n\treturn isDir, nil\r\n}\r\n\r\nfunc (ls *LocalStorage) GetReader(path string) (io.ReadCloser, error) {\r\n\tls.logger.Debug(\"Getting reader for local file: %s\", path)\r\n\tfullPath := ls.getFullPath(path)\r\n\treturn os.Open(fullPath)\r\n}\r\n\r\nfunc (ls *LocalStorage) Stat(path string) (FileInfo, error) {\r\n\tls.logger.Debug(\"Getting file info: %s\", path)\r\n\tfullPath := ls.getFullPath(path)\r\n\tinfo, err := os.Stat(fullPath)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to get file info: %w\", err)\r\n\t}\r\n\treturn \u0026localFileInfo{info}, nil\r\n}\r\n\r\n// Assurez-vous que localFileInfo implémente toutes les méthodes de FileInfo\r\nfunc (lfi *localFileInfo) Name() string       { return lfi.FileInfo.Name() }\r\nfunc (lfi *localFileInfo) Size() int64        { return lfi.FileInfo.Size() }\r\nfunc (lfi *localFileInfo) Mode() os.FileMode  { return lfi.FileInfo.Mode() }\r\nfunc (lfi *localFileInfo) ModTime() time.Time { return lfi.FileInfo.ModTime() }\r\nfunc (lfi *localFileInfo) IsDir() bool        { return lfi.FileInfo.IsDir() }\r\nfunc (lfi *localFileInfo) Sys() interface{}   { return lfi.FileInfo.Sys() }\r\n",
    "size": 5100,
    "modTime": "2024-11-05T23:17:30.9132253+01:00",
    "path": "internal\\storage\\local.go"
  },
  {
    "name": "s3.go",
    "content": "// internal/storage/s3.go\r\n\r\npackage storage\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"context\"\r\n\t\"errors\"\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"io/ioutil\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n\t\"time\"\r\n\r\n\t\"github.com/aws/aws-sdk-go-v2/aws\"\r\n\t\"github.com/aws/aws-sdk-go-v2/config\"\r\n\t\"github.com/aws/aws-sdk-go-v2/credentials\"\r\n\t\"github.com/aws/aws-sdk-go-v2/service/s3\"\r\n\t\"github.com/aws/aws-sdk-go-v2/service/s3/types\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\ntype S3ClientInterface interface {\r\n\tGetObject(ctx context.Context, params *s3.GetObjectInput, optFns ...func(*s3.Options)) (*s3.GetObjectOutput, error)\r\n\tPutObject(ctx context.Context, params *s3.PutObjectInput, optFns ...func(*s3.Options)) (*s3.PutObjectOutput, error)\r\n\tListObjectsV2(ctx context.Context, params *s3.ListObjectsV2Input, optFns ...func(*s3.Options)) (*s3.ListObjectsV2Output, error)\r\n\tDeleteObject(ctx context.Context, params *s3.DeleteObjectInput, optFns ...func(*s3.Options)) (*s3.DeleteObjectOutput, error)\r\n\tHeadObject(ctx context.Context, params *s3.HeadObjectInput, optFns ...func(*s3.Options)) (*s3.HeadObjectOutput, error)\r\n}\r\n\r\ntype S3Storage struct {\r\n\tclient S3ClientInterface\r\n\tbucket string\r\n\tlogger Logger\r\n}\r\n\r\ntype s3FileInfo struct {\r\n\tname    string\r\n\tsize    int64\r\n\tmodTime time.Time\r\n}\r\n\r\nfunc (fi *s3FileInfo) Name() string       { return fi.name }\r\nfunc (fi *s3FileInfo) Size() int64        { return fi.size }\r\nfunc (fi *s3FileInfo) Mode() os.FileMode  { return 0 }\r\nfunc (fi *s3FileInfo) ModTime() time.Time { return fi.modTime }\r\nfunc (fi *s3FileInfo) IsDir() bool        { return false }\r\nfunc (fi *s3FileInfo) Sys() interface{}   { return nil }\r\n\r\nfunc NewS3Storage(region, endpoint, accessKeyID, secretAccessKey string, logger *logger.Logger) (*S3Storage, error) {\r\n\tlogger.Debug(\"Initializing S3 storage with region: %s, endpoint: %s\", region, endpoint)\r\n\r\n\tcfg, err := config.LoadDefaultConfig(context.TODO(),\r\n\t\tconfig.WithRegion(region),\r\n\t\tconfig.WithEndpointResolver(aws.EndpointResolverFunc(\r\n\t\t\tfunc(service, region string) (aws.Endpoint, error) {\r\n\t\t\t\treturn aws.Endpoint{URL: endpoint}, nil\r\n\t\t\t})),\r\n\t\tconfig.WithCredentialsProvider(credentials.NewStaticCredentialsProvider(accessKeyID, secretAccessKey, \"\")),\r\n\t)\r\n\tif err != nil {\r\n\t\tlogger.Error(\"Failed to load S3 config: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to load S3 config: %w\", err)\r\n\t}\r\n\r\n\tclient := s3.NewFromConfig(cfg, func(o *s3.Options) {\r\n\t\to.UsePathStyle = true\r\n\t})\r\n\r\n\treturn \u0026S3Storage{\r\n\t\tclient: client,\r\n\t\tlogger: logger,\r\n\t}, nil\r\n}\r\n\r\nfunc (s *S3Storage) Read(path string) ([]byte, error) {\r\n\ts.logger.Debug(\"Reading from S3: %s\", path)\r\n\tbucket, key, err := ParseS3URI(path)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to parse S3 URI: %w\", err)\r\n\t}\r\n\r\n\ts.logger.Debug(\"Parsed S3 path - Bucket: %s, Key: %s\", bucket, key)\r\n\r\n\tresult, err := s.client.GetObject(context.TODO(), \u0026s3.GetObjectInput{\r\n\t\tBucket: aws.String(bucket),\r\n\t\tKey:    aws.String(key),\r\n\t})\r\n\tif err != nil {\r\n\t\ts.logger.Error(\"Failed to read file from S3: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to read file from S3: %w\", err)\r\n\t}\r\n\tdefer result.Body.Close()\r\n\r\n\tcontent, err := ioutil.ReadAll(result.Body)\r\n\tif err != nil {\r\n\t\ts.logger.Error(\"Failed to read content from S3 object: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to read content from S3 object: %w\", err)\r\n\t}\r\n\r\n\ts.logger.Debug(\"Successfully read %d bytes from S3\", len(content))\r\n\treturn content, nil\r\n}\r\n\r\nfunc (s *S3Storage) Write(path string, data []byte) error {\r\n\ts.logger.Debug(\"Writing file to S3: %s\", path)\r\n\r\n\tbucket, key, err := ParseS3URI(path)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"failed to parse S3 URI: %w\", err)\r\n\t}\r\n\r\n\t_, err = s.client.PutObject(context.TODO(), \u0026s3.PutObjectInput{\r\n\t\tBucket: aws.String(bucket),\r\n\t\tKey:    aws.String(key),\r\n\t\tBody:   bytes.NewReader(data),\r\n\t})\r\n\tif err != nil {\r\n\t\ts.logger.Error(\"Failed to write file to S3: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to write file to S3: %w\", err)\r\n\t}\r\n\r\n\ts.logger.Debug(\"Successfully wrote %d bytes to S3\", len(data))\r\n\treturn nil\r\n}\r\n\r\nfunc (s *S3Storage) List(prefix string) ([]string, error) {\r\n\ts.logger.Debug(\"Listing files in S3 with prefix: %s\", prefix)\r\n\r\n\tbucket, key, err := ParseS3URI(prefix)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to parse S3 URI: %w\", err)\r\n\t}\r\n\r\n\tvar files []string\r\n\tpaginator := s3.NewListObjectsV2Paginator(s.client, \u0026s3.ListObjectsV2Input{\r\n\t\tBucket: aws.String(bucket),\r\n\t\tPrefix: aws.String(key),\r\n\t})\r\n\r\n\t// Extrayez le domaine de l'URL originale\r\n\tdomainParts := strings.SplitN(prefix, \"/\", 4)\r\n\tdomain := strings.Join(domainParts[:3], \"/\")\r\n\r\n\tfor paginator.HasMorePages() {\r\n\t\tpage, err := paginator.NextPage(context.TODO())\r\n\t\tif err != nil {\r\n\t\t\ts.logger.Error(\"Failed to list files in S3: %v\", err)\r\n\t\t\treturn nil, fmt.Errorf(\"failed to list files in S3: %w\", err)\r\n\t\t}\r\n\t\tfor _, obj := range page.Contents {\r\n\t\t\tif !strings.HasSuffix(*obj.Key, \"/\") { // Ignore directory markers\r\n\t\t\t\tfiles = append(files, fmt.Sprintf(\"%s/%s/%s\", domain, bucket, *obj.Key))\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\treturn files, nil\r\n}\r\n\r\nfunc (s *S3Storage) Delete(path string) error {\r\n\ts.logger.Debug(\"Deleting file from S3: %s\", path)\r\n\r\n\t_, err := s.client.DeleteObject(context.TODO(), \u0026s3.DeleteObjectInput{\r\n\t\tBucket: aws.String(s.bucket),\r\n\t\tKey:    aws.String(path),\r\n\t})\r\n\tif err != nil {\r\n\t\ts.logger.Error(\"Failed to delete file from S3: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to delete file from S3: %w\", err)\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\nfunc (s *S3Storage) Exists(path string) (bool, error) {\r\n\ts.logger.Debug(\"Checking if file exists in S3: %s\", path)\r\n\r\n\t_, err := s.client.HeadObject(context.TODO(), \u0026s3.HeadObjectInput{\r\n\t\tBucket: aws.String(s.bucket),\r\n\t\tKey:    aws.String(path),\r\n\t})\r\n\tif err != nil {\r\n\t\tvar nsk *types.NoSuchKey\r\n\t\tif errors.As(err, \u0026nsk) || strings.Contains(err.Error(), \"NotFound\") || strings.Contains(err.Error(), \"404\") {\r\n\t\t\treturn false, nil\r\n\t\t}\r\n\t\ts.logger.Error(\"Error checking if file exists in S3: %v\", err)\r\n\t\treturn false, fmt.Errorf(\"error checking if file exists in S3: %w\", err)\r\n\t}\r\n\r\n\treturn true, nil\r\n}\r\n\r\nfunc (s *S3Storage) IsDirectory(path string) (bool, error) {\r\n\ts.logger.Debug(\"Checking if path is a directory in S3: %s\", path)\r\n\r\n\tbucket, key, err := ParseS3URI(path)\r\n\tif err != nil {\r\n\t\treturn false, fmt.Errorf(\"failed to parse S3 URI: %w\", err)\r\n\t}\r\n\ts.logger.Error(\"Parsed S3 URI: %s %s\", bucket, key)\r\n\r\n\t// Assurez-vous que la clé se termine par '/'\r\n\tif !strings.HasSuffix(key, \"/\") {\r\n\t\tkey += \"/\"\r\n\t}\r\n\r\n\ts.logger.Debug(\"Checking key %s on bucket %s for path %s\", key, bucket, path)\r\n\r\n\tresult, err := s.client.ListObjectsV2(context.TODO(), \u0026s3.ListObjectsV2Input{\r\n\t\tBucket:    aws.String(bucket),\r\n\t\tPrefix:    aws.String(key),\r\n\t\tDelimiter: aws.String(\"/\"),\r\n\t\tMaxKeys:   aws.Int32(1),\r\n\t})\r\n\tif err != nil {\r\n\t\ts.logger.Error(\"Error checking if path is a directory in S3: %v\", err)\r\n\t\treturn false, fmt.Errorf(\"error checking if path is a directory in S3: %w\", err)\r\n\t}\r\n\r\n\treturn len(result.CommonPrefixes) \u003e 0 || len(result.Contents) \u003e 0, nil\r\n}\r\n\r\nfunc (s *S3Storage) Stat(path string) (FileInfo, error) {\r\n\ts.logger.Debug(\"Getting file info from S3: %s\", path)\r\n\r\n\tresult, err := s.client.HeadObject(context.TODO(), \u0026s3.HeadObjectInput{\r\n\t\tBucket: aws.String(s.bucket),\r\n\t\tKey:    aws.String(path),\r\n\t})\r\n\tif err != nil {\r\n\t\ts.logger.Error(\"Failed to get file info from S3: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to get file info from S3: %w\", err)\r\n\t}\r\n\r\n\tsize := result.ContentLength\r\n\tif size == nil {\r\n\t\tsize = aws.Int64(0)\r\n\t}\r\n\treturn \u0026s3FileInfo{\r\n\t\tname:    filepath.Base(path),\r\n\t\tsize:    *size,\r\n\t\tmodTime: *result.LastModified,\r\n\t}, nil\r\n}\r\n\r\nfunc (s *S3Storage) ReadFromBucket(bucket, key string) ([]byte, error) {\r\n\ts.logger.Debug(\"Reading file from S3: bucket=%s, key=%s\", bucket, key)\r\n\r\n\tresult, err := s.client.GetObject(context.TODO(), \u0026s3.GetObjectInput{\r\n\t\tBucket: aws.String(bucket),\r\n\t\tKey:    aws.String(key),\r\n\t})\r\n\tif err != nil {\r\n\t\ts.logger.Error(\"Failed to read file from S3: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to read file from S3: %w\", err)\r\n\t}\r\n\tdefer result.Body.Close()\r\n\r\n\treturn ioutil.ReadAll(result.Body)\r\n}\r\n\r\nfunc (s *S3Storage) StatObject(bucket, key string) (os.FileInfo, error) {\r\n\ts.logger.Debug(\"Getting file info from S3 - Bucket: %s, Key: %s\", bucket, key)\r\n\r\n\tresult, err := s.client.HeadObject(context.TODO(), \u0026s3.HeadObjectInput{\r\n\t\tBucket: aws.String(bucket),\r\n\t\tKey:    aws.String(key),\r\n\t})\r\n\tif err != nil {\r\n\t\ts.logger.Error(\"Failed to get file info from S3: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to get file info from S3: %w\", err)\r\n\t}\r\n\r\n\treturn \u0026s3FileInfo{\r\n\t\tname:    filepath.Base(key),\r\n\t\tsize:    *result.ContentLength,\r\n\t\tmodTime: *result.LastModified,\r\n\t}, nil\r\n}\r\n\r\nfunc ParseS3URI(uri string) (bucket, key string, err error) {\r\n\tif !strings.HasPrefix(strings.ToLower(uri), \"s3://\") {\r\n\t\treturn \"\", \"\", fmt.Errorf(\"invalid S3 URI format on prefix\")\r\n\t}\r\n\r\n\t// Remove the \"s3://\" prefix\r\n\turi = strings.TrimPrefix(uri, \"s3://\")\r\n\r\n\t// Split the remaining string into parts\r\n\tparts := strings.SplitN(uri, \"/\", 3)\r\n\r\n\tif len(parts) \u003c 3 {\r\n\t\treturn \"\", \"\", fmt.Errorf(\"invalid S3 URI format on len(part) : %s\", uri)\r\n\t}\r\n\r\n\t// The first part is the domain\r\n\t// The second part is the bucket\r\n\tbucket = parts[1]\r\n\t// The key is everything after the bucket\r\n\tkey = parts[2]\r\n\r\n\treturn bucket, key, nil\r\n}\r\n\r\nfunc (s *S3Storage) readS3Directory(bucket, prefix string) ([]byte, error) {\r\n\ts.logger.Debug(\"Reading S3 directory: bucket=%s, prefix=%s\", bucket, prefix)\r\n\tvar content []byte\r\n\tpaginator := s3.NewListObjectsV2Paginator(s.client, \u0026s3.ListObjectsV2Input{\r\n\t\tBucket: aws.String(bucket),\r\n\t\tPrefix: aws.String(prefix),\r\n\t})\r\n\r\n\tfor paginator.HasMorePages() {\r\n\t\tpage, err := paginator.NextPage(context.TODO())\r\n\t\tif err != nil {\r\n\t\t\treturn nil, fmt.Errorf(\"failed to list S3 objects: %w\", err)\r\n\t\t}\r\n\r\n\t\tfor _, obj := range page.Contents {\r\n\t\t\tif !strings.HasSuffix(*obj.Key, \"/\") { // Skip directory markers\r\n\t\t\t\tobjContent, err := s.getS3ObjectContent(bucket, *obj.Key)\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\treturn nil, err\r\n\t\t\t\t}\r\n\t\t\t\tcontent = append(content, objContent...)\r\n\t\t\t\tcontent = append(content, '\\n') // Add newline between files\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\tif len(content) == 0 {\r\n\t\treturn nil, fmt.Errorf(\"no content found in S3 directory: %s/%s\", bucket, prefix)\r\n\t}\r\n\r\n\ts.logger.Debug(\"Successfully read %d bytes from S3 directory\", len(content))\r\n\treturn content, nil\r\n}\r\n\r\nfunc (s *S3Storage) getS3ObjectContent(bucket, key string) ([]byte, error) {\r\n\tresult, err := s.client.GetObject(context.TODO(), \u0026s3.GetObjectInput{\r\n\t\tBucket: aws.String(bucket),\r\n\t\tKey:    aws.String(key),\r\n\t})\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to get S3 object: %w\", err)\r\n\t}\r\n\tdefer result.Body.Close()\r\n\r\n\treturn ioutil.ReadAll(result.Body)\r\n}\r\n\r\nfunc (s *S3Storage) GetReader(path string) (io.ReadCloser, error) {\r\n\ts.logger.Debug(\"Getting reader for S3 object: %s\", path)\r\n\tbucket, key, err := ParseS3URI(path)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to parse S3 URI: %w\", err)\r\n\t}\r\n\r\n\tresult, err := s.client.GetObject(context.TODO(), \u0026s3.GetObjectInput{\r\n\t\tBucket: aws.String(bucket),\r\n\t\tKey:    aws.String(key),\r\n\t})\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to get S3 object: %w\", err)\r\n\t}\r\n\r\n\treturn result.Body, nil\r\n}\r\n",
    "size": 11350,
    "modTime": "2024-11-05T19:20:57.630719+01:00",
    "path": "internal\\storage\\s3.go"
  },
  {
    "name": "s3_test.go",
    "content": "// internal/storage/s3_test.go\r\n\r\npackage storage\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"context\"\r\n\t\"io/ioutil\"\r\n\t\"testing\"\r\n\t\"time\"\r\n\r\n\t\"github.com/aws/aws-sdk-go-v2/aws\"\r\n\t\"github.com/aws/aws-sdk-go-v2/service/s3\"\r\n\t\"github.com/aws/aws-sdk-go-v2/service/s3/types\"\r\n\t\"github.com/stretchr/testify/assert\"\r\n\t\"github.com/stretchr/testify/mock\"\r\n)\r\n\r\n// MockLogger est un mock de l'interface Logger\r\ntype MockLogger struct {\r\n\tmock.Mock\r\n}\r\n\r\nfunc (m *MockLogger) Debug(format string, args ...interface{}) {\r\n\tm.Called(format, args)\r\n}\r\n\r\nfunc (m *MockLogger) Info(format string, args ...interface{}) {\r\n\tm.Called(format, args)\r\n}\r\n\r\nfunc (m *MockLogger) Warning(format string, args ...interface{}) {\r\n\tm.Called(format, args)\r\n}\r\n\r\nfunc (m *MockLogger) Error(format string, args ...interface{}) {\r\n\tm.Called(format, args)\r\n}\r\n\r\n// MockS3Client est un mock du client S3\r\ntype MockS3Client struct {\r\n\tmock.Mock\r\n}\r\n\r\nfunc (m *MockS3Client) GetObject(ctx context.Context, params *s3.GetObjectInput, optFns ...func(*s3.Options)) (*s3.GetObjectOutput, error) {\r\n\targs := m.Called(ctx, params, optFns)\r\n\treturn args.Get(0).(*s3.GetObjectOutput), args.Error(1)\r\n}\r\n\r\nfunc (m *MockS3Client) PutObject(ctx context.Context, params *s3.PutObjectInput, optFns ...func(*s3.Options)) (*s3.PutObjectOutput, error) {\r\n\targs := m.Called(ctx, params, optFns)\r\n\treturn args.Get(0).(*s3.PutObjectOutput), args.Error(1)\r\n}\r\n\r\nfunc (m *MockS3Client) ListObjectsV2(ctx context.Context, params *s3.ListObjectsV2Input, optFns ...func(*s3.Options)) (*s3.ListObjectsV2Output, error) {\r\n\targs := m.Called(ctx, params, optFns)\r\n\treturn args.Get(0).(*s3.ListObjectsV2Output), args.Error(1)\r\n}\r\n\r\nfunc (m *MockS3Client) DeleteObject(ctx context.Context, params *s3.DeleteObjectInput, optFns ...func(*s3.Options)) (*s3.DeleteObjectOutput, error) {\r\n\targs := m.Called(ctx, params, optFns)\r\n\treturn args.Get(0).(*s3.DeleteObjectOutput), args.Error(1)\r\n}\r\n\r\nfunc (m *MockS3Client) HeadObject(ctx context.Context, params *s3.HeadObjectInput, optFns ...func(*s3.Options)) (*s3.HeadObjectOutput, error) {\r\n\targs := m.Called(ctx, params, optFns)\r\n\treturn args.Get(0).(*s3.HeadObjectOutput), args.Error(1)\r\n}\r\n\r\nfunc TestS3StorageRead(t *testing.T) {\r\n\tmockClient := new(MockS3Client)\r\n\tmockLogger := new(MockLogger)\r\n\ts3Storage := \u0026S3Storage{\r\n\t\tclient: mockClient,\r\n\t\tbucket: \"test-bucket\",\r\n\t\tlogger: mockLogger,\r\n\t}\r\n\r\n\texpectedContent := []byte(\"test content\")\r\n\tmockClient.On(\"GetObject\", mock.Anything, mock.Anything, mock.Anything).Return(\r\n\t\t\u0026s3.GetObjectOutput{\r\n\t\t\tBody: ioutil.NopCloser(bytes.NewReader(expectedContent)),\r\n\t\t},\r\n\t\tnil,\r\n\t)\r\n\r\n\tmockLogger.On(\"Debug\", mock.Anything, mock.Anything).Return()\r\n\r\n\tcontent, err := s3Storage.Read(\"test.txt\")\r\n\r\n\tassert.NoError(t, err)\r\n\tassert.Equal(t, expectedContent, content)\r\n\tmockClient.AssertExpectations(t)\r\n\tmockLogger.AssertExpectations(t)\r\n}\r\n\r\nfunc TestS3StorageWrite(t *testing.T) {\r\n\tmockClient := new(MockS3Client)\r\n\tmockLogger := new(MockLogger)\r\n\ts3Storage := \u0026S3Storage{\r\n\t\tclient: mockClient,\r\n\t\tbucket: \"test-bucket\",\r\n\t\tlogger: mockLogger,\r\n\t}\r\n\r\n\tcontent := []byte(\"test content\")\r\n\tmockClient.On(\"PutObject\", mock.Anything, mock.Anything, mock.Anything).Return(\r\n\t\t\u0026s3.PutObjectOutput{},\r\n\t\tnil,\r\n\t)\r\n\r\n\tmockLogger.On(\"Debug\", mock.Anything, mock.Anything).Return()\r\n\r\n\terr := s3Storage.Write(\"test.txt\", content)\r\n\r\n\tassert.NoError(t, err)\r\n\tmockClient.AssertExpectations(t)\r\n\tmockLogger.AssertExpectations(t)\r\n}\r\n\r\nfunc TestS3StorageList(t *testing.T) {\r\n\tmockClient := new(MockS3Client)\r\n\tmockLogger := new(MockLogger)\r\n\ts3Storage := \u0026S3Storage{\r\n\t\tclient: mockClient,\r\n\t\tbucket: \"test-bucket\",\r\n\t\tlogger: mockLogger,\r\n\t}\r\n\r\n\tmockClient.On(\"ListObjectsV2\", mock.Anything, mock.Anything, mock.Anything).Return(\r\n\t\t\u0026s3.ListObjectsV2Output{\r\n\t\t\tContents: []types.Object{\r\n\t\t\t\t{Key: aws.String(\"file1.txt\")},\r\n\t\t\t\t{Key: aws.String(\"file2.txt\")},\r\n\t\t\t},\r\n\t\t},\r\n\t\tnil,\r\n\t)\r\n\r\n\tmockLogger.On(\"Debug\", mock.Anything, mock.Anything).Return()\r\n\r\n\tfiles, err := s3Storage.List(\"prefix\")\r\n\r\n\tassert.NoError(t, err)\r\n\tassert.Equal(t, []string{\"file1.txt\", \"file2.txt\"}, files)\r\n\tmockClient.AssertExpectations(t)\r\n\tmockLogger.AssertExpectations(t)\r\n}\r\n\r\nfunc TestS3StorageDelete(t *testing.T) {\r\n\tmockClient := new(MockS3Client)\r\n\tmockLogger := new(MockLogger)\r\n\ts3Storage := \u0026S3Storage{\r\n\t\tclient: mockClient,\r\n\t\tbucket: \"test-bucket\",\r\n\t\tlogger: mockLogger,\r\n\t}\r\n\r\n\tmockClient.On(\"DeleteObject\", mock.Anything, mock.Anything, mock.Anything).Return(\r\n\t\t\u0026s3.DeleteObjectOutput{},\r\n\t\tnil,\r\n\t)\r\n\r\n\tmockLogger.On(\"Debug\", mock.Anything, mock.Anything).Return()\r\n\r\n\terr := s3Storage.Delete(\"test.txt\")\r\n\r\n\tassert.NoError(t, err)\r\n\tmockClient.AssertExpectations(t)\r\n\tmockLogger.AssertExpectations(t)\r\n}\r\n\r\nfunc TestS3StorageExists(t *testing.T) {\r\n\tmockClient := new(MockS3Client)\r\n\tmockLogger := new(MockLogger)\r\n\ts3Storage := \u0026S3Storage{\r\n\t\tclient: mockClient,\r\n\t\tbucket: \"test-bucket\",\r\n\t\tlogger: mockLogger,\r\n\t}\r\n\r\n\tmockClient.On(\"HeadObject\", mock.Anything, mock.Anything, mock.Anything).Return(\r\n\t\t\u0026s3.HeadObjectOutput{},\r\n\t\tnil,\r\n\t)\r\n\r\n\tmockLogger.On(\"Debug\", mock.Anything, mock.Anything).Return()\r\n\r\n\texists, err := s3Storage.Exists(\"test.txt\")\r\n\r\n\tassert.NoError(t, err)\r\n\tassert.True(t, exists)\r\n\tmockClient.AssertExpectations(t)\r\n\tmockLogger.AssertExpectations(t)\r\n}\r\n\r\nfunc TestS3StorageIsDirectory(t *testing.T) {\r\n\tmockClient := new(MockS3Client)\r\n\tmockLogger := new(MockLogger)\r\n\ts3Storage := \u0026S3Storage{\r\n\t\tclient: mockClient,\r\n\t\tbucket: \"test-bucket\",\r\n\t\tlogger: mockLogger,\r\n\t}\r\n\r\n\tmockClient.On(\"ListObjectsV2\", mock.Anything, mock.Anything, mock.Anything).Return(\r\n\t\t\u0026s3.ListObjectsV2Output{\r\n\t\t\tCommonPrefixes: []types.CommonPrefix{\r\n\t\t\t\t{Prefix: aws.String(\"test/\")},\r\n\t\t\t},\r\n\t\t},\r\n\t\tnil,\r\n\t)\r\n\r\n\tmockLogger.On(\"Debug\", mock.Anything, mock.Anything).Return()\r\n\r\n\tisDir, err := s3Storage.IsDirectory(\"test/\")\r\n\r\n\tassert.NoError(t, err)\r\n\tassert.True(t, isDir)\r\n\tmockClient.AssertExpectations(t)\r\n\tmockLogger.AssertExpectations(t)\r\n}\r\n\r\nfunc TestS3StorageStat(t *testing.T) {\r\n\tmockClient := new(MockS3Client)\r\n\tmockLogger := new(MockLogger)\r\n\ts3Storage := \u0026S3Storage{\r\n\t\tclient: mockClient,\r\n\t\tbucket: \"test-bucket\",\r\n\t\tlogger: mockLogger,\r\n\t}\r\n\r\n\tlastModified := time.Now()\r\n\tmockClient.On(\"HeadObject\", mock.Anything, mock.Anything, mock.Anything).Return(\r\n\t\t\u0026s3.HeadObjectOutput{\r\n\t\t\tContentLength: aws.Int64(100),\r\n\t\t\tLastModified:  \u0026lastModified,\r\n\t\t},\r\n\t\tnil,\r\n\t)\r\n\r\n\tmockLogger.On(\"Debug\", mock.Anything, mock.Anything).Return()\r\n\r\n\tinfo, err := s3Storage.Stat(\"test.txt\")\r\n\r\n\tassert.NoError(t, err)\r\n\tassert.Equal(t, \"test.txt\", info.Name())\r\n\tassert.Equal(t, int64(100), info.Size())\r\n\tassert.Equal(t, lastModified, info.ModTime())\r\n\tmockClient.AssertExpectations(t)\r\n\tmockLogger.AssertExpectations(t)\r\n}\r\n",
    "size": 6743,
    "modTime": "2024-11-04T14:44:37.9738004+01:00",
    "path": "internal\\storage\\s3_test.go"
  },
  {
    "name": "storage.go",
    "content": "// internal/storage/storage.go\r\n\r\npackage storage\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"os\"\r\n\t\"io\"\r\n\t\"time\"\r\n)\r\n\r\n\r\n// FileInfo définit l'interface pour les informations sur les fichiers\r\ntype FileInfo interface {\r\n\tName() string       // Nom du fichier\r\n\tSize() int64        // Taille du fichier en octets\r\n\tMode() os.FileMode  // Mode du fichier (permissions, etc.)\r\n\tModTime() time.Time // Heure de la dernière modification\r\n\tIsDir() bool        // Indique si c'est un répertoire\r\n\tSys() interface{}   // Informations système sous-jacentes\r\n}\r\n\r\ntype Logger interface {\r\n    Debug(format string, args ...interface{})\r\n    Info(format string, args ...interface{})\r\n    Warning(format string, args ...interface{})\r\n    Error(format string, args ...interface{})\r\n}\r\n\r\n// Storage définit l'interface pour les opérations de stockage\r\ntype Storage interface {\r\n\t// Read lit le contenu d'un fichier\r\n\tRead(path string) ([]byte, error)\r\n\r\n\t// Write écrit des données dans un fichier\r\n\tWrite(path string, data []byte) error\r\n\r\n\t// List retourne une liste de chemins de fichiers dans le répertoire spécifié\r\n\tList(prefix string) ([]string, error)\r\n\r\n\t// Delete supprime un fichier\r\n\tDelete(path string) error\r\n\r\n\t// Exists vérifie si un fichier existe\r\n\tExists(path string) (bool, error)\r\n\r\n\t// IsDirectory vérifie si le chemin est un répertoire\r\n\tIsDirectory(path string) (bool, error)\r\n\r\n\t// Stat retourne les informations sur un fichier\r\n    Stat(path string) (FileInfo, error)\r\n\r\n\t// GetReader retourne un io.ReadCloser pour lire le contenu du fichier spécifié par le chemin.\r\n    // Il est de la responsabilité de l'appelant de fermer le reader une fois terminé.\r\n\tGetReader(path string) (io.ReadCloser, error)\r\n\r\n}\r\n\r\n// Constantes pour les types de stockage\r\nconst (\r\n\tLocalStorageType = \"local\"\r\n\tS3StorageType    = \"s3\"\r\n)\r\n\r\nvar log = logger.GetLogger()\r\n\r\n\r\n// LogStorageOperation est une fonction utilitaire pour logger les opérations de stockage\r\nfunc LogStorageOperation(operation, path string) {\r\n\tlog.Debug(\"Storage operation: %s, Path: %s\", operation, path)\r\n}\r\n\r\n// CheckError est une fonction utilitaire pour vérifier et logger les erreurs\r\nfunc CheckError(err error, operation, path string) {\r\n    if err != nil {\r\n        log.Error(\"Storage error: Operation: %s, Path: %s, Error: %v\", operation, path, err)\r\n    }\r\n}\r\n",
    "size": 2378,
    "modTime": "2024-11-05T09:23:06.5161489+01:00",
    "path": "internal\\storage\\storage.go"
  },
  {
    "name": "tokenizer.go",
    "content": "package tokenizer\r\n\r\nimport (\r\n\t\"github.com/pkoukk/tiktoken-go\"\r\n)\r\n\r\n// CountTokens returns the number of tokens in the given text\r\nfunc CountTokens(text string) (int, error) {\r\n\tencoding, err := tiktoken.GetEncoding(\"cl100k_base\")\r\n\tif err != nil {\r\n\t\treturn 0, err\r\n\t}\r\n\ttokens := encoding.Encode(text, nil, nil)\r\n\treturn len(tokens), nil\r\n}\r\n",
    "size": 346,
    "modTime": "2024-10-20T15:16:21.4160242+02:00",
    "path": "internal\\tokenizer\\tokenizer.go"
  }
]