[
  {
    "name": "main.go",
    "content": "// cmd/ontology/main.go\n\npackage main\n\nimport \"github.com/chrlesur/Ontology/internal/ontology\"\n\n// main is the entry point of the Ontology application.\n// It calls the Run function from the ontology package.\nfunc main() {\n\tontology.Execute()\n}\n",
    "size": 244,
    "modTime": "2024-10-21T20:24:11.756592+02:00",
    "path": "cmd\\ontology\\main.go"
  },
  {
    "name": "config.yaml",
    "content": "base_uri: \"http://www.wikidata.org/entity/\"\r\nopenai_api_url: \"https://api.openai.com/v1/chat/completions\"\r\nclaude_api_url: \"https://api.anthropic.com/v1/messages\"\r\nollama_api_url: \"http://localhost:11434/api/generate\"\r\nlog_directory: \"logs\"\r\nlog_level: \"info\"\r\nmax_tokens: 2000\r\ncontext_size: 4000\r\ndefault_llm: \"claude\"\r\ndefault_model: \"claude-3-5-sonnet-20240620\"\r\ninclude_positions: true\r\ncontext_output: false\r\ncontext_words: 30",
    "size": 432,
    "modTime": "2024-10-24T00:15:45.4132939+02:00",
    "path": "config.yaml"
  },
  {
    "name": "config.go",
    "content": "package config\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"log\"\r\n\t\"os\"\r\n\t\"strconv\"\r\n\t\"sync\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"gopkg.in/yaml.v2\"\r\n)\r\n\r\nvar (\r\n\tonce     sync.Once\r\n\tinstance *Config\r\n)\r\n\r\n// Config structure definition\r\ntype Config struct {\r\n\tBaseURI          string `yaml:\"base_uri\"`\r\n\tOpenAIAPIURL     string `yaml:\"openai_api_url\"`\r\n\tClaudeAPIURL     string `yaml:\"claude_api_url\"`\r\n\tOllamaAPIURL     string `yaml:\"ollama_api_url\"`\r\n\tOpenAIAPIKey     string `yaml:\"openai_api_key\"`\r\n\tClaudeAPIKey     string `yaml:\"claude_api_key\"`\r\n\tLogDirectory     string `yaml:\"log_directory\"`\r\n\tLogLevel         string `yaml:\"log_level\"`\r\n\tMaxTokens        int    `yaml:\"max_tokens\"`\r\n\tContextSize      int    `yaml:\"context_size\"`\r\n\tDefaultLLM       string `yaml:\"default_llm\"`\r\n\tDefaultModel     string `yaml:\"default_model\"`\r\n\tOntologyName     string `yaml:\"ontology_name\"`\r\n\tInput            string `yaml:\"input\"`\r\n\tIncludePositions bool   `yaml:\"include_positions\"`\r\n\tContextOutput    bool   `yaml:\"context_output\"`\r\n\tContextWords     int    `yaml:\"context_words\"`\r\n\tAIYOUAPIURL      string `yaml:\"aiyou_api_url\"`\r\n\tAIYOUAssistantID string `yaml:\"aiyou_assistant_id\"`\r\n\tAIYOUEmail       string `yaml:\"aiyou_email\"`\r\n\tAIYOUPassword    string `yaml:\"aiyou_password\"`\r\n}\r\n\r\n// GetConfig returns the singleton instance of Config\r\nfunc GetConfig() *Config {\r\n\tonce.Do(func() {\r\n\t\tinstance = \u0026Config{\r\n\t\t\tOpenAIAPIURL:     \"https://api.openai.com/v1/chat/completions\",\r\n\t\t\tClaudeAPIURL:     \"https://api.anthropic.com/v1/messages\",\r\n\t\t\tOllamaAPIURL:     \"http://localhost:11434/api/generate\",\r\n\t\t\tBaseURI:          \"http://www.wikidata.org/entity/\",\r\n\t\t\tLogDirectory:     \"logs\",\r\n\t\t\tLogLevel:         \"info\",\r\n\t\t\tMaxTokens:        1000,\r\n\t\t\tContextSize:      4000,\r\n\t\t\tDefaultLLM:       \"claude\",\r\n\t\t\tDefaultModel:     \"claude-3-5-sonnet-20240620\",\r\n\t\t\tIncludePositions: true,\r\n\t\t\tContextOutput:    false,                           // Par défaut, la sortie de contexte est désactivée\r\n\t\t\tContextWords:     30,                              // Par défaut, 30 mots de contexte\r\n\t\t\tAIYOUAssistantID: \"asst_q2YbeHKeSxBzNr43KhIESkqj\", // Secnumcloud\r\n\t\t\tAIYOUAPIURL:      \"https://ai.dragonflygroup.fr/api\",\r\n\t\t}\r\n\t\tinstance.loadConfigFile()\r\n\t\tinstance.loadEnvVariables()\r\n\t})\r\n\treturn instance\r\n}\r\n\r\n// loadConfigFile loads the configuration from a YAML file\r\nfunc (c *Config) loadConfigFile() {\r\n\tconfigPath := os.Getenv(\"ONTOLOGY_CONFIG_PATH\")\r\n\tif configPath == \"\" {\r\n\t\tconfigPath = \"config.yaml\"\r\n\t}\r\n\r\n\tdata, err := ioutil.ReadFile(configPath)\r\n\tif err != nil {\r\n\t\tlog.Printf(i18n.GetMessage(\"ErrReadConfigFile\"), err)\r\n\t\treturn\r\n\t}\r\n\r\n\terr = yaml.Unmarshal(data, c)\r\n\tif err != nil {\r\n\t\tlog.Printf(i18n.GetMessage(\"ErrParseConfigFile\"), err)\r\n\t}\r\n\tif contextOutput := os.Getenv(\"ONTOLOGY_CONTEXT_OUTPUT\"); contextOutput == \"true\" {\r\n\t\tc.ContextOutput = true\r\n\t}\r\n\tif contextWords := os.Getenv(\"ONTOLOGY_CONTEXT_WORDS\"); contextWords != \"\" {\r\n\t\tif words, err := strconv.Atoi(contextWords); err == nil {\r\n\t\t\tc.ContextWords = words\r\n\t\t}\r\n\t}\r\n}\r\n\r\n// loadEnvVariables loads configuration from environment variables\r\nfunc (c *Config) loadEnvVariables() {\r\n\tif apiKey := os.Getenv(\"OPENAI_API_KEY\"); apiKey != \"\" {\r\n\t\tc.OpenAIAPIKey = apiKey\r\n\t}\r\n\tif apiKey := os.Getenv(\"CLAUDE_API_KEY\"); apiKey != \"\" {\r\n\t\tc.ClaudeAPIKey = apiKey\r\n\t}\r\n\t// Add more environment variables as needed\r\n}\r\n\r\n// ValidateConfig checks if the configuration is valid\r\nfunc (c *Config) ValidateConfig() error {\r\n\tif c.OpenAIAPIKey == \"\" \u0026\u0026 c.ClaudeAPIKey == \"\" {\r\n\t\treturn fmt.Errorf(i18n.GetMessage(\"ErrNoAPIKeys\"))\r\n\t}\r\n\t// Add more validation checks as needed\r\n\treturn nil\r\n}\r\n\r\n// Reload reloads the configuration from file and environment variables\r\nfunc (c *Config) Reload() error {\r\n\tc.loadConfigFile()\r\n\tc.loadEnvVariables()\r\n\treturn c.ValidateConfig()\r\n}\r\n",
    "size": 3856,
    "modTime": "2024-11-04T09:35:54.5266863+01:00",
    "path": "internal\\config\\config.go"
  },
  {
    "name": "convert.go",
    "content": "package converter\r\n\r\nimport (\r\n\t\"bufio\"\r\n\t\"bytes\"\r\n\t\"fmt\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\n// Convert converts a document segment into QuickStatement format\r\nfunc (qsc *QuickStatementConverter) Convert(segment []byte, context string, ontology string) (string, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ConvertStarted\"))\r\n\tlog.Debug(\"Input segment:\\n%s\", string(segment))\r\n\r\n\tstatements, err := qsc.parseSegment(segment)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.GetMessage(\"FailedToParseSegment\"), err)\r\n\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"FailedToParseSegment\"), err)\r\n\t}\r\n\r\n\tlog.Debug(\"Parsed %d statements\", len(statements))\r\n\r\n\tvar tsvBuilder strings.Builder\r\n\tfor _, stmt := range statements {\r\n\t\t// Vérifier si l'objet contient des positions\r\n\t\tobjectParts := strings.Split(stmt.Object.(string), \"@\")\r\n\t\tobject := objectParts[0]\r\n\t\tpositions := \"\"\r\n\t\tif len(objectParts) \u003e 1 {\r\n\t\t\tpositions = \"@\" + objectParts[1]\r\n\t\t}\r\n\r\n\t\t// Écrire la ligne TSV avec les positions si elles existent\r\n\t\ttsvLine := fmt.Sprintf(\"%s\\t%s\\t%s%s\\n\", stmt.Subject.ID, stmt.Property.ID, object, positions)\r\n\t\ttsvBuilder.WriteString(tsvLine)\r\n\t}\r\n\r\n\tresult := tsvBuilder.String()\r\n\tlog.Debug(\"Generated TSV output:\\n%s\", result)\r\n\treturn result, nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) cleanAndNormalizeInput(input string) string {\r\n\tlines := strings.Split(input, \"\\n\")\r\n\tvar cleanedLines []string\r\n\tfor _, line := range lines {\r\n\t\ttrimmedLine := strings.TrimSpace(line)\r\n\t\tif trimmedLine != \"\" {\r\n\t\t\tcleanedLines = append(cleanedLines, trimmedLine)\r\n\t\t}\r\n\t}\r\n\treturn strings.Join(cleanedLines, \"\\n\")\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) parseSegment(segment []byte) ([]Statement, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParsingSegment\"))\r\n\tvar statements []Statement\r\n\tscanner := bufio.NewScanner(bytes.NewReader(segment))\r\n\tfor scanner.Scan() {\r\n\t\tline := scanner.Text()\r\n\t\t// Remplacer les doubles backslashes par un caractère temporaire\r\n\t\tline = strings.ReplaceAll(line, \"\\\\\\\\\", \"\\uFFFD\")\r\n\t\t// Remplacer les \\t par des tabulations réelles\r\n\t\tline = strings.ReplaceAll(line, \"\\\\t\", \"\\t\")\r\n\t\t// Restaurer les doubles backslashes\r\n\t\tline = strings.ReplaceAll(line, \"\\uFFFD\", \"\\\\\")\r\n\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tif len(parts) \u003c 3 {\r\n\t\t\tlog.Debug(\"Skipping invalid line: %s\", line)\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tstatement := Statement{\r\n\t\t\tSubject:  Entity{ID: strings.TrimSpace(parts[0])},\r\n\t\t\tProperty: Property{ID: strings.TrimSpace(parts[1])},\r\n\t\t\tObject:   strings.TrimSpace(strings.Join(parts[2:], \"\\t\")),\r\n\t\t}\r\n\t\tstatements = append(statements, statement)\r\n\t}\r\n\tif err := scanner.Err(); err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorScanningSegment\"), err)\r\n\t}\r\n\tif len(statements) == 0 {\r\n\t\treturn nil, fmt.Errorf(i18n.GetMessage(\"NoValidStatementsFound\"))\r\n\t}\r\n\treturn statements, nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) applyContextAndOntology(statements []Statement, context string, ontology string) ([]Statement, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ApplyingContextAndOntology\"))\r\n\t// This is a placeholder implementation. In a real-world scenario, this function would\r\n\t// use the context and ontology to enrich the statements.\r\n\tfor i := range statements {\r\n\t\tstatements[i].Subject.Label = fmt.Sprintf(\"%s (from context)\", statements[i].Subject.ID)\r\n\t}\r\n\treturn statements, nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) toQuickStatementTSV(statements []Statement) (string, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ConvertingToQuickStatementTSV\"))\r\n\tvar buffer bytes.Buffer\r\n\tfor _, stmt := range statements {\r\n\t\tline := fmt.Sprintf(\"%s\\t%s\\t%v\\n\", stmt.Subject.ID, stmt.Property.ID, stmt.Object)\r\n\t\t_, err := buffer.WriteString(line)\r\n\t\tif err != nil {\r\n\t\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorWritingQuickStatement\"), err)\r\n\t\t}\r\n\t}\r\n\treturn buffer.String(), nil\r\n}\r\n",
    "size": 3865,
    "modTime": "2024-10-22T10:32:48.7791638+02:00",
    "path": "internal\\converter\\convert.go"
  },
  {
    "name": "logger.go",
    "content": "package converter\r\n\r\nimport (\r\n    \"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()",
    "size": 116,
    "modTime": "2024-10-20T14:40:18.8000271+02:00",
    "path": "internal\\converter\\logger.go"
  },
  {
    "name": "parse.go",
    "content": "package converter\r\n\r\nimport (\r\n\t\"encoding/xml\"\r\n\t\"fmt\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/knakk/rdf\"\r\n)\r\n\r\n// ParseOntology parses an ontology string and returns a structured representation\r\nfunc ParseOntology(ontology string) (map[string]interface{}, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParseOntologyStarted\"))\r\n\r\n\tformat := detectOntologyFormat(ontology)\r\n\tvar result map[string]interface{}\r\n\tvar err error\r\n\r\n\tswitch format {\r\n\tcase \"QuickStatement\":\r\n\t\tresult, err = parseQuickStatementOntology(ontology)\r\n\tcase \"RDF\":\r\n\t\tresult, err = parseRDFOntology(ontology)\r\n\tcase \"OWL\":\r\n\t\tresult, err = parseOWLOntology(ontology)\r\n\tdefault:\r\n\t\treturn nil, fmt.Errorf(i18n.GetMessage(\"UnknownOntologyFormat\"))\r\n\t}\r\n\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"FailedToParseOntology\"), err)\r\n\t}\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ParseOntologyFinished\"))\r\n\treturn result, nil\r\n}\r\n\r\nfunc detectOntologyFormat(ontology string) string {\r\n\tif strings.Contains(ontology, \"Q\") \u0026\u0026 strings.Contains(ontology, \"P\") \u0026\u0026 strings.Contains(ontology, \"\\t\") {\r\n\t\treturn \"QuickStatement\"\r\n\t}\r\n\tif strings.Contains(ontology, \"\u003crdf:RDF\") {\r\n\t\treturn \"RDF\"\r\n\t}\r\n\tif strings.Contains(ontology, \"\u003cOntology\") {\r\n\t\treturn \"OWL\"\r\n\t}\r\n\treturn \"Unknown\"\r\n}\r\n\r\nfunc parseQuickStatementOntology(ontology string) (map[string]interface{}, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParseQuickStatementOntologyStarted\"))\r\n\r\n\tresult := make(map[string]interface{})\r\n\tentities := make(map[string]map[string]interface{})\r\n\r\n\tlines := strings.Split(ontology, \"\\n\")\r\n\tfor _, line := range lines {\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tif len(parts) != 3 {\r\n\t\t\treturn nil, fmt.Errorf(i18n.GetMessage(\"InvalidQuickStatementLine\"))\r\n\t\t}\r\n\r\n\t\tsubject, predicate, object := parts[0], parts[1], parts[2]\r\n\t\tif _, exists := entities[subject]; !exists {\r\n\t\t\tentities[subject] = make(map[string]interface{})\r\n\t\t}\r\n\t\tentities[subject][predicate] = object\r\n\t}\r\n\r\n\tresult[\"entities\"] = entities\r\n\tlog.Debug(i18n.GetMessage(\"ParseQuickStatementOntologyFinished\"))\r\n\treturn result, nil\r\n}\r\n\r\nfunc parseRDFOntology(ontology string) (map[string]interface{}, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParseRDFOntologyStarted\"))\r\n\r\n\tresult := make(map[string]interface{})\r\n\tentities := make(map[string]map[string]interface{})\r\n\r\n\tdec := rdf.NewTripleDecoder(strings.NewReader(ontology), rdf.RDFXML)\r\n\tfor {\r\n\t\ttriple, err := dec.Decode()\r\n\t\tif err != nil {\r\n\t\t\tif err.Error() == \"EOF\" {\r\n\t\t\t\tbreak\r\n\t\t\t}\r\n\t\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorDecodingRDF\"), err)\r\n\t\t}\r\n\r\n\t\tsubject := triple.Subj.String()\r\n\t\tpredicate := triple.Pred.String()\r\n\t\tobject := triple.Obj.String()\r\n\r\n\t\tif _, exists := entities[subject]; !exists {\r\n\t\t\tentities[subject] = make(map[string]interface{})\r\n\t\t}\r\n\t\tentities[subject][predicate] = object\r\n\t}\r\n\r\n\tresult[\"entities\"] = entities\r\n\tlog.Debug(i18n.GetMessage(\"ParseRDFOntologyFinished\"))\r\n\treturn result, nil\r\n}\r\n\r\nfunc parseOWLOntology(ontology string) (map[string]interface{}, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParseOWLOntologyStarted\"))\r\n\r\n\tresult := make(map[string]interface{})\r\n\r\n\t// Simplified OWL parsing\r\n\tvar owlData struct {\r\n\t\tXMLName xml.Name `xml:\"Ontology\"`\r\n\t\tClasses []struct {\r\n\t\t\tIRI   string `xml:\"IRI,attr\"`\r\n\t\t\tLabel string `xml:\"label,attr\"`\r\n\t\t} `xml:\"Declaration\u003eClass\"`\r\n\t}\r\n\r\n\terr := xml.Unmarshal([]byte(ontology), \u0026owlData)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorParsingOWL\"), err)\r\n\t}\r\n\r\n\tclasses := make(map[string]interface{})\r\n\tfor _, class := range owlData.Classes {\r\n\t\tclasses[class.IRI] = map[string]interface{}{\r\n\t\t\t\"label\": class.Label,\r\n\t\t}\r\n\t}\r\n\tresult[\"classes\"] = classes\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ParseOWLOntologyFinished\"))\r\n\treturn result, nil\r\n}\r\n",
    "size": 3787,
    "modTime": "2024-10-20T14:19:06.7515729+02:00",
    "path": "internal\\converter\\parse.go"
  },
  {
    "name": "quickstatement.go",
    "content": "// Package converter provides functionality for converting document segments\r\n// into QuickStatement format and other ontology representations.\r\npackage converter\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\n// Converter defines the interface for QuickStatement conversion\r\ntype Converter interface {\r\n\tConvert(segment []byte, context string, ontology string) (string, error)\r\n}\r\n\r\n// QuickStatementConverter implements the Converter interface\r\ntype QuickStatementConverter struct {\r\n\tlogger           *logger.Logger\r\n\tincludePositions bool\r\n}\r\n\r\n// NewQuickStatementConverter creates a new QuickStatementConverter\r\nfunc NewQuickStatementConverter(log *logger.Logger, includePositions bool) *QuickStatementConverter {\r\n\treturn \u0026QuickStatementConverter{\r\n\t\tlogger:           log,\r\n\t\tincludePositions: includePositions,\r\n\t}\r\n}\r\n\r\n// Entity represents a Wikibase entity\r\ntype Entity struct {\r\n\tID    string\r\n\tLabel string\r\n}\r\n\r\n// Property represents a Wikibase property\r\ntype Property struct {\r\n\tID       string\r\n\tDataType string\r\n}\r\n\r\n// Statement represents a complete QuickStatement\r\ntype Statement struct {\r\n\tSubject  Entity\r\n\tProperty Property\r\n\tObject   interface{}\r\n}\r\n",
    "size": 1196,
    "modTime": "2024-11-04T09:26:58.2842666+01:00",
    "path": "internal\\converter\\quickstatement.go"
  },
  {
    "name": "utils.go",
    "content": "package converter\r\n\r\nimport (\r\n\t\"crypto/rand\"\r\n\t\"fmt\"\r\n\t\"net/url\"\r\n\t\"strings\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\n// EscapeString escapes special characters in a string for safe use in output formats\r\nfunc EscapeString(s string) string {\r\n\treturn strings.NewReplacer(\r\n\t\t`\"`, `\\\"`,\r\n\t\t`\\`, `\\\\`,\r\n\t\t`/`, `\\/`,\r\n\t\t\"\\b\", `\\b`,\r\n\t\t\"\\f\", `\\f`,\r\n\t\t\"\\n\", `\\n`,\r\n\t\t\"\\r\", `\\r`,\r\n\t\t\"\\t\", `\\t`,\r\n\t).Replace(s)\r\n}\r\n\r\n// IsValidURI checks if a string is a valid URI\r\nfunc IsValidURI(uri string) bool {\r\n\t_, err := url.ParseRequestURI(uri)\r\n\tif err != nil {\r\n\t\tlog.Debug(i18n.GetMessage(\"InvalidURI\")) // Cette ligne est correcte car log.Debug n'attend qu'un seul argument\r\n\t\treturn false\r\n\t}\r\n\treturn true\r\n}\r\n\r\n// FormatDate formats a date string to a standard format (ISO 8601)\r\nfunc FormatDate(date string) (string, error) {\r\n\tparsedDate, err := time.Parse(time.RFC3339, date)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidDateFormat\"), err) // Ajout de l'erreur comme second argument\r\n\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorParsingDate\"), err)\r\n\t}\r\n\treturn parsedDate.Format(time.RFC3339), nil\r\n}\r\n\r\n// GenerateUniqueID generates a unique identifier for entities without one\r\nfunc GenerateUniqueID() string {\r\n\tb := make([]byte, 16)\r\n\t_, err := rand.Read(b)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.GetMessage(\"ErrorGeneratingUniqueID\"), err)\r\n\t\treturn \"\"\r\n\t}\r\n\treturn fmt.Sprintf(\"%x\", b)\r\n}\r\n\r\n// SplitIntoChunks divides a large dataset into smaller chunks for batch processing\r\nfunc SplitIntoChunks(data []byte, chunkSize int) [][]byte {\r\n\tvar chunks [][]byte\r\n\tfor i := 0; i \u003c len(data); i += chunkSize {\r\n\t\tend := i + chunkSize\r\n\t\tif end \u003e len(data) {\r\n\t\t\tend = len(data)\r\n\t\t}\r\n\t\tchunks = append(chunks, data[i:end])\r\n\t}\r\n\treturn chunks\r\n}\r\n\r\n// MergeMaps merges multiple maps into a single map\r\nfunc MergeMaps(maps ...map[string]interface{}) map[string]interface{} {\r\n\tresult := make(map[string]interface{})\r\n\tfor _, m := range maps {\r\n\t\tfor k, v := range m {\r\n\t\t\tresult[k] = v\r\n\t\t}\r\n\t}\r\n\treturn result\r\n}\r\n\r\n// TruncateString truncates a string to a specified length, adding an ellipsis if truncated\r\nfunc TruncateString(s string, maxLength int) string {\r\n\tif len(s) \u003c= maxLength {\r\n\t\treturn s\r\n\t}\r\n\treturn s[:maxLength-3] + \"...\"\r\n}\r\n\r\n// NormalizeWhitespace removes extra whitespace from a string\r\nfunc NormalizeWhitespace(s string) string {\r\n\treturn strings.Join(strings.Fields(s), \" \")\r\n}\r\n\r\n// IsNumeric checks if a string contains only numeric characters\r\nfunc IsNumeric(s string) bool {\r\n\tfor _, r := range s {\r\n\t\tif r \u003c '0' || r \u003e '9' {\r\n\t\t\treturn false\r\n\t\t}\r\n\t}\r\n\treturn true\r\n}\r\n",
    "size": 2639,
    "modTime": "2024-10-20T14:44:56.9267011+02:00",
    "path": "internal\\converter\\utils.go"
  },
  {
    "name": "validate.go",
    "content": "package converter\r\n\r\nimport (\r\n\t\"bufio\"\r\n\t\"fmt\"\r\n\t\"regexp\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\n// ValidateQuickStatement validates a QuickStatement string\r\nfunc ValidateQuickStatement(statement string) bool {\r\n\tlog.Debug(i18n.GetMessage(\"ValidateQuickStatementStarted\"))\r\n\r\n\terr := validateQuickStatementSyntax(statement)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidQuickStatementSyntax\"), err)\r\n\t\treturn false\r\n\t}\r\n\r\n\terr = checkEntityReferences(statement)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidEntityReferences\"), err)\r\n\t\treturn false\r\n\t}\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ValidateQuickStatementFinished\"))\r\n\treturn true\r\n}\r\n\r\n// ValidateRDF validates an RDF string\r\nfunc ValidateRDF(rdf string) bool {\r\n\tlog.Debug(i18n.GetMessage(\"ValidateRDFStarted\"))\r\n\r\n\terr := validateRDFSyntax(rdf)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidRDFSyntax\"), err)\r\n\t\treturn false\r\n\t}\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ValidateRDFFinished\"))\r\n\treturn true\r\n}\r\n\r\n// ValidateOWL validates an OWL string\r\nfunc ValidateOWL(owl string) bool {\r\n\tlog.Debug(i18n.GetMessage(\"ValidateOWLStarted\"))\r\n\r\n\terr := validateOWLSyntax(owl)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidOWLSyntax\"), err)\r\n\t\treturn false\r\n\t}\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ValidateOWLFinished\"))\r\n\treturn true\r\n}\r\n\r\nfunc validateQuickStatementSyntax(statement string) error {\r\n\tscanner := bufio.NewScanner(strings.NewReader(statement))\r\n\tlineNum := 0\r\n\tfor scanner.Scan() {\r\n\t\tlineNum++\r\n\t\tline := scanner.Text()\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tif len(parts) != 3 {\r\n\t\t\treturn fmt.Errorf(i18n.GetMessage(\"InvalidQuickStatementLine\"), lineNum)\r\n\t\t}\r\n\t\tif !isValidEntity(parts[0]) || !isValidProperty(parts[1]) {\r\n\t\t\treturn fmt.Errorf(i18n.GetMessage(\"InvalidEntityOrProperty\"), lineNum)\r\n\t\t}\r\n\t}\r\n\treturn scanner.Err()\r\n}\r\n\r\nfunc validateRDFSyntax(rdf string) error {\r\n\tif !strings.Contains(rdf, \"\u003crdf:RDF\") || !strings.Contains(rdf, \"\u003c/rdf:RDF\u003e\") {\r\n\t\treturn fmt.Errorf(i18n.GetMessage(\"MissingRDFTags\"))\r\n\t}\r\n\tif !strings.Contains(rdf, \"xmlns:rdf=\\\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\\\"\") {\r\n\t\treturn fmt.Errorf(i18n.GetMessage(\"MissingRDFNamespace\"))\r\n\t}\r\n\treturn nil\r\n}\r\n\r\nfunc validateOWLSyntax(owl string) error {\r\n\tif !strings.Contains(owl, \"\u003cowl:Ontology\") || !strings.Contains(owl, \"\u003c/owl:Ontology\u003e\") {\r\n\t\treturn fmt.Errorf(i18n.GetMessage(\"MissingOWLTags\"))\r\n\t}\r\n\tif !strings.Contains(owl, \"xmlns:owl=\\\"http://www.w3.org/2002/07/owl#\\\"\") {\r\n\t\treturn fmt.Errorf(i18n.GetMessage(\"MissingOWLNamespace\"))\r\n\t}\r\n\treturn nil\r\n}\r\n\r\nfunc checkEntityReferences(statement string) error {\r\n\tscanner := bufio.NewScanner(strings.NewReader(statement))\r\n\tlineNum := 0\r\n\tfor scanner.Scan() {\r\n\t\tlineNum++\r\n\t\tline := scanner.Text()\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tif !isValidEntity(parts[0]) {\r\n\t\t\treturn fmt.Errorf(i18n.GetMessage(\"InvalidEntityReference\"), parts[0], lineNum)\r\n\t\t}\r\n\t}\r\n\treturn scanner.Err()\r\n}\r\n\r\nfunc isValidEntity(entity string) bool {\r\n\treturn regexp.MustCompile(`^Q\\d+$`).MatchString(entity)\r\n}\r\n\r\nfunc isValidProperty(property string) bool {\r\n\treturn regexp.MustCompile(`^P\\d+$`).MatchString(property)\r\n}\r\n\r\nfunc isValidURI(uri string) bool {\r\n\t// This is a simplified URI validation. In a real-world scenario, you might want to use a more comprehensive validation.\r\n\treturn regexp.MustCompile(`^(http|https)://`).MatchString(uri)\r\n}\r\n",
    "size": 3409,
    "modTime": "2024-10-20T14:43:44.4003488+02:00",
    "path": "internal\\converter\\validate.go"
  },
  {
    "name": "messages.go",
    "content": "package i18n\r\n\r\nimport (\r\n\t\"reflect\"\r\n\t\"strings\"\r\n)\r\n\r\n// Messages contient tous les messages de l'application\r\nvar Messages = struct {\r\n\tRootCmdShortDesc                  string\r\n\tRootCmdLongDesc                   string\r\n\tEnrichCmdShortDesc                string\r\n\tEnrichCmdLongDesc                 string\r\n\tConfigFlagUsage                   string\r\n\tDebugFlagUsage                    string\r\n\tSilentFlagUsage                   string\r\n\tInputFlagUsage                    string\r\n\tOutputFlagUsage                   string\r\n\tFormatFlagUsage                   string\r\n\tLLMFlagUsage                      string\r\n\tLLMModelFlagUsage                 string\r\n\tPassesFlagUsage                   string\r\n\r\n\tRecursiveFlagUsage                string\r\n\tInitializingApplication           string\r\n\tStartingEnrichProcess             string\r\n\tEnrichProcessCompleted            string\r\n\tExecutingPipeline                 string\r\n\tErrorExecutingRootCmd             string\r\n\tErrorExecutingPipeline            string\r\n\tErrorCreatingPipeline             string\r\n\tErrUnsupportedModel               string\r\n\tErrAPIKeyMissing                  string\r\n\tErrTranslationFailed              string\r\n\tErrInvalidLLMType                 string\r\n\tErrContextTooLong                 string\r\n\tTranslationStarted                string\r\n\tTranslationRetry                  string\r\n\tTranslationCompleted              string\r\n\tErrCreateLogDir                   string\r\n\tErrOpenLogFile                    string\r\n\tParseStarted                      string\r\n\tParseFailed                       string\r\n\tParseCompleted                    string\r\n\tMetadataExtractionFailed          string\r\n\tPageParseFailed                   string\r\n\tTextExtractionFailed              string\r\n\tErrInvalidContent                 string\r\n\tErrTokenization                   string\r\n\tErrReadingContent                 string\r\n\tErrTokenizerInitialization        string\r\n\tErrTokenCounting                  string\r\n\tLogSegmentationStarted            string\r\n\tLogSegmentationCompleted          string\r\n\tLogContextGeneration              string\r\n\tLogMergingSegments                string\r\n\tErrReadConfigFile                 string\r\n\tErrParseConfigFile                string\r\n\tErrNoAPIKeys                      string\r\n\tStartingQuickStatementConversion  string\r\n\tQuickStatementConversionCompleted string\r\n\tStartingRDFConversion             string\r\n\tRDFConversionCompleted            string\r\n\tStartingOWLConversion             string\r\n\tOWLConversionCompleted            string\r\n\tExistingOntologyFlagUsage         string\r\n\tErrAccessInput                    string\r\n\tErrProcessingPass                 string\r\n\tErrNoInputSpecified               string\r\n\tTranslationFailed                 string\r\n\tRateLimitExceeded                 string\r\n\tStartingPipeline                  string\r\n\tStartingPass                      string\r\n\tPipelineCompleted                 string\r\n\tSegmentProcessingError            string\r\n\tErrLoadExistingOntology           string\r\n\tErrSavingResult                   string\r\n\tErrSegmentContent                 string\r\n\tIncludePositionsFlagUsage         string\r\n\tContextOutputFlagUsage            string\r\n\tContextWordsFlagUsage             string\r\n}{\r\n\tRootCmdShortDesc: \"Ontology enrichment tool\",\r\n\tRootCmdLongDesc: `Ontology is a command-line tool for enriching ontologies from various document formats.\r\nIt supports multiple input formats and can utilize different language models for analysis.`,\r\n\tEnrichCmdShortDesc: \"Enrich an ontology from input documents\",\r\n\tEnrichCmdLongDesc: `The enrich command processes input documents to create or update an ontology.\r\nIt can handle various input formats and use different language models for analysis.`,\r\n\tConfigFlagUsage:                   \"config file (default is $HOME/.ontology.yaml)\",\r\n\tDebugFlagUsage:                    \"enable debug mode\",\r\n\tSilentFlagUsage:                   \"silent mode, only show errors\",\r\n\tInputFlagUsage:                    \"input file or directory\",\r\n\tOutputFlagUsage:                   \"output file for the enriched ontology\",\r\n\tFormatFlagUsage:                   \"input format (auto-detected if not specified)\",\r\n\tLLMFlagUsage:                      \"language model to use for analysis\",\r\n\tLLMModelFlagUsage:                 \"specific model for the chosen LLM\",\r\n\tPassesFlagUsage:                   \"number of passes for ontology enrichment\",\r\n\tRecursiveFlagUsage:                \"process input directory recursively\",\r\n\tInitializingApplication:           \"Initializing Ontology application\",\r\n\tStartingEnrichProcess:             \"Starting ontology enrichment process\",\r\n\tEnrichProcessCompleted:            \"Ontology enrichment process completed\",\r\n\tExecutingPipeline:                 \"Executing ontology enrichment pipeline\",\r\n\tErrorExecutingRootCmd:             \"Error executing root command\",\r\n\tErrorExecutingPipeline:            \"Error executing pipeline\",\r\n\tErrorCreatingPipeline:             \"Error creating pipeline\",\r\n\tErrUnsupportedModel:               \"unsupported model\",\r\n\tErrAPIKeyMissing:                  \"API key is missing\",\r\n\tErrTranslationFailed:              \"translation failed\",\r\n\tErrInvalidLLMType:                 \"invalid LLM type\",\r\n\tErrContextTooLong:                 \"context is too long\",\r\n\tTranslationStarted:                \"Translation started\",\r\n\tTranslationRetry:                  \"Translation retry\",\r\n\tTranslationCompleted:              \"Translation completed\",\r\n\tErrCreateLogDir:                   \"Failed to create log directory\",\r\n\tErrOpenLogFile:                    \"Failed to open log file\",\r\n\tParseStarted:                      \"Parsing started\",\r\n\tParseFailed:                       \"Parsing failed\",\r\n\tParseCompleted:                    \"Parsing completed\",\r\n\tMetadataExtractionFailed:          \"Metadata extraction failed\",\r\n\tPageParseFailed:                   \"Failed to parse page\",\r\n\tTextExtractionFailed:              \"Failed to extract text from page\",\r\n\tErrInvalidContent:                 \"invalid content\",\r\n\tErrTokenization:                   \"tokenization error\",\r\n\tErrReadingContent:                 \"error reading content\",\r\n\tErrTokenizerInitialization:        \"error initializing tokenizer\",\r\n\tErrTokenCounting:                  \"error counting tokens\",\r\n\tLogSegmentationStarted:            \"Segmentation started\",\r\n\tLogSegmentationCompleted:          \"Segmentation completed: %d segments\",\r\n\tLogContextGeneration:              \"Generating context\",\r\n\tLogMergingSegments:                \"Merging segments\",\r\n\tErrReadConfigFile:                 \"Failed to read config file: %v\",\r\n\tErrParseConfigFile:                \"Failed to parse config file: %v\",\r\n\tErrNoAPIKeys:                      \"No API keys provided for any LLM service\",\r\n\tStartingQuickStatementConversion:  \"Starting conversion to QuickStatement format\",\r\n\tQuickStatementConversionCompleted: \"QuickStatement conversion completed\",\r\n\tStartingRDFConversion:             \"Starting conversion to RDF format\",\r\n\tRDFConversionCompleted:            \"RDF conversion completed\",\r\n\tStartingOWLConversion:             \"Starting conversion to OWL format\",\r\n\tOWLConversionCompleted:            \"OWL conversion completed\",\r\n\tExistingOntologyFlagUsage:         \"path to an existing ontology file to enrich\",\r\n\tErrAccessInput:                    \"Failed to access input: %v\",\r\n\tErrProcessingPass:                 \"Error processing pass: %v\",\r\n\tErrNoInputSpecified:               \"No input specified. Please use --input flag to specify an input file or directory.\",\r\n\tTranslationFailed:                 \"Translation failed after maximum retries: %v\",\r\n\tRateLimitExceeded:                 \"Rate limit exceeded. Waiting %v before retrying.\",\r\n\tStartingPipeline:                  \"Starting pipeline execution\",\r\n\tStartingPass:                      \"Starting pass %d\",\r\n\tPipelineCompleted:                 \"Pipeline execution completed successfully\",\r\n\tSegmentProcessingError:            \"Error processing segment %d: %v\",\r\n\tErrLoadExistingOntology:           \"Failed to load existing ontology\",\r\n\tErrSavingResult:                   \"Error saving result\",\r\n\tErrSegmentContent:                 \"Failed to segment content\",\r\n\tIncludePositionsFlagUsage:         \"Active to not include position information in the ontology\",\r\n\tContextOutputFlagUsage:            \"Enable context output in JSON format\",\r\n\tContextWordsFlagUsage:             \"Number of context words before and after each position\",\r\n}\r\n\r\n// GetMessage retourne le message correspondant à la clé donnée\r\nfunc GetMessage(key string) string {\r\n\tv := reflect.ValueOf(Messages)\r\n\tf := v.FieldByName(key)\r\n\tif !f.IsValid() {\r\n\t\treturn key\r\n\t}\r\n\treturn strings.TrimSpace(f.String())\r\n}\r\n",
    "size": 8667,
    "modTime": "2024-11-04T09:27:51.1177263+01:00",
    "path": "internal\\i18n\\messages.go"
  },
  {
    "name": "aiyou.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"net/http\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n)\r\n\r\nconst AIYOUAPIURL = \"https://ai.dragonflygroup.fr/api\"\r\n\r\ntype APICaller interface {\r\n\tCall(endpoint, method string, data interface{}, response interface{}) error\r\n\tSetToken(token string)\r\n}\r\n\r\ntype AIYOUClient struct {\r\n\tapiCaller   APICaller\r\n\tAssistantID string\r\n\tTimeout     time.Duration\r\n\tconfig      *config.Config\r\n\tlogger      *logger.Logger\r\n}\r\n\r\ntype HTTPAPICaller struct {\r\n\tbaseURL    string\r\n\thttpClient *http.Client\r\n\ttoken      string\r\n}\r\n\r\ntype Run struct {\r\n\tID       string `json:\"id\"`\r\n\tStatus   string `json:\"status\"`\r\n\tResponse string `json:\"response\"`\r\n}\r\n\r\nfunc NewAIYOUClient(assistantID string, email string, password string) (*AIYOUClient, error) {\r\n\tcfg := config.GetConfig()\r\n\tlog := logger.GetLogger()\r\n\r\n\tif assistantID == \"\" {\r\n\t\treturn nil, fmt.Errorf(\"AI.YOU assistant ID is required\")\r\n\t}\r\n\r\n\tclient := \u0026AIYOUClient{\r\n\t\tapiCaller:   NewHTTPAPICaller(AIYOUAPIURL),\r\n\t\tAssistantID: assistantID,\r\n\t\tTimeout:     120 * time.Second,\r\n\t\tconfig:      cfg,\r\n\t\tlogger:      log,\r\n\t}\r\n\r\n\terr := client.Login(email, password)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to login to AI.YOU: %w\", err)\r\n\t}\r\n\r\n\treturn client, nil\r\n}\r\n\r\nfunc (c *AIYOUClient) Login(email, password string) error {\r\n\tc.logger.Info(\"Attempting to log in to AI.YOU\")\r\n\tloginData := map[string]string{\r\n\t\t\"email\":    email,\r\n\t\t\"password\": password,\r\n\t}\r\n\r\n\tvar loginResp struct {\r\n\t\tToken     string `json:\"token\"`\r\n\t\tExpiresAt string `json:\"expires_at\"`\r\n\t}\r\n\r\n\terr := c.apiCaller.Call(\"/login\", \"POST\", loginData, \u0026loginResp)\r\n\tif err != nil {\r\n\t\tc.logger.Error(fmt.Sprintf(\"Login failed: %v\", err))\r\n\t\treturn fmt.Errorf(\"login failed: %w\", err)\r\n\t}\r\n\r\n\tc.apiCaller.SetToken(loginResp.Token)\r\n\tc.logger.Info(\"Successfully logged in to AI.YOU\")\r\n\treturn nil\r\n}\r\n\r\nfunc (c *AIYOUClient) Translate(prompt string, context string) (string, error) {\r\n\tc.logger.Debug(\"Starting AI.YOU translation\")\r\n\r\n\tthreadID, err := c.CreateThread()\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"failed to create thread: %w\", err)\r\n\t}\r\n\r\n\tfullPrompt := fmt.Sprintf(\"%s\\n\\nContext: %s\", prompt, context)\r\n\tresponse, err := c.ChatInThread(threadID, fullPrompt)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"error during chat: %w\", err)\r\n\t}\r\n\r\n\tc.logger.Debug(\"AI.YOU translation completed\")\r\n\treturn response, nil\r\n}\r\n\r\nfunc (c *AIYOUClient) ProcessWithPrompt(promptTemplate *prompt.PromptTemplate, values map[string]string) (string, error) {\r\n\tc.logger.Debug(\"Processing with prompt using AI.YOU\")\r\n\r\n\tformattedPrompt := promptTemplate.Format(values)\r\n\treturn c.Translate(formattedPrompt, \"\")\r\n}\r\n\r\nfunc (c *AIYOUClient) CreateThread() (string, error) {\r\n\tc.logger.Debug(\"Creating new AI.YOU thread\")\r\n\r\n\tvar threadResp struct {\r\n\t\tID string `json:\"id\"`\r\n\t}\r\n\r\n\terr := c.apiCaller.Call(\"/v1/threads\", \"POST\", map[string]string{}, \u0026threadResp)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"error creating thread: %w\", err)\r\n\t}\r\n\r\n\tif threadResp.ID == \"\" {\r\n\t\treturn \"\", fmt.Errorf(\"thread ID is empty in response\")\r\n\t}\r\n\r\n\tc.logger.Debug(fmt.Sprintf(\"Thread created with ID: %s\", threadResp.ID))\r\n\treturn threadResp.ID, nil\r\n}\r\n\r\nfunc (c *AIYOUClient) ChatInThread(threadID, input string) (string, error) {\r\n\tc.logger.Debug(fmt.Sprintf(\"Chatting in thread %s\", threadID))\r\n\r\n\terr := c.addMessage(threadID, input)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"failed to add message: %w\", err)\r\n\t}\r\n\r\n\trunID, err := c.createRun(threadID)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"failed to create run: %w\", err)\r\n\t}\r\n\r\n\tcompletedRun, err := c.waitForCompletion(threadID, runID)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"run failed: %w\", err)\r\n\t}\r\n\r\n\treturn completedRun.Response, nil\r\n}\r\n\r\nfunc (c *AIYOUClient) addMessage(threadID, content string) error {\r\n\tc.logger.Debug(fmt.Sprintf(\"Adding message to thread %s\", threadID))\r\n\r\n\tmessageData := map[string]string{\r\n\t\t\"role\":    \"user\",\r\n\t\t\"content\": content,\r\n\t}\r\n\r\n\tvar response interface{}\r\n\terr := c.apiCaller.Call(fmt.Sprintf(\"/v1/threads/%s/messages\", threadID), \"POST\", messageData, \u0026response)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"error adding message: %w\", err)\r\n\t}\r\n\r\n\tc.logger.Debug(\"Message added successfully\")\r\n\treturn nil\r\n}\r\n\r\nfunc (c *AIYOUClient) createRun(threadID string) (string, error) {\r\n\tc.logger.Debug(fmt.Sprintf(\"Creating run for thread %s\", threadID))\r\n\r\n\trunData := map[string]string{\r\n\t\t\"assistantId\": c.AssistantID,\r\n\t}\r\n\r\n\tvar runResp struct {\r\n\t\tID string `json:\"id\"`\r\n\t}\r\n\terr := c.apiCaller.Call(fmt.Sprintf(\"/v1/threads/%s/runs\", threadID), \"POST\", runData, \u0026runResp)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"error creating run: %w\", err)\r\n\t}\r\n\r\n\tc.logger.Debug(fmt.Sprintf(\"Run created with ID: %s\", runResp.ID))\r\n\treturn runResp.ID, nil\r\n}\r\n\r\nfunc (c *AIYOUClient) waitForCompletion(threadID, runID string) (*Run, error) {\r\n\tmaxAttempts := 30\r\n\tdelayBetweenAttempts := 2 * time.Second\r\n\r\n\tfor i := 0; i \u003c maxAttempts; i++ {\r\n\t\tc.logger.Debug(fmt.Sprintf(\"Attempt %d to retrieve run status\", i+1))\r\n\t\trun, err := c.retrieveRun(threadID, runID)\r\n\t\tif err != nil {\r\n\t\t\treturn nil, err\r\n\t\t}\r\n\r\n\t\tswitch run.Status {\r\n\t\tcase \"completed\":\r\n\t\t\tc.logger.Debug(\"Run completed successfully\")\r\n\t\t\treturn run, nil\r\n\t\tcase \"failed\", \"cancelled\":\r\n\t\t\treturn nil, fmt.Errorf(\"run failed with status: %s\", run.Status)\r\n\t\tdefault:\r\n\t\t\tc.logger.Debug(fmt.Sprintf(\"Waiting for run completion. Pausing for %v\", delayBetweenAttempts))\r\n\t\t\ttime.Sleep(delayBetweenAttempts)\r\n\t\t}\r\n\t}\r\n\r\n\treturn nil, fmt.Errorf(\"timeout waiting for run completion\")\r\n}\r\n\r\nfunc (c *AIYOUClient) retrieveRun(threadID, runID string) (*Run, error) {\r\n\tc.logger.Debug(fmt.Sprintf(\"Retrieving run %s for thread %s\", runID, threadID))\r\n\r\n\tvar runStatus Run\r\n\terr := c.apiCaller.Call(fmt.Sprintf(\"/v1/threads/%s/runs/%s\", threadID, runID), \"POST\", map[string]string{}, \u0026runStatus)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"error retrieving run: %w\", err)\r\n\t}\r\n\r\n\tc.logger.Debug(fmt.Sprintf(\"Run status retrieved: %v\", runStatus))\r\n\treturn \u0026runStatus, nil\r\n}\r\n\r\nfunc NewHTTPAPICaller(baseURL string) APICaller {\r\n\treturn \u0026HTTPAPICaller{\r\n\t\tbaseURL: baseURL,\r\n\t\thttpClient: \u0026http.Client{\r\n\t\t\tTimeout: 120 * time.Second,\r\n\t\t},\r\n\t}\r\n}\r\n\r\nfunc (c *HTTPAPICaller) Call(endpoint, method string, data interface{}, response interface{}) error {\r\n\turl := c.baseURL + endpoint\r\n\tvar req *http.Request\r\n\tvar err error\r\n\r\n\tif data != nil {\r\n\t\tjsonData, err := json.Marshal(data)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"error marshaling request data: %w\", err)\r\n\t\t}\r\n\t\treq, err = http.NewRequest(method, url, bytes.NewBuffer(jsonData))\r\n\t} else {\r\n\t\treq, err = http.NewRequest(method, url, nil)\r\n\t}\r\n\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"error creating request: %w\", err)\r\n\t}\r\n\r\n\treq.Header.Set(\"Content-Type\", \"application/json\")\r\n\tif c.token != \"\" {\r\n\t\treq.Header.Set(\"Authorization\", \"Bearer \"+c.token)\r\n\t}\r\n\r\n\tresp, err := c.httpClient.Do(req)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"error sending request: %w\", err)\r\n\t}\r\n\tdefer resp.Body.Close()\r\n\r\n\tbody, err := ioutil.ReadAll(resp.Body)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"error reading response body: %w\", err)\r\n\t}\r\n\r\n\tif resp.StatusCode != http.StatusOK \u0026\u0026 resp.StatusCode != http.StatusCreated {\r\n\t\treturn fmt.Errorf(\"API error: status code %d, body: %s\", resp.StatusCode, string(body))\r\n\t}\r\n\r\n\terr = json.Unmarshal(body, response)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"error unmarshaling response: %w\", err)\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n\r\nfunc (c *HTTPAPICaller) SetToken(token string) {\r\n\tc.token = token\r\n}\r\n",
    "size": 7742,
    "modTime": "2024-10-30T22:49:14.3112954+01:00",
    "path": "internal\\llm\\aiyou.go"
  },
  {
    "name": "claude.go",
    "content": "// internal/llm/claude.go\r\n\r\npackage llm\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"net/http\"\r\n\t\"strings\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n)\r\n\r\n// ClaudeClient implements the Client interface for Claude\r\ntype ClaudeClient struct {\r\n\tapiKey string\r\n\tmodel  string\r\n\tclient *http.Client\r\n\tconfig *config.Config\r\n}\r\n\r\n// supportedClaudeModels defines the list of supported Claude models\r\nvar supportedClaudeModels = map[string]bool{\r\n\t\"claude-3-5-sonnet-20240620\": true,\r\n\t\"claude-3-opus-20240229\":     true,\r\n\t\"claude-3-haiku-20240307\":    true,\r\n}\r\n\r\n// NewClaudeClient creates a new Claude client\r\nfunc NewClaudeClient(apiKey string, model string) (*ClaudeClient, error) {\r\n\tlog.Debug(\"Creating new Claude client with model: %s\", model)\r\n\tif apiKey == \"\" {\r\n\t\tlog.Error(\"API key is missing for Claude client\")\r\n\t\treturn nil, ErrAPIKeyMissing\r\n\t}\r\n\r\n\tif !supportedClaudeModels[model] {\r\n\t\tlog.Error(\"Unsupported Claude model: %s\", model)\r\n\t\treturn nil, fmt.Errorf(\"%w: %s\", ErrUnsupportedModel, model)\r\n\t}\r\n\r\n\treturn \u0026ClaudeClient{\r\n\t\tapiKey: apiKey,\r\n\t\tmodel:  model,\r\n\t\tclient: \u0026http.Client{Timeout: 60 * time.Second},\r\n\t\tconfig: config.GetConfig(),\r\n\t}, nil\r\n}\r\n\r\n// Translate sends a prompt to the Claude API and returns the response\r\nfunc (c *ClaudeClient) Translate(prompt string, context string) (string, error) {\r\n\tlog.Debug(i18n.Messages.TranslationStarted, \"Claude\", c.model)\r\n\tlog.Debug(\"Starting Translate. Prompt length: %d, Context length: %d\", len(prompt), len(context))\r\n\r\n\tvar result string\r\n\tvar err error\r\n\tmaxRetries := 5\r\n\tbaseDelay := time.Second * 10\r\n\tmaxDelay := time.Minute * 2\r\n\r\n\tfor attempt := 0; attempt \u003c maxRetries; attempt++ {\r\n\t\tlog.Debug(\"Attempt %d of %d\", attempt+1, maxRetries)\r\n\t\tresult, err = c.makeRequest(prompt, context)\r\n\t\tif err == nil {\r\n\t\t\tlog.Debug(i18n.Messages.TranslationCompleted, \"Claude\", c.model)\r\n\t\t\treturn result, nil\r\n\t\t}\r\n\r\n\t\tif !isRateLimitError(err) {\r\n\t\t\tlog.Warning(i18n.Messages.TranslationRetry, attempt+1, err)\r\n\t\t\ttime.Sleep(time.Duration(attempt+1) * time.Second)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tdelay := baseDelay * time.Duration(1\u003c\u003cuint(attempt))\r\n\t\tif delay \u003e maxDelay {\r\n\t\t\tdelay = maxDelay\r\n\t\t}\r\n\t\tlog.Warning(i18n.Messages.RateLimitExceeded, delay)\r\n\t\ttime.Sleep(delay)\r\n\t}\r\n\r\n\tlog.Error(i18n.Messages.TranslationFailed, err)\r\n\tlog.Debug(\"Translation completed. Result length: %d\", len(result))\r\n\r\n\treturn \"\", fmt.Errorf(\"%w: %v\", ErrTranslationFailed, err)\r\n}\r\n\r\nfunc isRateLimitError(err error) bool {\r\n\treturn strings.Contains(err.Error(), \"rate_limit_error\")\r\n}\r\n\r\nfunc (c *ClaudeClient) makeRequest(prompt string, context string) (string, error) {\r\n\tlog.Debug(\"Making request to Claude API\")\r\n\turl := config.GetConfig().ClaudeAPIURL\r\n\r\n\trequestBody, err := json.Marshal(map[string]interface{}{\r\n\t\t\"model\": c.model,\r\n\t\t\"messages\": []map[string]string{\r\n\t\t\t{\"role\": \"user\", \"content\": prompt},\r\n\t\t},\r\n\t\t\"system\":     context,\r\n\t\t\"max_tokens\": c.config.MaxTokens,\r\n\t})\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error marshalling request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error marshalling request: %w\", err)\r\n\t}\r\n\t//log.Debug(\"Claude API Request Body : %s\", requestBody)\r\n\treq, err := http.NewRequest(\"POST\", url, bytes.NewBuffer(requestBody))\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error creating request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error creating request: %w\", err)\r\n\t}\r\n\r\n\treq.Header.Set(\"Content-Type\", \"application/json\")\r\n\treq.Header.Set(\"x-api-key\", c.apiKey)\r\n\treq.Header.Set(\"anthropic-version\", \"2023-06-01\")\r\n\r\n\t//log.Debug(\"Sending request to Claude API : %s\", prompt)\r\n\tresp, err := c.client.Do(req)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error sending request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error sending request: %w\", err)\r\n\t}\r\n\tdefer resp.Body.Close()\r\n\r\n\tbody, err := ioutil.ReadAll(resp.Body)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error reading response: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error reading response: %w\", err)\r\n\t}\r\n\r\n\tif resp.StatusCode != http.StatusOK {\r\n\t\tlog.Error(\"API request failed with status code %d: %s\", resp.StatusCode, string(body))\r\n\t\treturn \"\", fmt.Errorf(\"API request failed with status code %d: %s\", resp.StatusCode, string(body))\r\n\t}\r\n\r\n\tvar response struct {\r\n\t\tContent []struct {\r\n\t\t\tText string `json:\"text\"`\r\n\t\t} `json:\"content\"`\r\n\t}\r\n\r\n\terr = json.Unmarshal(body, \u0026response)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error unmarshalling response: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error unmarshalling response: %w\", err)\r\n\t}\r\n\r\n\tif len(response.Content) == 0 {\r\n\t\tlog.Error(\"No content in response\")\r\n\t\treturn \"\", fmt.Errorf(\"no content in response\")\r\n\t}\r\n\r\n\tlog.Debug(\"Successfully received and parsed response from Claude API.\")\r\n\treturn response.Content[0].Text, nil\r\n}\r\n\r\n// ProcessWithPrompt processes a prompt template with the given values and sends it to the Claude API\r\nfunc (c *ClaudeClient) ProcessWithPrompt(promptTemplate *prompt.PromptTemplate, values map[string]string) (string, error) {\r\n\tlog.Debug(\"Processing prompt with Claude\")\r\n\tformattedPrompt := promptTemplate.Format(values)\r\n\r\n\t// Utilisez la méthode Translate existante pour envoyer le prompt formatté\r\n\treturn c.Translate(formattedPrompt, \"\")\r\n}\r\n",
    "size": 5274,
    "modTime": "2024-10-30T22:49:14.3132957+01:00",
    "path": "internal\\llm\\claude.go"
  },
  {
    "name": "client.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n)\r\n\r\n// Client defines the interface for LLM clients\r\ntype Client interface {\r\n\t// Translate takes a prompt and context, and returns the LLM's response\r\n\tTranslate(prompt string, context string) (string, error)\r\n\tProcessWithPrompt(promptTemplate *prompt.PromptTemplate, values map[string]string) (string, error)\r\n}\r\n",
    "size": 390,
    "modTime": "2024-10-21T00:09:55.4348073+02:00",
    "path": "internal\\llm\\client.go"
  },
  {
    "name": "constants.go",
    "content": "// internal/llm/constants.go\r\n\r\npackage llm\r\n\r\nimport \"time\"\r\n\r\n\r\nconst (\r\n    MaxRetries        = 5\r\n    InitialRetryDelay = 1 * time.Second\r\n    MaxRetryDelay     = 32 * time.Second\r\n)\r\n\r\nvar ModelContextLimits = map[string]int{\r\n    \"GPT-4o\":                 8192,\r\n    \"GPT-4o mini\":            4096,\r\n    \"o1-preview\":             16384,\r\n    \"o1-mini\":                2048,\r\n    \"claude-3-5-sonnet-20240620\": 200000,\r\n    \"claude-3-opus-20240229\":     200000,\r\n    \"claude-3-haiku-20240307\":    200000,\r\n    \"llama3.2:3B\":            4096,\r\n    \"llama3.1:8B\":            4096,\r\n    \"mistral-nemo:12B\":       8192,\r\n    \"mixtral:7B\":             32768,\r\n    \"mistral:7B\":             8192,\r\n    \"mistral-small:22B\":      16384,\r\n}",
    "size": 735,
    "modTime": "2024-10-20T15:18:13.4507413+02:00",
    "path": "internal\\llm\\constants.go"
  },
  {
    "name": "errors.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"errors\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\nvar (\r\n\tErrUnsupportedModel  = errors.New(i18n.Messages.ErrUnsupportedModel)\r\n\tErrAPIKeyMissing     = errors.New(i18n.Messages.ErrAPIKeyMissing)\r\n\tErrTranslationFailed = errors.New(i18n.Messages.ErrTranslationFailed)\r\n\tErrInvalidLLMType    = errors.New(i18n.Messages.ErrInvalidLLMType)\r\n\tErrContextTooLong    = errors.New(i18n.Messages.ErrContextTooLong)\r\n)\r\n",
    "size": 449,
    "modTime": "2024-10-20T20:00:43.9609939+02:00",
    "path": "internal\\llm\\errors.go"
  },
  {
    "name": "factory.go",
    "content": "// internal/llm/factory.go\r\n\r\npackage llm\r\n\r\nimport (\r\n\t\"fmt\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n)\r\n\r\nfunc GetClient(llmType string, model string) (Client, error) {\r\n\tcfg := config.GetConfig()\r\n\r\n\tswitch llmType {\r\n\tcase \"openai\":\r\n\t\treturn NewOpenAIClient(cfg.OpenAIAPIKey, model)\r\n\tcase \"claude\":\r\n\t\treturn NewClaudeClient(cfg.ClaudeAPIKey, model)\r\n\tcase \"ollama\":\r\n\t\treturn NewOllamaClient(model)\r\n\tcase \"aiyou\":\r\n        return NewAIYOUClient(cfg.AIYOUAssistantID, cfg.AIYOUEmail, cfg.AIYOUPassword)\r\n\tdefault:\r\n\t\treturn nil, fmt.Errorf(\"%w: %s\", ErrInvalidLLMType, llmType)\r\n\t}\r\n}\r\n\r\ntype contextCheckingClient struct {\r\n\tbaseClient Client\r\n\tmodel      string\r\n}\r\n\r\nfunc (c *contextCheckingClient) Translate(prompt string, context string) (string, error) {\r\n\tif err := CheckContextLength(c.model, context); err != nil {\r\n\t\treturn \"\", err\r\n\t}\r\n\treturn c.baseClient.Translate(prompt, context)\r\n}\r\n",
    "size": 917,
    "modTime": "2024-10-30T22:49:14.3158037+01:00",
    "path": "internal\\llm\\factory.go"
  },
  {
    "name": "logger.go",
    "content": "// internal/llm/logger.go\r\n\r\npackage llm\r\n\r\nimport (\r\n    \"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()",
    "size": 139,
    "modTime": "2024-10-20T15:51:39.514526+02:00",
    "path": "internal\\llm\\logger.go"
  },
  {
    "name": "ollama.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"net/http\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n)\r\n\r\ntype OllamaClient struct {\r\n\tmodel  string\r\n\tclient *http.Client\r\n\tconfig *config.Config\r\n}\r\n\r\nvar supportedOllamaModels = map[string]bool{\r\n\t\"llama3.2\":      true,\r\n\t\"llama3.1\":      true,\r\n\t\"mistral-nemo\":  true,\r\n\t\"mixtral\":       true,\r\n\t\"mistral\":       true,\r\n\t\"mistral-small\": true,\r\n}\r\n\r\nfunc NewOllamaClient(model string) (*OllamaClient, error) {\r\n\tlog.Debug(\"Creating new Ollama client with model: %s\", model)\r\n\tif !supportedOllamaModels[model] {\r\n\t\tlog.Error(\"Unsupported Ollama model: %s\", model)\r\n\t\treturn nil, fmt.Errorf(\"%w: %s\", ErrUnsupportedModel, model)\r\n\t}\r\n\r\n\treturn \u0026OllamaClient{\r\n\t\tmodel:  model,\r\n\t\tclient: \u0026http.Client{Timeout: 60 * time.Second},\r\n\t\tconfig: config.GetConfig(),\r\n\t}, nil\r\n}\r\n\r\nfunc (c *OllamaClient) Translate(prompt string, context string) (string, error) {\r\n\tlog.Debug(i18n.Messages.TranslationStarted, \"Ollama\", c.model)\r\n\tlog.Debug(\"Starting Translate. Prompt length: %d, Context length: %d\", len(prompt), len(context))\r\n\r\n\tvar result string\r\n\tvar err error\r\n\tmaxRetries := 5\r\n\tbaseDelay := time.Second * 60\r\n\tmaxDelay := time.Minute * 5\r\n\r\n\tfor attempt := 0; attempt \u003c maxRetries; attempt++ {\r\n\t\tlog.Debug(\"Attempt %d of %d\", attempt+1, maxRetries)\r\n\t\tresult, err = c.makeRequest(prompt, context)\r\n\t\tif err == nil {\r\n\t\t\tlog.Debug(i18n.Messages.TranslationCompleted, \"Ollama\", c.model)\r\n\t\t\treturn result, nil\r\n\t\t}\r\n\r\n\t\tif !isRateLimitError(err) {\r\n\t\t\tlog.Warning(i18n.Messages.TranslationRetry, attempt+1, err)\r\n\t\t\ttime.Sleep(time.Duration(attempt+1) * time.Second)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tdelay := baseDelay * time.Duration(1\u003c\u003cuint(attempt))\r\n\t\tif delay \u003e maxDelay {\r\n\t\t\tdelay = maxDelay\r\n\t\t}\r\n\t\tlog.Warning(i18n.Messages.RateLimitExceeded, delay)\r\n\t\ttime.Sleep(delay)\r\n\t}\r\n\r\n\tlog.Error(i18n.Messages.TranslationFailed, err)\r\n\tlog.Debug(\"Translation completed. Result length: %d\", len(result))\r\n\r\n\treturn \"\", fmt.Errorf(\"%w: %v\", ErrTranslationFailed, err)\r\n}\r\n\r\nfunc (c *OllamaClient) makeRequest(prompt string, context string) (string, error) {\r\n\tlog.Debug(\"Making request to Ollama API\")\r\n\turl := c.config.OllamaAPIURL\r\n\r\n\trequestBody, err := json.Marshal(map[string]interface{}{\r\n\t\t\"model\":  c.model,\r\n\t\t\"prompt\": prompt,\r\n\t\t\"system\": context,\r\n\t\t\"stream\": false,\r\n\t})\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error marshalling request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error marshalling request: %w\", err)\r\n\t}\r\n\r\n\treq, err := http.NewRequest(\"POST\", url, bytes.NewBuffer(requestBody))\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error creating request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error creating request: %w\", err)\r\n\t}\r\n\r\n\treq.Header.Set(\"Content-Type\", \"application/json\")\r\n\r\n\tresp, err := c.client.Do(req)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error sending request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error sending request: %w\", err)\r\n\t}\r\n\tdefer resp.Body.Close()\r\n\r\n\tbody, err := ioutil.ReadAll(resp.Body)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error reading response: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error reading response: %w\", err)\r\n\t}\r\n\r\n\tif resp.StatusCode != http.StatusOK {\r\n\t\tlog.Error(\"API request failed with status code %d: %s\", resp.StatusCode, string(body))\r\n\t\treturn \"\", fmt.Errorf(\"API request failed with status code %d: %s\", resp.StatusCode, string(body))\r\n\t}\r\n\r\n\tvar response struct {\r\n\t\tResponse string `json:\"response\"`\r\n\t}\r\n\r\n\terr = json.Unmarshal(body, \u0026response)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error unmarshalling response: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error unmarshalling response: %w\", err)\r\n\t}\r\n\r\n\tlog.Debug(\"Successfully received and parsed response from Ollama API.\")\r\n\treturn response.Response, nil\r\n}\r\n\r\nfunc (c *OllamaClient) ProcessWithPrompt(promptTemplate *prompt.PromptTemplate, values map[string]string) (string, error) {\r\n\tlog.Debug(\"Processing prompt with Ollama\")\r\n\tformattedPrompt := promptTemplate.Format(values)\r\n\r\n\treturn c.Translate(formattedPrompt, \"\")\r\n}\r\n",
    "size": 4096,
    "modTime": "2024-10-30T22:49:14.3188118+01:00",
    "path": "internal\\llm\\ollama.go"
  },
  {
    "name": "openai.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"context\"\r\n\t\"fmt\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n\t\"github.com/sashabaranov/go-openai\"\r\n)\r\n\r\ntype OpenAIClient struct {\r\n\tapiKey string\r\n\tmodel  string\r\n\tclient *openai.Client\r\n\tconfig *config.Config\r\n}\r\n\r\nvar supportedModels = map[string]bool{\r\n\t\"gpt-4o\":      true,\r\n\t\"gpt-4o-mini\": true,\r\n\t\"o1-preview\":  true,\r\n\t\"o1-mini\":     true,\r\n}\r\n\r\nfunc NewOpenAIClient(apiKey string, model string) (*OpenAIClient, error) {\r\n\tlog.Debug(\"Creating new OpenAI client with model: %s\", model)\r\n\tif apiKey == \"\" {\r\n\t\tlog.Error(\"API key is missing for OpenAI client\")\r\n\t\treturn nil, ErrAPIKeyMissing\r\n\t}\r\n\r\n\tif !supportedModels[model] {\r\n\t\tlog.Error(\"Unsupported OpenAI model: %s\", model)\r\n\t\treturn nil, fmt.Errorf(\"%w: %s\", ErrUnsupportedModel, model)\r\n\t}\r\n\r\n\tclient := openai.NewClient(apiKey)\r\n\treturn \u0026OpenAIClient{\r\n\t\tapiKey: apiKey,\r\n\t\tmodel:  model,\r\n\t\tclient: client,\r\n\t\tconfig: config.GetConfig(),\r\n\t}, nil\r\n}\r\n\r\nfunc (c *OpenAIClient) Translate(prompt string, context string) (string, error) {\r\n\tlog.Debug(i18n.Messages.TranslationStarted, \"OpenAI\", c.model)\r\n\tlog.Debug(\"Starting Translate. Prompt length: %d, Context length: %d\", len(prompt), len(context))\r\n\r\n\tvar result string\r\n\tvar err error\r\n\tmaxRetries := 5\r\n\tbaseDelay := time.Second * 10\r\n\tmaxDelay := time.Minute * 2\r\n\r\n\tfor attempt := 0; attempt \u003c maxRetries; attempt++ {\r\n\t\tlog.Debug(\"Attempt %d of %d\", attempt+1, maxRetries)\r\n\t\tresult, err = c.makeRequest(prompt, context)\r\n\t\tif err == nil {\r\n\t\t\tlog.Debug(i18n.Messages.TranslationCompleted, \"OpenAI\", c.model)\r\n\t\t\treturn result, nil\r\n\t\t}\r\n\r\n\t\tif !isRateLimitError(err) {\r\n\t\t\tlog.Warning(i18n.Messages.TranslationRetry, attempt+1, err)\r\n\t\t\ttime.Sleep(time.Duration(attempt+1) * time.Second)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tdelay := baseDelay * time.Duration(1\u003c\u003cuint(attempt))\r\n\t\tif delay \u003e maxDelay {\r\n\t\t\tdelay = maxDelay\r\n\t\t}\r\n\t\tlog.Warning(i18n.Messages.RateLimitExceeded, delay)\r\n\t\ttime.Sleep(delay)\r\n\t}\r\n\r\n\tlog.Error(i18n.Messages.TranslationFailed, err)\r\n\tlog.Debug(\"Translation completed. Result length: %d\", len(result))\r\n\r\n\treturn \"\", fmt.Errorf(\"%w: %v\", ErrTranslationFailed, err)\r\n}\r\n\r\nfunc (c *OpenAIClient) makeRequest(prompt string, systemContext string) (string, error) {\r\n\tlog.Debug(\"Making request to OpenAI API\")\r\n\r\n\tmessages := []openai.ChatCompletionMessage{\r\n\t\t{\r\n\t\t\tRole:    openai.ChatMessageRoleSystem,\r\n\t\t\tContent: systemContext,\r\n\t\t},\r\n\t\t{\r\n\t\t\tRole:    openai.ChatMessageRoleUser,\r\n\t\t\tContent: prompt,\r\n\t\t},\r\n\t}\r\n\r\n\tresp, err := c.client.CreateChatCompletion(\r\n\t\tcontext.Background(),\r\n\t\topenai.ChatCompletionRequest{\r\n\t\t\tModel:       c.model,\r\n\t\t\tMessages:    messages,\r\n\t\t\tMaxTokens:   c.config.MaxTokens,\r\n\t\t\tTemperature: 0.7,\r\n\t\t},\r\n\t)\r\n\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error creating chat completion: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error creating chat completion: %w\", err)\r\n\t}\r\n\r\n\tif len(resp.Choices) == 0 {\r\n\t\tlog.Error(\"No content in response\")\r\n\t\treturn \"\", fmt.Errorf(\"no content in response\")\r\n\t}\r\n\r\n\tlog.Debug(\"Successfully received and parsed response from OpenAI API.\")\r\n\treturn resp.Choices[0].Message.Content, nil\r\n}\r\n\r\nfunc (c *OpenAIClient) ProcessWithPrompt(promptTemplate *prompt.PromptTemplate, values map[string]string) (string, error) {\r\n\tlog.Debug(\"Processing prompt with OpenAI\")\r\n\tformattedPrompt := promptTemplate.Format(values)\r\n\r\n\treturn c.Translate(formattedPrompt, \"\")\r\n}\r\n",
    "size": 3489,
    "modTime": "2024-10-30T22:49:14.3208109+01:00",
    "path": "internal\\llm\\openai.go"
  },
  {
    "name": "utils.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/tokenizer\"\r\n)\r\n\r\nfunc CheckContextLength(model string, context string) error {\r\n\tlimit, ok := ModelContextLimits[model]\r\n\tif !ok {\r\n\t\treturn ErrUnsupportedModel\r\n\t}\r\n\r\n\ttokenCount, err := tokenizer.CountTokens(context)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\tif tokenCount \u003e limit {\r\n\t\treturn ErrContextTooLong\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n",
    "size": 399,
    "modTime": "2024-10-20T15:16:37.2625502+02:00",
    "path": "internal\\llm\\utils.go"
  },
  {
    "name": "logger.go",
    "content": "// internal/logger/logger.go\r\n\r\npackage logger\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"log\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"runtime\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\n// LogLevel represents the severity of a log message\r\ntype LogLevel int\r\n\r\nconst (\r\n\t// DebugLevel is used for detailed system operations\r\n\tDebugLevel LogLevel = iota\r\n\t// InfoLevel is used for general operational entries\r\n\tInfoLevel\r\n\t// WarningLevel is used for non-critical issues\r\n\tWarningLevel\r\n\t// ErrorLevel is used for errors that need attention\r\n\tErrorLevel\r\n)\r\n\r\nvar (\r\n\tinstance *Logger\r\n\tonce     sync.Once\r\n)\r\n\r\n// Logger handles all logging operations\r\ntype Logger struct {\r\n\tlevel  LogLevel\r\n\tlogger *log.Logger\r\n\tfile   *os.File\r\n\tmu     sync.Mutex\r\n}\r\n\r\n// GetLogger returns the singleton instance of Logger\r\nfunc GetLogger() *Logger {\r\n\tonce.Do(func() {\r\n\t\tinstance = \u0026Logger{\r\n\t\t\tlevel:  InfoLevel,\r\n\t\t\tlogger: log.New(os.Stdout, \"\", log.Ldate|log.Ltime),\r\n\t\t}\r\n\t\tinstance.setupLogFile()\r\n\t})\r\n\treturn instance\r\n}\r\n\r\nfunc (l *Logger) setupLogFile() {\r\n\tlogDir := config.GetConfig().LogDirectory\r\n\tif logDir == \"\" {\r\n\t\tlogDir = \"logs\"\r\n\t}\r\n\r\n\tif err := os.MkdirAll(logDir, 0755); err != nil {\r\n\t\tl.Error(i18n.GetMessage(\"ErrCreateLogDir\"), err)\r\n\t\treturn\r\n\t}\r\n\r\n\tlogFile := filepath.Join(logDir, fmt.Sprintf(\"ontology_%s.log\", time.Now().Format(\"2006-01-02\")))\r\n\tfile, err := os.OpenFile(logFile, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644)\r\n\tif err != nil {\r\n\t\tl.Error(i18n.GetMessage(\"ErrOpenLogFile\"), err)\r\n\t\treturn\r\n\t}\r\n\r\n\tl.file = file\r\n\tl.logger.SetOutput(io.MultiWriter(os.Stdout, file))\r\n}\r\n\r\n// SetLevel sets the current log level\r\nfunc (l *Logger) SetLevel(level LogLevel) {\r\n\tl.mu.Lock()\r\n\tdefer l.mu.Unlock()\r\n\tl.level = level\r\n}\r\n\r\nfunc (l *Logger) log(level LogLevel, message string, args ...interface{}) {\r\n\tif level \u003c l.level {\r\n\t\treturn\r\n\t}\r\n\r\n\tl.mu.Lock()\r\n\tdefer l.mu.Unlock()\r\n\r\n\t_, file, line, _ := runtime.Caller(2)\r\n\tlevelStr := [...]string{\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"}[level]\r\n\tlogMessage := fmt.Sprintf(\"[%s] %s:%d - %s\", levelStr, filepath.Base(file), line, fmt.Sprintf(message, args...))\r\n\tl.logger.Println(logMessage)\r\n}\r\n\r\n// Debug logs a message at DebugLevel\r\nfunc (l *Logger) Debug(format string, args ...interface{}) {\r\n    if l.GetLevel() \u003c= DebugLevel {\r\n        l.log(DebugLevel, format, args...)\r\n    }\r\n}\r\n\r\n// Info logs a message at InfoLevel\r\nfunc (l *Logger) Info(message string, args ...interface{}) {\r\n\tl.log(InfoLevel, message, args...)\r\n}\r\n\r\n// Warning logs a message at WarningLevel\r\nfunc (l *Logger) Warning(message string, args ...interface{}) {\r\n\tl.log(WarningLevel, message, args...)\r\n}\r\n\r\n// Error logs a message at ErrorLevel\r\nfunc (l *Logger) Error(message string, args ...interface{}) {\r\n\tl.log(ErrorLevel, message, args...)\r\n}\r\n\r\n// Close closes the log file\r\nfunc (l *Logger) Close() {\r\n\tif l.file != nil {\r\n\t\tl.file.Close()\r\n\t}\r\n}\r\n\r\n// UpdateProgress updates the progress on the console\r\nfunc (l *Logger) UpdateProgress(current, total int) {\r\n\tfmt.Printf(\"\\rProgress: %d/%d\", current, total)\r\n}\r\n\r\n// RotateLogs archives old log files\r\nfunc (l *Logger) RotateLogs() error {\r\n\t// Implementation of log rotation\r\n\t// This is a placeholder and should be implemented based on specific requirements\r\n\treturn nil\r\n}\r\n\r\n// ParseLevel converts a string level to LogLevel\r\nfunc ParseLevel(level string) LogLevel {\r\n\tswitch strings.ToLower(level) {\r\n\tcase \"debug\":\r\n\t\treturn DebugLevel\r\n\tcase \"info\":\r\n\t\treturn InfoLevel\r\n\tcase \"warning\":\r\n\t\treturn WarningLevel\r\n\tcase \"error\":\r\n\t\treturn ErrorLevel\r\n\tdefault:\r\n\t\treturn InfoLevel\r\n\t}\r\n}\r\n\r\n// Ajoutez cette méthode\r\nfunc (l *Logger) GetLevel() LogLevel {\r\n\tl.mu.Lock()\r\n\tdefer l.mu.Unlock()\r\n\treturn l.level\r\n}\r\n",
    "size": 3790,
    "modTime": "2024-10-20T23:45:56.7432409+02:00",
    "path": "internal\\logger\\logger.go"
  },
  {
    "name": "metadata.go",
    "content": "// metadata/metadata.go\r\n\r\npackage metadata\r\n\r\nimport (\r\n\t\"crypto/sha256\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()\r\n\r\n// FileMetadata représente les métadonnées d'un fichier source\r\ntype FileMetadata struct {\r\n\tSourceFile      string    `json:\"source_file\"`\r\n\tDirectory       string    `json:\"directory\"`\r\n\tFileDate        time.Time `json:\"file_date\"`\r\n\tSHA256Hash      string    `json:\"sha256_hash\"`\r\n\tOntologyFile    string    `json:\"ontology_file\"`\r\n\tContextFile     string    `json:\"context_file,omitempty\"`\r\n\tProcessingDate  time.Time `json:\"processing_date\"`\r\n}\r\n\r\n// Generator gère la génération des métadonnées\r\ntype Generator struct {\r\n\tlogger *logger.Logger\r\n}\r\n\r\n// NewGenerator crée une nouvelle instance de Generator\r\nfunc NewGenerator() *Generator {\r\n\treturn \u0026Generator{\r\n\t\tlogger: logger.GetLogger(),\r\n\t}\r\n}\r\n\r\n// GenerateMetadata crée les métadonnées pour un fichier source\r\nfunc (g *Generator) GenerateMetadata(sourcePath, ontologyFile, contextFile string) (*FileMetadata, error) {\r\n\tg.logger.Debug(\"Generating metadata for source file: %s\", sourcePath)\r\n\r\n\t// Obtenir les informations sur le fichier\r\n\tfileInfo, err := os.Stat(sourcePath)\r\n\tif err != nil {\r\n\t\tg.logger.Error(\"Failed to get file info: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to get file info: %w\", err)\r\n\t}\r\n\r\n\t// Calculer le hash SHA256\r\n\thash, err := g.calculateSHA256(sourcePath)\r\n\tif err != nil {\r\n\t\tg.logger.Error(\"Failed to calculate SHA256: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to calculate SHA256: %w\", err)\r\n\t}\r\n\r\n\t// Créer les métadonnées\r\n\tmetadata := \u0026FileMetadata{\r\n\t\tSourceFile:     filepath.Base(sourcePath),\r\n\t\tDirectory:      filepath.Dir(sourcePath),\r\n\t\tFileDate:      fileInfo.ModTime(),\r\n\t\tSHA256Hash:    hash,\r\n\t\tOntologyFile:  ontologyFile,\r\n\t\tProcessingDate: time.Now(),\r\n\t}\r\n\r\n\t// Ajouter le fichier de contexte s'il existe\r\n\tif contextFile != \"\" {\r\n\t\tmetadata.ContextFile = contextFile\r\n\t}\r\n\r\n\tg.logger.Debug(\"Generated metadata: %+v\", metadata)\r\n\treturn metadata, nil\r\n}\r\n\r\n// SaveMetadata sauvegarde les métadonnées dans un fichier JSON\r\nfunc (g *Generator) SaveMetadata(metadata *FileMetadata, outputPath string) error {\r\n\tg.logger.Debug(\"Saving metadata to file: %s\", outputPath)\r\n\r\n\t// Créer le contenu JSON avec une indentation pour la lisibilité\r\n\tjsonData, err := json.MarshalIndent(metadata, \"\", \"  \")\r\n\tif err != nil {\r\n\t\tg.logger.Error(\"Failed to marshal metadata: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to marshal metadata: %w\", err)\r\n\t}\r\n\r\n\t// Écrire dans le fichier\r\n\terr = os.WriteFile(outputPath, jsonData, 0644)\r\n\tif err != nil {\r\n\t\tg.logger.Error(\"Failed to write metadata file: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to write metadata file: %w\", err)\r\n\t}\r\n\r\n\tg.logger.Info(\"Metadata saved successfully to: %s\", outputPath)\r\n\treturn nil\r\n}\r\n\r\n// calculateSHA256 calcule le hash SHA256 d'un fichier\r\nfunc (g *Generator) calculateSHA256(filePath string) (string, error) {\r\n\tfile, err := os.Open(filePath)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"failed to open file: %w\", err)\r\n\t}\r\n\tdefer file.Close()\r\n\r\n\thash := sha256.New()\r\n\tif _, err := io.Copy(hash, file); err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"failed to calculate hash: %w\", err)\r\n\t}\r\n\r\n\treturn fmt.Sprintf(\"%x\", hash.Sum(nil)), nil\r\n}\r\n\r\n// GetMetadataFilename génère le nom du fichier de métadonnées\r\nfunc (g *Generator) GetMetadataFilename(sourcePath string) string {\r\n\tbaseFileName := filepath.Base(sourcePath)\r\n\text := filepath.Ext(baseFileName)\r\n\tnameWithoutExt := baseFileName[:len(baseFileName)-len(ext)]\r\n\treturn nameWithoutExt + \"_meta.json\"\r\n}",
    "size": 3687,
    "modTime": "2024-10-30T22:49:14.3228122+01:00",
    "path": "internal\\metadata\\metadata.go"
  },
  {
    "name": "ontology.go",
    "content": "package model\r\n\r\n// OntologyElement représente un élément unique dans l'ontologie\r\ntype OntologyElement struct {\r\n\tName      string // Nom de l'élément\r\n\tType      string // Type de l'élément\r\n\tPositions []int  // Positions de l'élément dans le document source\r\n\tDescription string \r\n\r\n}\r\n\r\n// Relation représente une relation entre deux éléments de l'ontologie\r\ntype Relation struct {\r\n\tSource      string\r\n\tType        string\r\n\tTarget      string\r\n\tDescription string\r\n}\r\n\r\n// Ontology représente une collection d'éléments d'ontologie\r\ntype Ontology struct {\r\n\tElements  []*OntologyElement // Liste des éléments de l'ontologie\r\n\tRelations []*Relation\r\n}\r\n\r\n// SetPositions définit les positions de l'élément\r\nfunc (e *OntologyElement) SetPositions(positions []int) {\r\n    e.Positions = positions\r\n}\r\n\r\n// NewOntologyElement crée un nouvel élément d'ontologie\r\nfunc NewOntologyElement(name, elementType string) *OntologyElement {\r\n\treturn \u0026OntologyElement{\r\n\t\tName:      name,\r\n\t\tType:      elementType,\r\n\t\tPositions: []int{}, // Initialise un slice vide pour les positions\r\n\t}\r\n}\r\n\r\n// NewOntology crée une nouvelle instance d'Ontology\r\nfunc NewOntology() *Ontology {\r\n\treturn \u0026Ontology{\r\n\t\tElements: []*OntologyElement{}, // Initialise un slice vide pour les éléments\r\n\t}\r\n}\r\n\r\n// AddElement ajoute un nouvel élément à l'ontologie\r\nfunc (o *Ontology) AddElement(element *OntologyElement) {\r\n\to.Elements = append(o.Elements, element)\r\n}\r\n\r\n// GetElementByName recherche un élément par son nom\r\nfunc (o *Ontology) GetElementByName(name string) *OntologyElement {\r\n\tfor _, element := range o.Elements {\r\n\t\tif element.Name == name {\r\n\t\t\treturn element\r\n\t\t}\r\n\t}\r\n\treturn nil // Retourne nil si l'élément n'est pas trouvé\r\n}\r\n\r\n// AddRelation ajoute une nouvelle relation à l'ontologie\r\nfunc (o *Ontology) AddRelation(relation *Relation) {\r\n\to.Relations = append(o.Relations, relation)\r\n}\r\n",
    "size": 1922,
    "modTime": "2024-10-30T22:49:14.3248112+01:00",
    "path": "internal\\model\\ontology.go"
  },
  {
    "name": "element.go",
    "content": "package ontology\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"strconv\"\r\n\t\"strings\"\r\n)\r\n\r\n// OntologyElement représente un élément unique dans l'ontologie\r\ntype OntologyElement struct {\r\n\tName      string\r\n\tType      string\r\n\tPositions []int\r\n}\r\n\r\n// NewOntologyElement crée un nouvel OntologyElement\r\nfunc NewOntologyElement(name, elementType string) *OntologyElement {\r\n\treturn \u0026OntologyElement{\r\n\t\tName:      name,\r\n\t\tType:      elementType,\r\n\t\tPositions: []int{},\r\n\t}\r\n}\r\n\r\n// AddPosition ajoute une nouvelle position à l'élément\r\nfunc (e *OntologyElement) AddPosition(position int) {\r\n\te.Positions = append(e.Positions, position)\r\n}\r\n\r\n// SetPositions remplace toutes les positions existantes par un nouveau slice\r\nfunc (e *OntologyElement) SetPositions(positions []int) {\r\n\te.Positions = positions\r\n}\r\n\r\n// String retourne une représentation en chaîne de caractères de l'élément\r\nfunc (e *OntologyElement) String() string {\r\n\tposStr := strings.Trim(strings.Join(strings.Fields(fmt.Sprint(e.Positions)), \",\"), \"[]\")\r\n\treturn fmt.Sprintf(\"%s|%s@%s\", e.Name, e.Type, posStr)\r\n}\r\n\r\n// Ontology représente une collection d'éléments d'ontologie\r\ntype Ontology struct {\r\n\tElements []*OntologyElement\r\n}\r\n\r\n// NewOntology crée une nouvelle instance d'Ontology\r\nfunc NewOntology() *Ontology {\r\n\treturn \u0026Ontology{\r\n\t\tElements: []*OntologyElement{},\r\n\t}\r\n}\r\n\r\n// AddElement ajoute un nouvel élément à l'ontologie ou met à jour un élément existant\r\nfunc (o *Ontology) AddElement(element *OntologyElement) {\r\n\texistingElement := o.GetElementByName(element.Name)\r\n\tif existingElement != nil {\r\n\t\t// Mettre à jour l'élément existant\r\n\t\texistingElement.Type = element.Type\r\n\t\texistingElement.Positions = append(existingElement.Positions, element.Positions...)\r\n\t\t// Supprimer les doublons dans les positions\r\n\t\texistingElement.Positions = removeDuplicates(existingElement.Positions)\r\n\t} else {\r\n\t\t// Ajouter un nouvel élément\r\n\t\to.Elements = append(o.Elements, element)\r\n\t}\r\n}\r\n\r\n// GetElementByName recherche un élément par son nom\r\nfunc (o *Ontology) GetElementByName(name string) *OntologyElement {\r\n\tfor _, element := range o.Elements {\r\n\t\tif element.Name == name {\r\n\t\t\treturn element\r\n\t\t}\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// LoadFromString charge une ontologie à partir d'une chaîne de caractères au format QuickStatement\r\nfunc (o *Ontology) LoadFromString(content string) error {\r\n\tlines := strings.Split(content, \"\\n\")\r\n\tfor _, line := range lines {\r\n\t\tparts := strings.Split(line, \"|\")\r\n\t\tif len(parts) != 2 {\r\n\t\t\tcontinue // Ignorer les lignes mal formatées\r\n\t\t}\r\n\t\tname := parts[0]\r\n\t\ttypeParts := strings.Split(parts[1], \"@\")\r\n\t\tif len(typeParts) != 2 {\r\n\t\t\tcontinue // Ignorer les éléments sans positions\r\n\t\t}\r\n\t\telementType := typeParts[0]\r\n\t\tpositionsStr := strings.Split(typeParts[1], \",\")\r\n\r\n\t\telement := NewOntologyElement(name, elementType)\r\n\t\tfor _, posStr := range positionsStr {\r\n\t\t\tpos, err := strconv.Atoi(posStr)\r\n\t\t\tif err != nil {\r\n\t\t\t\tcontinue // Ignorer les positions non valides\r\n\t\t\t}\r\n\t\t\telement.AddPosition(pos)\r\n\t\t}\r\n\t\to.AddElement(element)\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// ToString convertit l'ontologie en chaîne de caractères au format QuickStatement\r\nfunc (o *Ontology) ToString() string {\r\n\tvar builder strings.Builder\r\n\tfor _, element := range o.Elements {\r\n\t\tposStr := strings.Trim(strings.Join(strings.Fields(fmt.Sprint(element.Positions)), \",\"), \"[]\")\r\n\t\tbuilder.WriteString(fmt.Sprintf(\"%s|%s@%s\\n\", element.Name, element.Type, posStr))\r\n\t}\r\n\treturn builder.String()\r\n}\r\n\r\n// removeDuplicates supprime les doublons dans un slice d'entiers\r\nfunc removeDuplicates(slice []int) []int {\r\n\tkeys := make(map[int]bool)\r\n\tlist := []int{}\r\n\tfor _, entry := range slice {\r\n\t\tif _, value := keys[entry]; !value {\r\n\t\t\tkeys[entry] = true\r\n\t\t\tlist = append(list, entry)\r\n\t\t}\r\n\t}\r\n\treturn list\r\n}\r\n",
    "size": 3812,
    "modTime": "2024-10-22T10:40:49.6022691+02:00",
    "path": "internal\\ontology\\element.go"
  },
  {
    "name": "enrich.go",
    "content": "package ontology\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/chrlesur/Ontology/internal/model\"\r\n\t\"github.com/chrlesur/Ontology/internal/pipeline\"\r\n\r\n\t\"github.com/spf13/cobra\"\r\n)\r\n\r\nvar (\r\n\toutput                   string\r\n\tformat                   string\r\n\tllm                      string\r\n\tllmModel                 string\r\n\tpasses                   int\r\n\trecursive                bool\r\n\texistingOntology         string\r\n\tentityExtractionPrompt   string\r\n\trelationExtractionPrompt string\r\n\tontologyEnrichmentPrompt string\r\n\tontologyMergePrompt      string\r\n)\r\n\r\n// enrichCmd represents the enrich command\r\nvar enrichCmd = \u0026cobra.Command{\r\n\tUse:   \"enrich [input]\",\r\n\tShort: i18n.Messages.EnrichCmdShortDesc,\r\n\tLong:  i18n.Messages.EnrichCmdLongDesc,\r\n\tArgs:  cobra.ExactArgs(1),\r\n\tRunE: func(cmd *cobra.Command, args []string) error {\r\n\t\tinput := args[0]\r\n\t\tlog := logger.GetLogger()\r\n\t\tlog.Info(i18n.Messages.StartingEnrichProcess)\r\n\t\taiyouAssistantID, _ := cmd.Flags().GetString(\"aiyou-assistant-id\")\r\n\t\taiyouEmail, _ := cmd.Flags().GetString(\"aiyou-email\")\r\n\t\taiyouPassword, _ := cmd.Flags().GetString(\"aiyou-password\")\r\n\r\n\t\t// Mettre à jour la configuration si les flags sont fournis\r\n\t\tcfg := config.GetConfig() // Obtenez l'instance de configuration\r\n\t\tif aiyouAssistantID != \"\" {\r\n\t\t\tcfg.AIYOUAssistantID = aiyouAssistantID\r\n\t\t}\r\n\t\tif aiyouEmail != \"\" {\r\n\t\t\tcfg.AIYOUEmail = aiyouEmail\r\n\t\t}\r\n\t\tif aiyouPassword != \"\" {\r\n\t\t\tcfg.AIYOUPassword = aiyouPassword\r\n\t\t}\r\n\t\t// Utiliser le chemin absolu pour l'entrée\r\n\t\tabsInput, err := filepath.Abs(input)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"error getting absolute path: %w\", err)\r\n\t\t}\r\n\r\n\t\t// Déterminer le nom de fichier de sortie si non spécifié\r\n\t\tif output == \"\" {\r\n\t\t\toutput = generateOutputFilename(absInput)\r\n\t\t} else {\r\n\t\t\t// Si output est spécifié, s'assurer qu'il est absolu\r\n\t\t\toutput, err = filepath.Abs(output)\r\n\t\t\tif err != nil {\r\n\t\t\t\treturn fmt.Errorf(\"error getting absolute path for output: %w\", err)\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tp, err := pipeline.NewPipeline(includePositions, contextOutput, contextWords, entityExtractionPrompt, relationExtractionPrompt, ontologyEnrichmentPrompt, ontologyMergePrompt, llm, llmModel)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.Messages.ErrorCreatingPipeline, err)\r\n\t\t}\r\n\r\n\t\tp.SetProgressCallback(func(info pipeline.ProgressInfo) {\r\n\t\t\tswitch info.CurrentStep {\r\n\t\t\tcase \"Starting Pass\":\r\n\t\t\t\tlog.Info(\"Starting pass %d of %d\", info.CurrentPass, info.TotalPasses)\r\n\t\t\tcase \"Segmenting\":\r\n\t\t\t\tlog.Info(\"Segmenting input into %d parts\", info.TotalSegments)\r\n\t\t\tcase \"Processing Segment\":\r\n\t\t\t\tlog.Debug(\"Processing segment %d of %d\", info.ProcessedSegments, info.TotalSegments)\r\n\t\t\t}\r\n\t\t})\r\n\r\n\t\tontology := model.NewOntology() // Utiliser le nouveau package model\r\n\r\n\t\terr = p.ExecutePipeline(absInput, output, passes, existingOntology, ontology)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.Messages.ErrorExecutingPipeline, err)\r\n\t\t}\r\n\r\n\t\tlog.Info(i18n.Messages.EnrichProcessCompleted)\r\n\t\tlog.Info(\"File processed: %s, output: %s\", absInput, output)\r\n\t\treturn nil\r\n\t},\r\n}\r\n\r\nfunc init() {\r\n\trootCmd.AddCommand(enrichCmd)\r\n\r\n\tenrichCmd.Flags().StringVar(\u0026output, \"output\", \"\", i18n.Messages.OutputFlagUsage)\r\n\tenrichCmd.Flags().StringVar(\u0026format, \"format\", \"\", i18n.Messages.FormatFlagUsage)\r\n\tenrichCmd.Flags().StringVar(\u0026llm, \"llm\", \"\", i18n.Messages.LLMFlagUsage)\r\n\tenrichCmd.Flags().StringVar(\u0026llmModel, \"llm-model\", \"\", i18n.Messages.LLMModelFlagUsage)\r\n\tenrichCmd.Flags().IntVar(\u0026passes, \"passes\", 1, i18n.Messages.PassesFlagUsage)\r\n\tenrichCmd.Flags().BoolVar(\u0026recursive, \"recursive\", false, i18n.Messages.RecursiveFlagUsage)\r\n\tenrichCmd.Flags().StringVar(\u0026existingOntology, \"existing-ontology\", \"\", i18n.Messages.ExistingOntologyFlagUsage)\r\n\r\n\tenrichCmd.Flags().StringVarP(\u0026entityExtractionPrompt, \"entity-prompt\", \"e\", \"\", \"Additional prompt for entity extraction\")\r\n\tenrichCmd.Flags().StringVarP(\u0026relationExtractionPrompt, \"relation-prompt\", \"r\", \"\", \"Additional prompt for relation extraction\")\r\n\tenrichCmd.Flags().StringVarP(\u0026ontologyEnrichmentPrompt, \"enrichment-prompt\", \"n\", \"\", \"Additional prompt for ontology enrichment\")\r\n\tenrichCmd.Flags().StringVarP(\u0026ontologyMergePrompt, \"merge-prompt\", \"m\", \"\", \"Additional prompt for ontology merging\")\r\n}\r\n\r\nfunc ExecuteEnrichCommand(input, output string, passes int, existingOntology string, includePositions, contextOutput bool, contextWords int, entityPrompt, relationPrompt, enrichmentPrompt, mergePrompt string) error {\r\n\tlog := logger.GetLogger()\r\n\tlog.Info(i18n.Messages.StartingEnrichProcess)\r\n\r\n\tabsInput, err := filepath.Abs(input)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"error getting absolute path: %w\", err)\r\n\t}\r\n\r\n\tif output == \"\" {\r\n\t\toutput = generateOutputFilename(absInput)\r\n\t} else {\r\n\t\toutput, err = filepath.Abs(output)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"error getting absolute path for output: %w\", err)\r\n\t\t}\r\n\t}\r\n\r\n\tp, err := pipeline.NewPipeline(includePositions, contextOutput, contextWords, entityPrompt, relationPrompt, enrichmentPrompt, mergePrompt, llm, llmModel)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.Messages.ErrorCreatingPipeline, err)\r\n\t}\r\n\r\n\tp.SetProgressCallback(func(info pipeline.ProgressInfo) {\r\n\t\tswitch info.CurrentStep {\r\n\t\tcase \"Starting Pass\":\r\n\t\t\tlog.Info(\"Starting pass %d of %d\", info.CurrentPass, info.TotalPasses)\r\n\t\tcase \"Segmenting\":\r\n\t\t\tlog.Info(\"Segmenting input into %d parts\", info.TotalSegments)\r\n\t\tcase \"Processing Segment\":\r\n\t\t\tlog.Debug(\"Processing segment %d of %d\", info.ProcessedSegments, info.TotalSegments)\r\n\t\t}\r\n\t})\r\n\r\n\tonto := model.NewOntology()\r\n\terr = p.ExecutePipeline(absInput, output, passes, existingOntology, onto)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.Messages.ErrorExecutingPipeline, err)\r\n\t}\r\n\r\n\tlog.Info(i18n.Messages.EnrichProcessCompleted)\r\n\tlog.Info(\"File processed: %s, output: %s\", absInput, output)\r\n\treturn nil\r\n}\r\n\r\nfunc generateOutputFilename(input string) string {\r\n\tdir := filepath.Dir(input)\r\n\tbaseName := filepath.Base(input)\r\n\tbaseName = strings.TrimSuffix(baseName, filepath.Ext(baseName))\r\n\r\n\treturn filepath.Join(dir, baseName+\".tsv\")\r\n}",
    "size": 6317,
    "modTime": "2024-11-04T09:34:28.5796463+01:00",
    "path": "internal\\ontology\\enrich.go"
  },
  {
    "name": "logger.go",
    "content": "package ontology // ou le nom du package approprié\r\n\r\nimport (\r\n    \"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()",
    "size": 150,
    "modTime": "2024-10-20T17:29:08.5359463+02:00",
    "path": "internal\\ontology\\logger.go"
  },
  {
    "name": "root.go",
    "content": "package ontology\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"os\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/spf13/cobra\"\r\n)\r\n\r\nconst Version = \"0.6.0\"\r\n\r\nvar (\r\n\tcfgFile          string\r\n\tdebug            bool\r\n\tsilent           bool\r\n\tincludePositions bool\r\n\tcontextOutput    bool\r\n\tcontextWords     int\r\n)\r\n\r\n// rootCmd represents the base command when called without any subcommands\r\nvar rootCmd = \u0026cobra.Command{\r\n\tUse:     \"ontology\",\r\n\tShort:   i18n.GetMessage(\"RootCmdShortDesc\"),\r\n\tLong:    i18n.GetMessage(\"RootCmdLongDesc\"),\r\n\tVersion: Version,\r\n}\r\n\r\nvar versionCmd = \u0026cobra.Command{\r\n\tUse:   \"version\",\r\n\tShort: \"Print the version number of Ontology\",\r\n\tLong:  `All software has versions. This is Ontology's.`,\r\n\tRun: func(cmd *cobra.Command, args []string) {\r\n\t\tfmt.Printf(\"Ontology version %s\\n\", Version)\r\n\t},\r\n}\r\n\r\n// Execute adds all child commands to the root command and sets flags appropriately.\r\n// This is called by main.main(). It only needs to happen once to the rootCmd.\r\nfunc Execute() {\r\n\terr := rootCmd.Execute()\r\n\tif err != nil {\r\n\t\tos.Exit(1)\r\n\t}\r\n}\r\n\r\nfunc init() {\r\n\tcobra.OnInitialize(initConfig)\r\n\r\n\trootCmd.PersistentFlags().StringVar(\u0026cfgFile, \"config\", \"\", i18n.GetMessage(\"ConfigFlagUsage\"))\r\n\trootCmd.PersistentFlags().BoolVarP(\u0026debug, \"debug\", \"d\", false, i18n.GetMessage(\"DebugFlagUsage\"))\r\n\trootCmd.PersistentFlags().BoolVarP(\u0026silent, \"silent\", \"s\", false, i18n.GetMessage(\"SilentFlagUsage\"))\r\n\trootCmd.PersistentFlags().BoolVarP(\u0026includePositions, \"include-positions\", \"i\", true, i18n.GetMessage(\"IncludePositionsFlagUsage\"))\r\n\trootCmd.PersistentFlags().BoolVarP(\u0026contextOutput, \"context-output\", \"c\", false, i18n.GetMessage(\"ContextOutputFlagUsage\"))\r\n\trootCmd.PersistentFlags().IntVarP(\u0026contextWords, \"context-words\", \"w\", 30, i18n.GetMessage(\"ContextWordsFlagUsage\"))\r\n\trootCmd.PersistentFlags().StringP(\"aiyou-assistant-id\", \"a\", \"\", \"AI.YOU Assistant ID\")\r\n\trootCmd.PersistentFlags().String(\"aiyou-email\", \"\", \"AI.YOU Email\")\r\n\trootCmd.PersistentFlags().String(\"aiyou-password\", \"\", \"AI.YOU Password\")\r\n\trootCmd.Run = rootCmd.HelpFunc()\r\n\r\n\trootCmd.AddCommand(versionCmd)\r\n}\r\n\r\n// initConfig reads in config file and ENV variables if set.\r\nfunc initConfig() {\r\n\tif cfgFile != \"\" {\r\n\t\t// Set the config file path if specified\r\n\t\tos.Setenv(\"ONTOLOGY_CONFIG_PATH\", cfgFile)\r\n\t}\r\n\r\n\t// This will load the config from file and environment variables\r\n\tcfg := config.GetConfig()\r\n\tcfg.ContextOutput = contextOutput\r\n\tcfg.ContextWords = contextWords\r\n\r\n\t// Validate the config\r\n\tif err := cfg.ValidateConfig(); err != nil {\r\n\t\tfmt.Printf(\"Error in configuration: %v\\n\", err)\r\n\t\tos.Exit(1)\r\n\t}\r\n\r\n\tlog := logger.GetLogger()\r\n\r\n\t// Initialize logger based on debug and silent flags\r\n\tif debug {\r\n\t\tlog.SetLevel(logger.DebugLevel)\r\n\t} else if silent {\r\n\t\tlog.SetLevel(logger.ErrorLevel)\r\n\t} else {\r\n\t\tlogLevel := logger.ParseLevel(cfg.LogLevel)\r\n\t\tlog.SetLevel(logLevel)\r\n\t}\r\n\r\n\tlog.Debug(i18n.GetMessage(\"InitializingApplication\"))\r\n}\r\n",
    "size": 3070,
    "modTime": "2024-11-04T09:47:47.215993+01:00",
    "path": "internal\\ontology\\root.go"
  },
  {
    "name": "docx.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"archive/zip\"\r\n\t\"encoding/xml\"\r\n\t\"io/ioutil\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\nfunc init() {\r\n\tRegisterParser(\".docx\", NewDOCXParser)\r\n}\r\n\r\n// DOCXParser implémente l'interface Parser pour les fichiers DOCX\r\ntype DOCXParser struct {\r\n\tmetadata map[string]string\r\n}\r\n\r\n// NewDOCXParser crée une nouvelle instance de DOCXParser\r\nfunc NewDOCXParser() Parser {\r\n\treturn \u0026DOCXParser{\r\n\t\tmetadata: make(map[string]string),\r\n\t}\r\n}\r\n\r\n// Parse extrait le contenu textuel d'un fichier DOCX\r\nfunc (p *DOCXParser) Parse(path string) ([]byte, error) {\r\n\tlog.Debug(i18n.Messages.ParseStarted, \"DOCX\", path)\r\n\r\n\treader, err := zip.OpenReader(path)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.Messages.ParseFailed, \"DOCX\", path, err)\r\n\t\treturn nil, err\r\n\t}\r\n\tdefer reader.Close()\r\n\r\n\tvar textContent strings.Builder\r\n\tfor _, file := range reader.File {\r\n\t\tif file.Name == \"word/document.xml\" {\r\n\t\t\trc, err := file.Open()\r\n\t\t\tif err != nil {\r\n\t\t\t\tlog.Error(i18n.Messages.ParseFailed, \"DOCX\", path, err)\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\t\t\tdefer rc.Close()\r\n\r\n\t\t\tcontent, err := ioutil.ReadAll(rc)\r\n\t\t\tif err != nil {\r\n\t\t\t\tlog.Error(i18n.Messages.ParseFailed, \"DOCX\", path, err)\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\r\n\t\t\tvar document struct {\r\n\t\t\t\tBody struct {\r\n\t\t\t\t\tParagraphs []struct {\r\n\t\t\t\t\t\tRuns []struct {\r\n\t\t\t\t\t\t\tText string `xml:\"t\"`\r\n\t\t\t\t\t\t} `xml:\"r\"`\r\n\t\t\t\t\t} `xml:\"p\"`\r\n\t\t\t\t}\r\n\t\t\t}\r\n\r\n\t\t\terr = xml.Unmarshal(content, \u0026document)\r\n\t\t\tif err != nil {\r\n\t\t\t\tlog.Error(i18n.Messages.ParseFailed, \"DOCX\", path, err)\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\r\n\t\t\tfor _, paragraph := range document.Body.Paragraphs {\r\n\t\t\t\tfor _, run := range paragraph.Runs {\r\n\t\t\t\t\ttextContent.WriteString(run.Text)\r\n\t\t\t\t}\r\n\t\t\t\ttextContent.WriteString(\"\\n\")\r\n\t\t\t}\r\n\t\t\tbreak\r\n\t\t}\r\n\t}\r\n\r\n\tp.extractMetadata(reader)\r\n\tlog.Info(i18n.Messages.ParseCompleted, \"DOCX\", path)\r\n\treturn []byte(textContent.String()), nil\r\n}\r\n\r\n// GetMetadata retourne les métadonnées du fichier DOCX\r\nfunc (p *DOCXParser) GetMetadata() map[string]string {\r\n\treturn p.metadata\r\n}\r\n\r\n// extractMetadata extrait les métadonnées du DOCX\r\nfunc (p *DOCXParser) extractMetadata(reader *zip.ReadCloser) {\r\n\tfor _, file := range reader.File {\r\n\t\tif file.Name == \"docProps/core.xml\" {\r\n\t\t\trc, err := file.Open()\r\n\t\t\tif err != nil {\r\n\t\t\t\tlog.Warning(i18n.Messages.MetadataExtractionFailed, \"DOCX\", err)\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t\tdefer rc.Close()\r\n\r\n\t\t\tcontent, err := ioutil.ReadAll(rc)\r\n\t\t\tif err != nil {\r\n\t\t\t\tlog.Warning(i18n.Messages.MetadataExtractionFailed, \"DOCX\", err)\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\r\n\t\t\tvar coreProps struct {\r\n\t\t\t\tTitle          string `xml:\"title\"`\r\n\t\t\t\tSubject        string `xml:\"subject\"`\r\n\t\t\t\tCreator        string `xml:\"creator\"`\r\n\t\t\t\tKeywords       string `xml:\"keywords\"`\r\n\t\t\t\tDescription    string `xml:\"description\"`\r\n\t\t\t\tLastModifiedBy string `xml:\"lastModifiedBy\"`\r\n\t\t\t\tRevision       string `xml:\"revision\"`\r\n\t\t\t}\r\n\r\n\t\t\terr = xml.Unmarshal(content, \u0026coreProps)\r\n\t\t\tif err != nil {\r\n\t\t\t\tlog.Warning(i18n.Messages.MetadataExtractionFailed, \"DOCX\", err)\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\r\n\t\t\tp.metadata[\"title\"] = coreProps.Title\r\n\t\t\tp.metadata[\"subject\"] = coreProps.Subject\r\n\t\t\tp.metadata[\"creator\"] = coreProps.Creator\r\n\t\t\tp.metadata[\"keywords\"] = coreProps.Keywords\r\n\t\t\tp.metadata[\"description\"] = coreProps.Description\r\n\t\t\tp.metadata[\"lastModifiedBy\"] = coreProps.LastModifiedBy\r\n\t\t\tp.metadata[\"revision\"] = coreProps.Revision\r\n\t\t}\r\n\t}\r\n}\r\n",
    "size": 3415,
    "modTime": "2024-10-20T20:00:14.2181355+02:00",
    "path": "internal\\parser\\docx.go"
  },
  {
    "name": "html.go",
    "content": "package parser\r\n\r\nimport (\r\n    \"io/ioutil\"\r\n    \"strings\"\r\n\r\n    \"golang.org/x/net/html\"\r\n    \"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\nfunc init() {\r\n    RegisterParser(\".html\", NewHTMLParser)\r\n}\r\n\r\n// HTMLParser implémente l'interface Parser pour les fichiers HTML\r\ntype HTMLParser struct {\r\n    metadata map[string]string\r\n}\r\n\r\n// NewHTMLParser crée une nouvelle instance de HTMLParser\r\nfunc NewHTMLParser() Parser {\r\n    return \u0026HTMLParser{\r\n        metadata: make(map[string]string),\r\n    }\r\n}\r\n\r\n// Parse extrait le contenu textuel d'un fichier HTML\r\nfunc (p *HTMLParser) Parse(path string) ([]byte, error) {\r\n    log.Debug(i18n.Messages.ParseStarted, \"HTML\", path)\r\n    content, err := ioutil.ReadFile(path)\r\n    if err != nil {\r\n        log.Error(i18n.Messages.ParseFailed, \"HTML\", path, err)\r\n        return nil, err\r\n    }\r\n\r\n    doc, err := html.Parse(strings.NewReader(string(content)))\r\n    if err != nil {\r\n        log.Error(i18n.Messages.ParseFailed, \"HTML\", path, err)\r\n        return nil, err\r\n    }\r\n\r\n    var textContent strings.Builder\r\n    var extractText func(*html.Node)\r\n    extractText = func(n *html.Node) {\r\n        if n.Type == html.TextNode {\r\n            textContent.WriteString(n.Data)\r\n        }\r\n        for c := n.FirstChild; c != nil; c = c.NextSibling {\r\n            extractText(c)\r\n        }\r\n    }\r\n    extractText(doc)\r\n\r\n    p.extractMetadata(doc)\r\n    log.Info(i18n.Messages.ParseCompleted, \"HTML\", path)\r\n    return []byte(textContent.String()), nil\r\n}\r\n\r\n// GetMetadata retourne les métadonnées du fichier HTML\r\nfunc (p *HTMLParser) GetMetadata() map[string]string {\r\n    return p.metadata\r\n}\r\n\r\n// extractMetadata extrait les métadonnées du HTML\r\nfunc (p *HTMLParser) extractMetadata(doc *html.Node) {\r\n    var f func(*html.Node)\r\n    f = func(n *html.Node) {\r\n        if n.Type == html.ElementNode \u0026\u0026 n.Data == \"meta\" {\r\n            var name, content string\r\n            for _, a := range n.Attr {\r\n                if a.Key == \"name\" {\r\n                    name = a.Val\r\n                } else if a.Key == \"content\" {\r\n                    content = a.Val\r\n                }\r\n            }\r\n            if name != \"\" \u0026\u0026 content != \"\" {\r\n                p.metadata[name] = content\r\n            }\r\n        }\r\n        for c := n.FirstChild; c != nil; c = c.NextSibling {\r\n            f(c)\r\n        }\r\n    }\r\n    f(doc)\r\n}",
    "size": 2378,
    "modTime": "2024-10-20T20:00:18.1246764+02:00",
    "path": "internal\\parser\\html.go"
  },
  {
    "name": "logger.go",
    "content": "// internal/parser/logger.go\r\n\r\npackage parser\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()\r\n",
    "size": 144,
    "modTime": "2024-10-20T15:59:02.1149669+02:00",
    "path": "internal\\parser\\logger.go"
  },
  {
    "name": "markdown.go",
    "content": "package parser\r\n\r\nimport (\r\n    \"io/ioutil\"\r\n    \"os\"\r\n    \"path/filepath\"\r\n\t\"fmt\"\r\n    \"time\"\r\n\r\n    \"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\nfunc init() {\r\n    RegisterParser(\".md\", NewMarkdownParser)\r\n}\r\n\r\n// MarkdownParser implémente l'interface Parser pour les fichiers Markdown\r\ntype MarkdownParser struct {\r\n    metadata map[string]string\r\n}\r\n\r\n// NewMarkdownParser crée une nouvelle instance de MarkdownParser\r\nfunc NewMarkdownParser() Parser {\r\n    return \u0026MarkdownParser{\r\n        metadata: make(map[string]string),\r\n    }\r\n}\r\n\r\n// Parse lit le contenu d'un fichier Markdown\r\nfunc (p *MarkdownParser) Parse(path string) ([]byte, error) {\r\n    log.Debug(i18n.Messages.ParseStarted, \"Markdown\", path)\r\n    content, err := ioutil.ReadFile(path)\r\n    if err != nil {\r\n        log.Error(i18n.Messages.ParseFailed, \"Markdown\", path, err)\r\n        return nil, err\r\n    }\r\n    p.extractMetadata(path)\r\n    log.Info(i18n.Messages.ParseCompleted, \"Markdown\", path)\r\n    return content, nil\r\n}\r\n\r\n// GetMetadata retourne les métadonnées du fichier Markdown\r\nfunc (p *MarkdownParser) GetMetadata() map[string]string {\r\n    return p.metadata\r\n}\r\n\r\n// extractMetadata extrait les métadonnées basiques du fichier\r\nfunc (p *MarkdownParser) extractMetadata(path string) {\r\n    info, err := os.Stat(path)\r\n    if err != nil {\r\n        log.Warning(i18n.Messages.MetadataExtractionFailed, path, err)\r\n        return\r\n    }\r\n    p.metadata[\"filename\"] = filepath.Base(path)\r\n    p.metadata[\"size\"] = fmt.Sprintf(\"%d\", info.Size())\r\n    p.metadata[\"modified\"] = info.ModTime().Format(time.RFC3339)\r\n}",
    "size": 1604,
    "modTime": "2024-10-20T20:00:25.3760497+02:00",
    "path": "internal\\parser\\markdown.go"
  },
  {
    "name": "parser.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n)\r\n\r\n// Parser définit l'interface pour tous les analyseurs de documents\r\ntype Parser interface {\r\n\t// Parse prend le chemin d'un fichier et retourne son contenu en bytes\r\n\tParse(path string) ([]byte, error)\r\n\t// GetMetadata retourne les métadonnées du document sous forme de map\r\n\tGetMetadata() map[string]string\r\n}\r\n\r\n// FormatParser est une fonction qui crée un Parser spécifique à un format\r\ntype FormatParser func() Parser\r\n\r\n// formatParsers stocke les fonctions de création de Parser pour chaque format supporté\r\nvar formatParsers = make(map[string]FormatParser)\r\n\r\n// RegisterParser enregistre un nouveau parser pour un format donné\r\nfunc RegisterParser(format string, parser FormatParser) {\r\n\tformatParsers[format] = parser\r\n}\r\n\r\n// GetParser retourne le parser approprié basé sur le format spécifié\r\nfunc GetParser(format string) (Parser, error) {\r\n\tparserFunc, ok := formatParsers[format]\r\n\tif !ok {\r\n\t\treturn nil, fmt.Errorf(\"unsupported format: %s\", format)\r\n\t}\r\n\treturn parserFunc(), nil\r\n}\r\n\r\n// ParseDirectory parcourt un répertoire et parse tous les fichiers supportés\r\nfunc ParseDirectory(path string, recursive bool) ([][]byte, error) {\r\n\tvar results [][]byte\r\n\terr := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {\r\n\t\tif err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t\tif info.IsDir() {\r\n\t\t\tif !recursive \u0026\u0026 filePath != path {\r\n\t\t\t\treturn filepath.SkipDir\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t}\r\n\t\text := strings.ToLower(filepath.Ext(filePath))\r\n\t\tparser, err := GetParser(ext)\r\n\t\tif err != nil {\r\n\t\t\treturn nil // Skip unsupported files\r\n\t\t}\r\n\t\tcontent, err := parser.Parse(filePath)\r\n\t\tif err != nil {\r\n\t\t\tlog.Warning(\"Failed to parse file: %s, error: %v\", filePath, err)\r\n\t\t\treturn nil\r\n\t\t}\r\n\t\tresults = append(results, content)\r\n\t\treturn nil\r\n\t})\r\n\treturn results, err\r\n}\r\n",
    "size": 1892,
    "modTime": "2024-10-20T16:02:54.2283253+02:00",
    "path": "internal\\parser\\parser.go"
  },
  {
    "name": "parser_test.go",
    "content": "package parser\r\n\r\nimport (\r\n    \"os\"\r\n    \"path/filepath\"\r\n    \"testing\"\r\n)\r\n\r\nfunc TestParsers(t *testing.T) {\r\n    testCases := []struct {\r\n        format string\r\n        content string\r\n    }{\r\n        {\".txt\", \"This is a test text file.\"},\r\n        {\".md\", \"# This is a Markdown file\\n\\nWith some content.\"},\r\n        {\".html\", \"\u003chtml\u003e\u003cbody\u003eThis is an HTML file\u003c/body\u003e\u003c/html\u003e\"},\r\n    }\r\n\r\n    for _, tc := range testCases {\r\n        t.Run(tc.format, func(t *testing.T) {\r\n            tempFile, err := createTempFile(tc.format, tc.content)\r\n            if err != nil {\r\n                t.Fatalf(\"Failed to create temp file: %v\", err)\r\n            }\r\n            defer os.Remove(tempFile)\r\n\r\n            parser, err := GetParser(tc.format)\r\n            if err != nil {\r\n                t.Fatalf(\"Failed to get parser: %v\", err)\r\n            }\r\n\r\n            content, err := parser.Parse(tempFile)\r\n            if err != nil {\r\n                t.Fatalf(\"Failed to parse file: %v\", err)\r\n            }\r\n\r\n            if string(content) != tc.content {\r\n                t.Errorf(\"Expected content %s, got %s\", tc.content, string(content))\r\n            }\r\n\r\n            metadata := parser.GetMetadata()\r\n            if len(metadata) == 0 {\r\n                t.Error(\"Expected non-empty metadata\")\r\n            }\r\n        })\r\n    }\r\n}\r\n\r\nfunc TestParseDirectory(t *testing.T) {\r\n    tempDir, err := os.MkdirTemp(\"\", \"parser_test\")\r\n    if err != nil {\r\n        t.Fatalf(\"Failed to create temp directory: %v\", err)\r\n    }\r\n    defer os.RemoveAll(tempDir)\r\n\r\n    files := []struct {\r\n        name string\r\n        content string\r\n    }{\r\n        {\"test1.txt\", \"Text file 1\"},\r\n        {\"test2.md\", \"# Markdown file\"},\r\n        {\"test3.html\", \"\u003chtml\u003e\u003cbody\u003eHTML file\u003c/body\u003e\u003c/html\u003e\"},\r\n    }\r\n\r\n    for _, f := range files {\r\n        err := os.WriteFile(filepath.Join(tempDir, f.name), []byte(f.content), 0644)\r\n        if err != nil {\r\n            t.Fatalf(\"Failed to create test file: %v\", err)\r\n        }\r\n    }\r\n\r\n    results, err := ParseDirectory(tempDir, false)\r\n    if err != nil {\r\n        t.Fatalf(\"ParseDirectory failed: %v\", err)\r\n    }\r\n\r\n    if len(results) != len(files) {\r\n        t.Errorf(\"Expected %d results, got %d\", len(files), len(results))\r\n    }\r\n}\r\n\r\nfunc createTempFile(ext, content string) (string, error) {\r\n    tmpfile, err := os.CreateTemp(\"\", \"test*\"+ext)\r\n    if err != nil {\r\n        return \"\", err\r\n    }\r\n    defer tmpfile.Close()\r\n\r\n    if _, err := tmpfile.Write([]byte(content)); err != nil {\r\n        return \"\", err\r\n    }\r\n\r\n    return tmpfile.Name(), nil\r\n}",
    "size": 2588,
    "modTime": "2024-10-20T12:32:19.3183805+02:00",
    "path": "internal\\parser\\parser_test.go"
  },
  {
    "name": "pdf.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"fmt\"\r\n\t\"io\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/ledongthuc/pdf\"\r\n)\r\n\r\nfunc init() {\r\n\tlog.Debug(\"Registering PDF parser\")\r\n\tRegisterParser(\".pdf\", NewPDFParser)\r\n}\r\n\r\ntype PDFParser struct {\r\n\tmetadata map[string]string\r\n}\r\n\r\nfunc NewPDFParser() Parser {\r\n\tlog.Debug(\"Creating new PDF parser\")\r\n\treturn \u0026PDFParser{\r\n\t\tmetadata: make(map[string]string),\r\n\t}\r\n}\r\n\r\nfunc (p *PDFParser) Parse(path string) ([]byte, error) {\r\n\tlog.Debug(\"Parsing PDF file: %s\", path)\r\n\tcontent, err := ParsePDF(path)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\tp.extractMetadata(path)\r\n\treturn content, nil\r\n}\r\n\r\nfunc (p *PDFParser) GetMetadata() map[string]string {\r\n\treturn p.metadata\r\n}\r\n\r\nfunc (p *PDFParser) extractMetadata(path string) {\r\n\tlog.Debug(\"Extracting metadata from PDF file: %s\", path)\r\n\tf, r, err := pdf.Open(path)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to open PDF: %v\", err)\r\n\t\treturn\r\n\t}\r\n\tdefer f.Close()\r\n\r\n\t// Extract basic information\r\n\tp.metadata[\"PageCount\"] = fmt.Sprintf(\"%d\", r.NumPage())\r\n\r\n\tlog.Debug(\"Extracted %d metadata items\", len(p.metadata))\r\n}\r\n\r\n// ParsePDF reads a PDF file and returns its content as a byte slice.\r\nfunc ParsePDF(path string) ([]byte, error) {\r\n\tlog.Debug(i18n.Messages.ParseStarted)\r\n\r\n\tf, r, err := pdf.Open(path)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.Messages.ParseFailed, err)\r\n\t\treturn nil, fmt.Errorf(\"failed to open PDF: %w\", err)\r\n\t}\r\n\tdefer f.Close()\r\n\r\n\tvar buf bytes.Buffer\r\n\tb, err := r.GetPlainText()\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to get plain text: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to get plain text: %w\", err)\r\n\t}\r\n\r\n\t_, err = io.Copy(\u0026buf, b)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to read content: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to read content: %w\", err)\r\n\t}\r\n\r\n\tcontent := buf.Bytes()\r\n\tlog.Info(i18n.Messages.ParseCompleted)\r\n\tlog.Debug(\"Total extracted content length: %d characters\", len(content))\r\n\treturn content, nil\r\n}\r\n",
    "size": 1980,
    "modTime": "2024-10-21T00:58:53.4312994+02:00",
    "path": "internal\\parser\\pdf.go"
  },
  {
    "name": "text.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\nfunc init() {\r\n\tRegisterParser(\".txt\", NewTextParser)\r\n}\r\n\r\n// TextParser implémente l'interface Parser pour les fichiers texte\r\ntype TextParser struct {\r\n\tmetadata map[string]string\r\n}\r\n\r\n// NewTextParser crée une nouvelle instance de TextParser\r\nfunc NewTextParser() Parser {\r\n\treturn \u0026TextParser{\r\n\t\tmetadata: make(map[string]string),\r\n\t}\r\n}\r\n\r\n// Parse lit le contenu d'un fichier texte\r\nfunc (p *TextParser) Parse(path string) ([]byte, error) {\r\n\tlog.Debug(i18n.Messages.ParseStarted, \"text\", path)\r\n\tcontent, err := ioutil.ReadFile(path)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.Messages.ParseFailed, \"text\", path, err)\r\n\t\treturn nil, err\r\n\t}\r\n\tp.extractMetadata(path)\r\n\tlog.Info(i18n.Messages.ParseCompleted, \"text\", path)\r\n\treturn content, nil\r\n}\r\n\r\n// GetMetadata retourne les métadonnées du fichier texte\r\nfunc (p *TextParser) GetMetadata() map[string]string {\r\n\treturn p.metadata\r\n}\r\n\r\n// extractMetadata extrait les métadonnées basiques du fichier\r\nfunc (p *TextParser) extractMetadata(path string) {\r\n\tinfo, err := os.Stat(path)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.Messages.MetadataExtractionFailed, path, err)\r\n\t\treturn\r\n\t}\r\n\tp.metadata[\"filename\"] = filepath.Base(path)\r\n\tp.metadata[\"size\"] = fmt.Sprintf(\"%d\", info.Size())\r\n\tp.metadata[\"modified\"] = info.ModTime().Format(time.RFC3339)\r\n}\r\n",
    "size": 1447,
    "modTime": "2024-10-20T20:00:34.5625128+02:00",
    "path": "internal\\parser\\text.go"
  },
  {
    "name": "context.go",
    "content": "package pipeline\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"regexp\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()\r\n\r\n// ContextEntry représente le contexte pour une position spécifique dans le document\r\ntype ContextEntry struct {\r\n\tPosition int      `json:\"position\"`\r\n\tBefore   []string `json:\"before\"`\r\n\tAfter    []string `json:\"after\"`\r\n\tElement  string   `json:\"element\"`\r\n\tLength   int      `json:\"length\"`\r\n}\r\n\r\n// GenerateContextJSON génère un JSON contenant le contexte pour chaque position donnée\r\nfunc GenerateContextJSON(content []byte, positions []int, contextWords int, positionRanges []PositionRange) (string, error) {\r\n\tlog.Debug(\"Starting GenerateContextJSON\")\r\n\tlog.Debug(\"Number of positions: %d, Context words: %d\", len(positions), contextWords)\r\n\r\n\twords := strings.Fields(string(content))\r\n\tlog.Debug(\"Total words in content: %d\", len(words))\r\n\r\n\tentries := make([]ContextEntry, 0, len(positions))\r\n\r\n\tfor i, pos := range positions {\r\n\t\tlog.Debug(\"Processing position: %d\", pos)\r\n\r\n\t\tif pos \u003c 0 || pos \u003e= len(words) {\r\n\t\t\tlog.Warning(\"Invalid position %d. Skipping.\", pos)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tstart := max(0, pos-contextWords)\r\n\t\tbefore := words[start:pos]\r\n\r\n\t\tend := min(len(words), pos+contextWords+1)\r\n\t\tvar after []string\r\n\t\tif pos+1 \u003c len(words) {\r\n\t\t\tafter = words[pos+1 : end]\r\n\t\t}\r\n\r\n\t\tentry := ContextEntry{\r\n\t\t\tPosition: pos,\r\n\t\t\tBefore:   before,\r\n\t\t\tAfter:    after,\r\n\t\t\tElement:  positionRanges[i].Element,\r\n\t\t\tLength:   positionRanges[i].End - positionRanges[i].Start + 1,\r\n\t\t}\r\n\r\n\t\tentries = append(entries, entry)\r\n\t}\r\n\r\n\tlog.Debug(\"Number of context entries generated: %d\", len(entries))\r\n\r\n\t// Utiliser un encoder JSON personnalisé\r\n\tvar buf bytes.Buffer\r\n\tencoder := json.NewEncoder(\u0026buf)\r\n\tencoder.SetEscapeHTML(false)\r\n\tencoder.SetIndent(\"\", \"  \") // Deux espaces pour l'indentation\r\n\r\n\tif err := encoder.Encode(entries); err != nil {\r\n\t\tlog.Error(\"Error marshaling context JSON: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error marshaling context JSON: %w\", err)\r\n\t}\r\n\r\n\t// Fonction pour remplacer les tableaux multi-lignes par des tableaux sur une seule ligne\r\n\treplaceArrays := func(match string) string {\r\n\t\tlines := strings.Split(match, \"\\n\")\r\n\t\tfor i, line := range lines {\r\n\t\t\tlines[i] = strings.TrimSpace(line)\r\n\t\t}\r\n\t\treturn strings.Join(lines, \"\")\r\n\t}\r\n\r\n\t// Appliquer le remplacement sur le JSON généré\r\n\toutput := regexp.MustCompile(`\\[[\\s\\S]*?\\]`).ReplaceAllStringFunc(buf.String(), replaceArrays)\r\n\r\n\tlog.Debug(\"JSON data generated successfully. Length: %d bytes\", len(output))\r\n\r\n\treturn output, nil\r\n}\r\n\r\n// max retourne le maximum entre deux entiers\r\nfunc max(a, b int) int {\r\n\tif a \u003e b {\r\n\t\treturn a\r\n\t}\r\n\treturn b\r\n}\r\n\r\n// min retourne le minimum entre deux entiers\r\nfunc min(a, b int) int {\r\n\tif a \u003c b {\r\n\t\treturn a\r\n\t}\r\n\treturn b\r\n}\r\n",
    "size": 2863,
    "modTime": "2024-10-24T14:31:50.580942+02:00",
    "path": "internal\\pipeline\\context.go"
  },
  {
    "name": "pipeline.go",
    "content": "package pipeline\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"sort\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"unicode\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/llm\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/chrlesur/Ontology/internal/metadata\"\r\n\t\"github.com/chrlesur/Ontology/internal/model\"\r\n\t\"github.com/chrlesur/Ontology/internal/parser\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n\t\"github.com/chrlesur/Ontology/internal/segmenter\"\r\n\r\n\t\"github.com/pkoukk/tiktoken-go\"\r\n)\r\n\r\ntype ProgressInfo struct {\r\n\tCurrentPass       int\r\n\tTotalPasses       int\r\n\tCurrentStep       string\r\n\tTotalSegments     int\r\n\tProcessedSegments int\r\n}\r\n\r\ntype PositionRange struct {\r\n\tStart   int\r\n\tEnd     int\r\n\tElement string\r\n}\r\n\r\ntype ProgressCallback func(ProgressInfo)\r\n\r\n// Pipeline represents the main processing pipeline\r\ntype Pipeline struct {\r\n\tconfig                   *config.Config\r\n\tlogger                   *logger.Logger\r\n\tllm                      llm.Client\r\n\tprogressCallback         ProgressCallback\r\n\tontology                 *model.Ontology\r\n\tincludePositions         bool\r\n\tcontextOutput            bool\r\n\tcontextWords             int\r\n\tinputPath                string\r\n\tentityExtractionPrompt   string\r\n\trelationExtractionPrompt string\r\n\tontologyEnrichmentPrompt string\r\n\tontologyMergePrompt      string\r\n}\r\n\r\n// NewPipeline creates a new instance of the processing pipeline\r\nfunc NewPipeline(includePositions bool, contextOutput bool, contextWords int, entityPrompt, relationPrompt, enrichmentPrompt, mergePrompt, llmType, llmModel string) (*Pipeline, error) {\r\n\tcfg := config.GetConfig()\r\n\tlog := logger.GetLogger()\r\n\r\n\t// Utilisez les valeurs de la ligne de commande si elles sont fournies, sinon utilisez les valeurs de la configuration\r\n\tselectedLLM := cfg.DefaultLLM\r\n\tselectedModel := cfg.DefaultModel\r\n\tif llmType != \"\" {\r\n\t\tselectedLLM = llmType\r\n\t}\r\n\tif llmModel != \"\" {\r\n\t\tselectedModel = llmModel\r\n\t}\r\n\r\n\tclient, err := llm.GetClient(selectedLLM, selectedModel)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrInitLLMClient\"), err)\r\n\t}\r\n\r\n\treturn \u0026Pipeline{\r\n\t\tconfig:                   cfg,\r\n\t\tlogger:                   log,\r\n\t\tllm:                      client,\r\n\t\tontology:                 model.NewOntology(),\r\n\t\tincludePositions:         includePositions,\r\n\t\tcontextOutput:            contextOutput,\r\n\t\tcontextWords:             contextWords,\r\n\t\tentityExtractionPrompt:   entityPrompt,\r\n\t\trelationExtractionPrompt: relationPrompt,\r\n\t\tontologyEnrichmentPrompt: enrichmentPrompt,\r\n\t\tontologyMergePrompt:      mergePrompt,\r\n\t}, nil\r\n}\r\n\r\n// SetProgressCallback sets the callback function for progress updates\r\nfunc (p *Pipeline) SetProgressCallback(callback ProgressCallback) {\r\n\tp.progressCallback = callback\r\n}\r\n\r\n// ExecutePipeline orchestrates the entire workflow\r\nfunc (p *Pipeline) ExecutePipeline(input string, output string, passes int, existingOntology string, ontology *model.Ontology) error {\r\n\tp.inputPath = input\r\n\tp.logger.Info(i18n.GetMessage(\"StartingPipeline\"))\r\n\tp.logger.Debug(\"Input: %s, Output: %s, Passes: %d, Existing Ontology: %s\", input, output, passes, existingOntology)\r\n\r\n\tvar result string\r\n\tvar err error\r\n\tvar finalContent []byte\r\n\r\n\t// Initialiser le tokenizer\r\n\ttke, err := tiktoken.GetEncoding(\"cl100k_base\")\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to initialize tokenizer: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to initialize tokenizer: %w\", err)\r\n\t}\r\n\r\n\tif existingOntology != \"\" {\r\n\t\tresult, err = p.loadExistingOntology(existingOntology)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Error(i18n.GetMessage(\"ErrLoadExistingOntology\"), err)\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrLoadExistingOntology\"), err)\r\n\t\t}\r\n\t\ttokenCount := len(tke.Encode(result, nil, nil))\r\n\t\tp.logger.Debug(\"Loaded existing ontology, token count: %d\", tokenCount)\r\n\t}\r\n\r\n\tfor i := 0; i \u003c passes; i++ {\r\n\t\tif p.progressCallback != nil {\r\n\t\t\tp.progressCallback(ProgressInfo{\r\n\t\t\t\tCurrentPass: i + 1,\r\n\t\t\t\tTotalPasses: passes,\r\n\t\t\t\tCurrentStep: \"Starting Pass\",\r\n\t\t\t})\r\n\t\t}\r\n\t\tinitialTokenCount := len(tke.Encode(result, nil, nil))\r\n\t\tp.logger.Info(\"Starting pass %d with initial result token count: %d\", i+1, initialTokenCount)\r\n\r\n\t\tresult, newContent, err := p.processSinglePass(input, result, p.includePositions)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Error(i18n.GetMessage(\"ErrProcessingPass\"), i+1, err)\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrProcessingPass\"), err)\r\n\t\t}\r\n\r\n\t\tfinalContent = newContent // Sauvegarder le contenu de la dernière passe\r\n\r\n\t\tnewTokenCount := len(tke.Encode(result, nil, nil))\r\n\t\tp.logger.Info(\"Completed pass %d, new result token count: %d\", i+1, newTokenCount)\r\n\t\tp.logger.Info(\"Token count change in pass %d: %d\", i+1, newTokenCount-initialTokenCount)\r\n\t}\r\n\terr = p.saveResult(result, output, finalContent)\r\n\tif err != nil {\r\n\t\tp.logger.Error(i18n.GetMessage(\"ErrSavingResult\"), err)\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrSavingResult\"), err)\r\n\t}\r\n\r\n\tp.logger.Info(\"Pipeline completed.\")\r\n\treturn nil\r\n}\r\n\r\n// Helper function to truncate long strings for logging\r\nfunc truncateString(s string, maxLength int) string {\r\n\tif len(s) \u003c= maxLength {\r\n\t\treturn s\r\n\t}\r\n\treturn s[:maxLength] + \"...\"\r\n}\r\n\r\nfunc (p *Pipeline) processSinglePass(input string, previousResult string, includePositions bool) (string, []byte, error) {\r\n\tp.logger.Debug(\"Starting processSinglePass with input length: %d, previous result length: %d\", len(input), len(previousResult))\r\n\r\n\t// Initialiser le tokenizer\r\n\ttke, err := tiktoken.GetEncoding(\"cl100k_base\")\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to initialize tokenizer: %v\", err)\r\n\t\treturn \"\", nil, fmt.Errorf(\"failed to initialize tokenizer: %w\", err)\r\n\t}\r\n\r\n\tinputTokens := len(tke.Encode(input, nil, nil))\r\n\tpreviousResultTokens := len(tke.Encode(previousResult, nil, nil))\r\n\tp.logger.Info(\"Processing single pass, input tokens: %d, previous result tokens: %d\", inputTokens, previousResultTokens)\r\n\r\n\tcontent, err := p.parseInput(input)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to parse input: %v\", err)\r\n\t\treturn \"\", nil, err\r\n\t}\r\n\tp.logger.Debug(\"Parsed input content length: %d bytes\", len(content))\r\n\r\n\tpositionIndex := p.createPositionIndex(content)\r\n\tp.logger.Debug(\"Position index created. Number of entries: %d\", len(positionIndex))\r\n\r\n\tcontentTokens := len(tke.Encode(string(content), nil, nil))\r\n\tp.logger.Info(\"Parsed content tokens: %d\", contentTokens)\r\n\r\n\tsegments, err := segmenter.Segment(content, segmenter.SegmentConfig{\r\n\t\tMaxTokens:   p.config.MaxTokens,\r\n\t\tContextSize: p.config.ContextSize,\r\n\t\tModel:       p.config.DefaultModel,\r\n\t})\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to segment content: %v\", err)\r\n\t\treturn \"\", nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrSegmentContent\"), err)\r\n\t}\r\n\tp.logger.Info(\"Number of segments: %d\", len(segments))\r\n\r\n\tif p.progressCallback != nil {\r\n\t\tp.progressCallback(ProgressInfo{\r\n\t\t\tCurrentStep:   \"Segmenting\",\r\n\t\t\tTotalSegments: len(segments),\r\n\t\t})\r\n\t}\r\n\r\n\tresults := make([]string, len(segments))\r\n\tvar wg sync.WaitGroup\r\n\tsem := make(chan struct{}, 5) // Sémaphore pour limiter à 5 goroutines concurrentes\r\n\r\n\tfor i, segment := range segments {\r\n\t\twg.Add(1)\r\n\t\tgo func(i int, seg segmenter.SegmentInfo) {\r\n\t\t\tdefer wg.Done()\r\n\r\n\t\t\t// Acquérir une place dans le sémaphore\r\n\t\t\tsem \u003c- struct{}{}\r\n\t\t\tdefer func() { \u003c-sem }() // Libérer la place à la fin\r\n\r\n\t\t\tsegmentTokens := len(tke.Encode(string(seg.Content), nil, nil))\r\n\t\t\tp.logger.Debug(\"Processing segment %d/%d, Start: %d, End: %d, Length: %d bytes, Tokens: %d, Preview: %s\",\r\n\t\t\t\ti+1, len(segments), seg.Start, seg.End, len(seg.Content), segmentTokens,\r\n\t\t\t\ttruncateString(string(seg.Content), 100))\r\n\r\n\t\t\tcontext := segmenter.GetContext(segments, i, segmenter.SegmentConfig{\r\n\t\t\t\tMaxTokens:   p.config.MaxTokens,\r\n\t\t\t\tContextSize: p.config.ContextSize,\r\n\t\t\t\tModel:       p.config.DefaultModel,\r\n\t\t\t})\r\n\t\t\tp.logger.Debug(\"Context for segment %d/%d, Length: %d bytes, Preview: %s\",\r\n\t\t\t\ti+1, len(segments), len(context), truncateString(context, 100))\r\n\r\n\t\t\tresult, err := p.processSegment(seg.Content, context, previousResult, positionIndex, includePositions)\r\n\t\t\tif err != nil {\r\n\t\t\t\tp.logger.Error(i18n.GetMessage(\"SegmentProcessingError\"), i+1, err)\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t\tresultTokens := len(tke.Encode(result, nil, nil))\r\n\t\t\tresults[i] = result\r\n\t\t\tp.logger.Info(\"Segment %d processed successfully, result tokens: %d\", i+1, resultTokens)\r\n\t\t\tif p.progressCallback != nil {\r\n\t\t\t\tp.progressCallback(ProgressInfo{\r\n\t\t\t\t\tCurrentStep:       \"Processing Segment\",\r\n\t\t\t\t\tProcessedSegments: i + 1,\r\n\t\t\t\t\tTotalSegments:     len(segments),\r\n\t\t\t\t})\r\n\t\t\t}\r\n\t\t}(i, segment)\r\n\t}\r\n\twg.Wait()\r\n\r\n\t// Fusionner les résultats de tous les segments\r\n\tmergedResult, err := p.mergeResults(previousResult, results)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to merge segment results: %v\", err)\r\n\t\treturn \"\", nil, fmt.Errorf(\"failed to merge segment results: %w\", err)\r\n\t}\r\n\r\n\tmergedResultTokens := len(tke.Encode(mergedResult, nil, nil))\r\n\tp.logger.Info(\"Merged result tokens: %d\", mergedResultTokens)\r\n\tp.logger.Debug(\"processSinglePass completed. Merged result length: %d\", len(mergedResult))\r\n\treturn mergedResult, content, nil\r\n}\r\n\r\nfunc (p *Pipeline) parseInput(input string) ([]byte, error) {\r\n\tinfo, err := os.Stat(input)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Error accessing input file: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrAccessInput\"), err)\r\n\t}\r\n\r\n\tp.logger.Debug(\"File info: Size: %d, ModTime: %s\", info.Size(), info.ModTime())\r\n\r\n\tif info.IsDir() {\r\n\t\treturn p.parseDirectory(input)\r\n\t}\r\n\r\n\text := filepath.Ext(input)\r\n\tparser, err := parser.GetParser(ext)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Error getting parser for extension %s: %v\", ext, err)\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrUnsupportedFormat\"), err)\r\n\t}\r\n\r\n\tcontent, err := parser.Parse(input)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Error parsing file: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrParseFile\"), err)\r\n\t}\r\n\r\n\treturn content, nil\r\n}\r\n\r\nfunc (p *Pipeline) parseDirectory(dir string) ([]byte, error) {\r\n\tvar content []byte\r\n\terr := filepath.Walk(dir, func(path string, info os.FileInfo, err error) error {\r\n\t\tif err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t\tif !info.IsDir() {\r\n\t\t\text := filepath.Ext(path)\r\n\t\t\tparser, err := parser.GetParser(ext)\r\n\t\t\tif err != nil {\r\n\t\t\t\tp.logger.Warning(i18n.GetMessage(\"ErrUnsupportedFormat\"), path)\r\n\t\t\t\treturn nil\r\n\t\t\t}\r\n\t\t\tfileContent, err := parser.Parse(path)\r\n\t\t\tif err != nil {\r\n\t\t\t\tp.logger.Warning(i18n.GetMessage(\"ErrParseFile\"), path, err)\r\n\t\t\t\treturn nil\r\n\t\t\t}\r\n\t\t\tcontent = append(content, fileContent...)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrParseDirectory\"), err)\r\n\t}\r\n\treturn content, nil\r\n}\r\n\r\nfunc (p *Pipeline) processSegment(segment []byte, context string, previousResult string, positionIndex map[string][]int, includePositions bool) (string, error) {\r\n\tp.logger.Debug(\"Processing segment of length %d, context length %d, previous result length %d\", len(segment), len(context), len(previousResult))\r\n\tp.logger.Debug(\"Segment content: %s\", truncateString(string(segment), 200))\r\n\tp.logger.Debug(\"Context preview: %s\", truncateString(context, 200))\r\n\r\n\tenrichmentValues := map[string]string{\r\n\t\t\"text\":              string(segment),\r\n\t\t\"context\":           context,\r\n\t\t\"previous_result\":   previousResult,\r\n\t\t\"additional_prompt\": p.ontologyEnrichmentPrompt,\r\n\t}\r\n\r\n\tenrichedResult, err := p.llm.ProcessWithPrompt(prompt.OntologyEnrichmentPrompt, enrichmentValues)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"ontology enrichment failed: %w\", err)\r\n\t}\r\n\r\n\t// Normaliser le résultat enrichi\r\n\tnormalizedResult := normalizeTSV(enrichedResult)\r\n\r\n\tp.logger.Debug(\"Enriched result length: %d, preview: %s\", len(normalizedResult), truncateString(normalizedResult, 100))\r\n\r\n\tp.enrichOntologyWithPositions(normalizedResult, positionIndex, includePositions, string(segment))\r\n\r\n\treturn normalizedResult, nil\r\n}\r\n\r\nfunc (p *Pipeline) mergeResults(previousResult string, newResults []string) (string, error) {\r\n\tp.logger.Debug(\"Starting mergeResults. Previous result length: %d, Number of new results: %d\", len(previousResult), len(newResults))\r\n\r\n\t// Combiner tous les nouveaux résultats\r\n\tcombinedNewResults := strings.Join(newResults, \"\\n\")\r\n\tp.logger.Debug(\"Combined new results length: %d\", len(combinedNewResults))\r\n\r\n\t// Préparer les valeurs pour le prompt de fusion\r\n\tmergeValues := map[string]string{\r\n\t\t\"previous_ontology\": previousResult,\r\n\t\t\"new_ontology\":      combinedNewResults,\r\n\t\t\"additional_prompt\": p.ontologyMergePrompt,\r\n\t}\r\n\r\n\t// Utiliser le LLM pour fusionner les résultats\r\n\tmergedResult, err := p.llm.ProcessWithPrompt(prompt.OntologyMergePrompt, mergeValues)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"ontology merge failed: %w\", err)\r\n\t}\r\n\r\n\t// Normaliser le résultat fusionné\r\n\tnormalizedMergedResult := normalizeTSV(mergedResult)\r\n\r\n\tp.logger.Debug(\"Merged result length: %d, preview: %s\", len(normalizedMergedResult), truncateString(normalizedMergedResult, 100))\r\n\r\n\treturn normalizedMergedResult, nil\r\n}\r\n\r\nfunc (p *Pipeline) loadExistingOntology(path string) (string, error) {\r\n\tcontent, err := ioutil.ReadFile(path)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrReadExistingOntology\"), err)\r\n\t}\r\n\treturn string(content), nil\r\n}\r\n\r\n// Sauvegarde des resultats\r\nfunc (p *Pipeline) saveResult(result string, outputPath string, newContent []byte) error {\r\n\tp.logger.Debug(\"Starting saveResult\")\r\n\tp.logger.Debug(\"Number of elements in ontology: %d\", len(p.ontology.Elements))\r\n\tp.logger.Debug(\"Number of relations in ontology: %d\", len(p.ontology.Relations))\r\n\r\n\tvar quickStatements strings.Builder\r\n\r\n\t// Écrire les éléments\r\n\tp.logger.Debug(\"Writing elements:\")\r\n\tfor _, element := range p.ontology.Elements {\r\n\t\tp.logger.Debug(\"Raw positions for %s: %v\", element.Name, element.Positions)\r\n\t\tpositions := strings.Trim(strings.Join(strings.Fields(fmt.Sprint(element.Positions)), \",\"), \"[]\")\r\n\t\tif line := strings.TrimSpace(fmt.Sprintf(\"%s\\t%s\\t%s\\t%s\", element.Name, element.Type, element.Description, positions)); line != \"\" {\r\n\t\t\tquickStatements.WriteString(line + \"\\n\")\r\n\t\t\tp.logger.Debug(\"Element: %s\", line)\r\n\t\t} else {\r\n\t\t\tp.logger.Warning(\"Skipping empty element: %v\", element)\r\n\t\t}\r\n\t}\r\n\r\n\t// Écrire les relations\r\n\tp.logger.Debug(\"Writing relations:\")\r\n\tfor _, relation := range p.ontology.Relations {\r\n\t\tif line := strings.TrimSpace(fmt.Sprintf(\"%s\\t%s\\t%s\\t%s\", relation.Source, relation.Type, relation.Target, relation.Description)); line != \"\" {\r\n\t\t\tquickStatements.WriteString(line + \"\\n\")\r\n\t\t\tp.logger.Debug(\"Relation: %s\", line)\r\n\t\t} else {\r\n\t\t\tp.logger.Warning(\"Skipping empty relation: %v\", relation)\r\n\t\t}\r\n\t}\r\n\r\n\tqs := quickStatements.String()\r\n\tp.logger.Debug(\"Full TSV content:\\n%s\", qs)\r\n\r\n\tdir := filepath.Dir(outputPath)\r\n\tbaseName := filepath.Base(outputPath)\r\n\text := filepath.Ext(baseName)\r\n\tnameWithoutExt := strings.TrimSuffix(baseName, ext)\r\n\r\n\t// Sauvegarder le fichier TSV\r\n\ttsvPath := filepath.Join(dir, nameWithoutExt+\".tsv\")\r\n\terr := ioutil.WriteFile(tsvPath, []byte(qs), 0644)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to write TSV file: %v\", err)\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrWriteOutput\"), err)\r\n\t}\r\n\tp.logger.Debug(\"TSV file written: %s\", tsvPath)\r\n\r\n\t// Générer et sauvegarder le JSON de contexte si l'option est activée\r\n\tif p.contextOutput {\r\n\t\tp.logger.Info(\"Context output is enabled. Generating context JSON.\")\r\n\t\twords := strings.Fields(string(newContent))\r\n\t\tp.logger.Debug(\"Total words in new content: %d\", len(words))\r\n\r\n\t\tpositionRanges := p.getAllPositionsFromNewContent(words)\r\n\t\tp.logger.Info(\"Total position ranges collected: %d\", len(positionRanges))\r\n\r\n\t\t// Vérification supplémentaire pour s'assurer que toutes les positions sont incluses\r\n\t\tfor _, element := range p.ontology.Elements {\r\n\t\t\tfor _, pos := range element.Positions {\r\n\t\t\t\tfound := false\r\n\t\t\t\tfor _, pr := range positionRanges {\r\n\t\t\t\t\tif pr.Start \u003c= pos \u0026\u0026 pos \u003c= pr.End {\r\n\t\t\t\t\t\tfound = true\r\n\t\t\t\t\t\tbreak\r\n\t\t\t\t\t}\r\n\t\t\t\t}\r\n\t\t\t\tif !found {\r\n\t\t\t\t\tpositionRanges = append(positionRanges, PositionRange{\r\n\t\t\t\t\t\tStart:   pos,\r\n\t\t\t\t\t\tEnd:     pos + len(strings.Fields(element.Name)) - 1,\r\n\t\t\t\t\t\tElement: element.Name,\r\n\t\t\t\t\t})\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tmergedPositions := mergeOverlappingPositions(positionRanges)\r\n\t\tp.logger.Info(\"Merged position ranges: %d\", len(mergedPositions))\r\n\r\n\t\t// Convertir les PositionRange en positions simples pour GenerateContextJSON\r\n\t\tvalidPositions := make([]int, len(mergedPositions))\r\n\t\tfor i, pr := range mergedPositions {\r\n\t\t\tvalidPositions[i] = pr.Start\r\n\t\t}\r\n\r\n\t\tcontextJSON, err := GenerateContextJSON(newContent, validPositions, p.contextWords, mergedPositions)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Error(\"Failed to generate context JSON: %v\", err)\r\n\t\t\treturn fmt.Errorf(\"failed to generate context JSON: %w\", err)\r\n\t\t}\r\n\t\tp.logger.Debug(\"Context JSON generated successfully. Length: %d bytes\", len(contextJSON))\r\n\r\n\t\tcontextFile := filepath.Join(dir, nameWithoutExt+\"_context.json\")\r\n\t\tif err := ioutil.WriteFile(contextFile, []byte(contextJSON), 0644); err != nil {\r\n\t\t\tp.logger.Error(\"Failed to write context JSON file: %v\", err)\r\n\t\t\treturn fmt.Errorf(\"failed to write context JSON file: %w\", err)\r\n\t\t}\r\n\t\tp.logger.Info(\"Context JSON saved to: %s\", contextFile)\r\n\t} else {\r\n\t\tp.logger.Debug(\"Context output is disabled. Skipping context JSON generation.\")\r\n\t}\r\n\tp.logger.Debug(\"Elements with positions:\")\r\n\tfor _, element := range p.ontology.Elements {\r\n\t\tp.logger.Debug(\"Element: %s, Type: %s, Positions: %v\", element.Name, element.Type, element.Positions)\r\n\t}\r\n\r\n\t// Utiliser = au lieu de := pour les variables déjà déclarées\r\n\tdir = filepath.Dir(outputPath)\r\n\tbaseName = filepath.Base(outputPath)\r\n\text = filepath.Ext(baseName)\r\n\tnameWithoutExt = strings.TrimSuffix(baseName, ext)\r\n\r\n\t// Créer et sauvegarder les métadonnées\r\n\tmetadataGen := metadata.NewGenerator()\r\n\r\n\t// Chemin du fichier d'ontologie\r\n\tontologyFile := filepath.Base(outputPath)\r\n\r\n\t// Chemin du fichier de contexte (si activé)\r\n\tvar contextFile string\r\n\tif p.contextOutput {\r\n\t\tcontextFile = nameWithoutExt + \"_context.json\"\r\n\t}\r\n\r\n\t// Générer les métadonnées\r\n\tmeta, err := metadataGen.GenerateMetadata(p.inputPath, ontologyFile, contextFile)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to generate metadata: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to generate metadata: %w\", err)\r\n\t}\r\n\r\n\t// Sauvegarder les métadonnées\r\n\tmetaFilePath := filepath.Join(dir, metadataGen.GetMetadataFilename(p.inputPath))\r\n\tif err := metadataGen.SaveMetadata(meta, metaFilePath); err != nil {\r\n\t\tp.logger.Error(\"Failed to save metadata: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to save metadata: %w\", err)\r\n\t}\r\n\r\n\tp.logger.Info(\"Metadata saved to: %s\", metaFilePath)\r\n\r\n\tp.logger.Debug(\"Finished saveResult\")\r\n\treturn nil\r\n}\r\n\r\nfunc (p *Pipeline) createPositionIndex(content []byte) map[string][]int {\r\n\tp.logger.Debug(\"Starting createPositionIndex\")\r\n\tindex := make(map[string][]int)\r\n\twords := bytes.Fields(content)\r\n\r\n\tfor i, word := range words {\r\n\t\tvariants := generateArticleVariants(string(word))\r\n\t\tfor _, variant := range variants {\r\n\t\t\tnormalizedVariant := normalizeWord(variant)\r\n\t\t\tindex[normalizedVariant] = append(index[normalizedVariant], i)\r\n\t\t}\r\n\t}\r\n\r\n\t// Indexer les paires et triplets de mots\r\n\tfor i := 0; i \u003c len(words)-1; i++ {\r\n\t\tpair := normalizeWord(string(words[i])) + \" \" + normalizeWord(string(words[i+1]))\r\n\t\tindex[pair] = append(index[pair], i)\r\n\r\n\t\tif i \u003c len(words)-2 {\r\n\t\t\ttriplet := pair + \" \" + normalizeWord(string(words[i+2]))\r\n\t\t\tindex[triplet] = append(index[triplet], i)\r\n\t\t}\r\n\t}\r\n\r\n\tp.logger.Debug(\"Finished createPositionIndex. Total indexed terms: %d\", len(index))\r\n\treturn index\r\n}\r\n\r\nfunc (p *Pipeline) enrichOntologyWithPositions(enrichedResult string, positionIndex map[string][]int, includePositions bool, content string) {\r\n\tp.logger.Debug(\"Starting enrichOntologyWithPositions\")\r\n\tp.logger.Debug(\"Include positions: %v\", includePositions)\r\n\tp.logger.Debug(\"Position index size: %d\", len(positionIndex))\r\n\r\n\tlines := strings.Split(enrichedResult, \"\\n\")\r\n\tp.logger.Debug(\"Number of lines to process: %d\", len(lines))\r\n\r\n\tfor i, line := range lines {\r\n\t\tp.logger.Debug(\"Processing line %d: %s\", i, line)\r\n\t\tparts := strings.Fields(line)\r\n\t\tif len(parts) \u003e= 3 { // Vérifie s'il y a au moins 3 parties\r\n\t\t\tname := parts[0]\r\n\t\t\telementType := parts[1]\r\n\t\t\tdescription := strings.Join(parts[2:], \" \")\r\n\r\n\t\t\telement := p.ontology.GetElementByName(name)\r\n\t\t\tif element == nil {\r\n\t\t\t\telement = model.NewOntologyElement(name, elementType)\r\n\t\t\t\tp.ontology.AddElement(element)\r\n\t\t\t\tp.logger.Debug(\"Added new element: %v\", element)\r\n\t\t\t} else {\r\n\t\t\t\tp.logger.Debug(\"Updated existing element: %v\", element)\r\n\t\t\t}\r\n\t\t\telement.Description = description\r\n\r\n\t\t\tif includePositions {\r\n\t\t\t\tp.logger.Debug(\"Searching for positions of entity: %s\", name)\r\n\t\t\t\tallPositions := p.findPositions(name, positionIndex, content)\r\n\t\t\t\tp.logger.Debug(\"Found %d positions for entity %s: %v\", len(allPositions), name, allPositions)\r\n\t\t\t\tif len(allPositions) \u003e 0 {\r\n\t\t\t\t\tuniquePos := uniquePositions(allPositions)\r\n\t\t\t\t\telement.SetPositions(uniquePos)\r\n\t\t\t\t\tp.logger.Debug(\"Set %d unique positions for element %s: %v\", len(uniquePos), name, uniquePos)\r\n\t\t\t\t} else {\r\n\t\t\t\t\tp.logger.Debug(\"No positions found for element %s\", name)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\r\n\t\t\tif len(parts) \u003e= 4 { // C'est une relation\r\n\t\t\t\tp.logger.Debug(\"Processing relation: %v\", parts)\r\n\t\t\t\tsource := parts[0]\r\n\t\t\t\trelationType := parts[1]\r\n\t\t\t\ttarget := parts[2]\r\n\t\t\t\trelationDescription := strings.Join(parts[3:], \" \")\r\n\t\t\t\trelation := \u0026model.Relation{\r\n\t\t\t\t\tSource:      source,\r\n\t\t\t\t\tType:        relationType,\r\n\t\t\t\t\tTarget:      target,\r\n\t\t\t\t\tDescription: relationDescription,\r\n\t\t\t\t}\r\n\t\t\t\tp.ontology.AddRelation(relation)\r\n\t\t\t\tp.logger.Info(\"Added new relation: %v\", relation)\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\tp.logger.Debug(\"Skipping invalid line: %s\", line)\r\n\t\t}\r\n\t}\r\n\r\n\tp.logger.Debug(\"Ontology after enrichment:\")\r\n\tfor _, element := range p.ontology.Elements {\r\n\t\tp.logger.Debug(\"Element: %s, Type: %s, Description: %s, Positions: %v\",\r\n\t\t\telement.Name, element.Type, element.Description, element.Positions)\r\n\t}\r\n\tp.logger.Debug(\"Finished enrichOntologyWithPositions\")\r\n\tp.logger.Debug(\"Final ontology state - Elements: %d, Relations: %d\",\r\n\t\tlen(p.ontology.Elements), len(p.ontology.Relations))\r\n}\r\n\r\nfunc uniquePositions(positions []int) []int {\r\n\tkeys := make(map[int]bool)\r\n\tlist := []int{}\r\n\tfor _, entry := range positions {\r\n\t\tif _, value := keys[entry]; !value {\r\n\t\t\tkeys[entry] = true\r\n\t\t\tlist = append(list, entry)\r\n\t\t}\r\n\t}\r\n\treturn list\r\n}\r\n\r\nfunc (p *Pipeline) findPositions(word string, index map[string][]int, content string) []int {\r\n\tparts := strings.Split(word, \"\\t\")\r\n\tentityName := parts[0]\r\n\r\n\tp.logger.Debug(\"Searching for positions of entity: %s\", entityName)\r\n\r\n\tallVariants := generateArticleVariants(entityName)\r\n\tvar allPositions []int\r\n\r\n\tfor _, variant := range allVariants {\r\n\t\tnormalizedVariant := normalizeWord(variant)\r\n\t\tp.logger.Debug(\"Trying variant: %s\", normalizedVariant)\r\n\r\n\t\t// Recherche exacte d'abord\r\n\t\tif positions, ok := index[normalizedVariant]; ok {\r\n\t\t\tp.logger.Debug(\"Found exact match positions for %s: %v\", variant, positions)\r\n\t\t\tallPositions = append(allPositions, positions...)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\t// Utiliser la recherche approximative seulement pour la variante originale\r\n\t\tif variant == entityName {\r\n\t\t\tpositions := p.findApproximatePositions(normalizedVariant, content)\r\n\t\t\tif len(positions) \u003e 0 {\r\n\t\t\t\tp.logger.Debug(\"Found approximate positions for %s: %v\", variant, positions)\r\n\t\t\t\tallPositions = append(allPositions, positions...)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Dédupliquer et trier les positions trouvées\r\n\tuniquePositions := uniqueIntSlice(allPositions)\r\n\tsort.Ints(uniquePositions)\r\n\r\n\tif len(uniquePositions) \u003e 0 {\r\n\t\tp.logger.Debug(\"Found total unique positions for %s: %v\", entityName, uniquePositions)\r\n\t} else {\r\n\t\tp.logger.Debug(\"No positions found for entity: %s\", entityName)\r\n\t}\r\n\r\n\treturn uniquePositions\r\n}\r\n\r\nfunc (p *Pipeline) findSequentialPositions(words []string, index map[string][]int) []int {\r\n\tvar positions []int\r\n\tif firstWordPositions, ok := index[words[0]]; ok {\r\n\t\tfor _, startPos := range firstWordPositions {\r\n\t\t\tmatch := true\r\n\t\t\tfor i, word := range words[1:] {\r\n\t\t\t\texpectedPos := startPos + i + 1\r\n\t\t\t\tif wordPositions, ok := index[word]; !ok || !contains(wordPositions, expectedPos) {\r\n\t\t\t\t\tmatch = false\r\n\t\t\t\t\tbreak\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif match {\r\n\t\t\t\tpositions = append(positions, startPos)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn positions\r\n}\r\n\r\nfunc (p *Pipeline) findApproximatePositions(entityName string, content string) []int {\r\n\twords := strings.Fields(strings.ToLower(entityName))\r\n\tcontentLower := strings.ToLower(content)\r\n\tvar positions []int\r\n\r\n\t// Recherche exacte de la phrase complète\r\n\tif index := strings.Index(contentLower, strings.Join(words, \" \")); index != -1 {\r\n\t\tpositions = append(positions, index)\r\n\t\tp.logger.Debug(\"Found exact match for %s at position %d\", entityName, index)\r\n\t\treturn positions\r\n\t}\r\n\r\n\t// Recherche approximative\r\n\tcontentWords := strings.Fields(contentLower)\r\n\tmaxDistance := 5 // Nombre maximum de mots entre les termes recherchés\r\n\r\n\tfor i := 0; i \u003c len(contentWords); i++ {\r\n\t\tif matchFound, endPos := p.checkApproximateMatch(words, contentWords[i:], maxDistance); matchFound {\r\n\t\t\tpositions = append(positions, i)\r\n\t\t\tmatchedPhrase := strings.Join(contentWords[i:i+endPos+1], \" \")\r\n\t\t\tp.logger.Debug(\"Found approximate match for %s at position %d: %s\", entityName, i, matchedPhrase)\r\n\t\t}\r\n\t}\r\n\r\n\treturn positions\r\n}\r\n\r\nfunc (p *Pipeline) checkApproximateMatch(searchWords, contentWords []string, maxDistance int) (bool, int) {\r\n\twordIndex := 0\r\n\tdistanceCount := 0\r\n\tfor i, word := range contentWords {\r\n\t\tif strings.Contains(word, searchWords[wordIndex]) {\r\n\t\t\twordIndex++\r\n\t\t\tdistanceCount = 0\r\n\t\t\tif wordIndex == len(searchWords) {\r\n\t\t\t\treturn true, i\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\tdistanceCount++\r\n\t\t\tif distanceCount \u003e maxDistance {\r\n\t\t\t\treturn false, -1\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\treturn false, -1\r\n}\r\n\r\n// getAllPositions retourne toutes les positions des éléments de l'ontologie\r\nfunc (p *Pipeline) getAllPositions() []int {\r\n\tvar allPositions []int\r\n\tfor _, element := range p.ontology.Elements {\r\n\t\tallPositions = append(allPositions, element.Positions...)\r\n\t}\r\n\tp.logger.Debug(\"Total positions collected: %d\", len(allPositions))\r\n\treturn allPositions\r\n}\r\n\r\nfunc (p *Pipeline) getAllPositionsFromNewContent(words []string) []PositionRange {\r\n\tvar allPositions []PositionRange\r\n\tfor _, element := range p.ontology.Elements {\r\n\t\telementWords := strings.Fields(strings.ToLower(element.Name))\r\n\t\t// Utiliser les positions déjà connues dans l'ontologie\r\n\t\tfor _, pos := range element.Positions {\r\n\t\t\tif pos \u003e= 0 \u0026\u0026 pos \u003c len(words) {\r\n\t\t\t\tend := min(pos+len(elementWords), len(words)) - 1\r\n\t\t\t\tallPositions = append(allPositions, PositionRange{\r\n\t\t\t\t\tStart:   pos,\r\n\t\t\t\t\tEnd:     end,\r\n\t\t\t\t\tElement: element.Name,\r\n\t\t\t\t})\r\n\t\t\t}\r\n\t\t}\r\n\t\t// Rechercher également de nouvelles occurrences\r\n\t\tfor i := 0; i \u003c= len(words)-len(elementWords); i++ {\r\n\t\t\tmatch := true\r\n\t\t\tfor j, ew := range elementWords {\r\n\t\t\t\tif strings.ToLower(words[i+j]) != ew {\r\n\t\t\t\t\tmatch = false\r\n\t\t\t\t\tbreak\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif match {\r\n\t\t\t\tallPositions = append(allPositions, PositionRange{\r\n\t\t\t\t\tStart:   i,\r\n\t\t\t\t\tEnd:     i + len(elementWords) - 1,\r\n\t\t\t\t\tElement: element.Name,\r\n\t\t\t\t})\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\tp.logger.Debug(\"Total position ranges collected from new content: %d\", len(allPositions))\r\n\treturn allPositions\r\n}\r\n\r\nfunc mergeOverlappingPositions(positions []PositionRange) []PositionRange {\r\n\tif len(positions) == 0 {\r\n\t\treturn positions\r\n\t}\r\n\r\n\tsort.Slice(positions, func(i, j int) bool {\r\n\t\treturn positions[i].Start \u003c positions[j].Start\r\n\t})\r\n\r\n\tmerged := []PositionRange{positions[0]}\r\n\r\n\tfor _, current := range positions[1:] {\r\n\t\tlast := \u0026merged[len(merged)-1]\r\n\t\tif current.Start \u003c= last.End+1 {\r\n\t\t\tif current.End \u003e last.End {\r\n\t\t\t\tlast.End = current.End\r\n\t\t\t}\r\n\t\t\tif len(current.Element) \u003e len(last.Element) {\r\n\t\t\t\tlast.Element = current.Element\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\tmerged = append(merged, current)\r\n\t\t}\r\n\t}\r\n\r\n\treturn merged\r\n}\r\n\r\nfunc contains(slice []int, val int) bool {\r\n\tfor _, item := range slice {\r\n\t\tif item == val {\r\n\t\t\treturn true\r\n\t\t}\r\n\t}\r\n\treturn false\r\n}\r\n\r\nfunc normalizeWord(word string) string {\r\n\t// Conserver les apostrophes\r\n\treturn strings.Map(func(r rune) rune {\r\n\t\tif unicode.IsLetter(r) || unicode.IsNumber(r) || r == '\\'' {\r\n\t\t\treturn unicode.ToLower(r)\r\n\t\t}\r\n\t\treturn ' '\r\n\t}, word)\r\n}\r\n\r\nfunc normalizeTSV(input string) string {\r\n\tlines := strings.Split(input, \"\\n\")\r\n\tvar normalizedLines []string\r\n\tfor _, line := range lines {\r\n\t\t// Remplacer les séquences de \"\\t\" par un seul espace\r\n\t\tline = strings.ReplaceAll(line, \"\\\\t\", \" \")\r\n\t\t// Remplacer les tabulations réelles par un espace\r\n\t\tline = strings.ReplaceAll(line, \"\\t\", \" \")\r\n\t\t// Diviser la ligne en champs en utilisant un ou plusieurs espaces comme séparateur\r\n\t\tfields := strings.Fields(line)\r\n\t\tif len(fields) \u003e= 3 {\r\n\t\t\t// Reconstruire la ligne TSV avec des tabulations\r\n\t\t\tnormalizedLine := strings.Join(fields[:2], \"\\t\") + \"\\t\" + strings.Join(fields[2:], \" \")\r\n\t\t\tnormalizedLines = append(normalizedLines, normalizedLine)\r\n\t\t}\r\n\t}\r\n\treturn strings.Join(normalizedLines, \"\\n\")\r\n}\r\n\r\nfunc generateArticleVariants(word string) []string {\r\n\tvariants := []string{word}\r\n\tlowercaseWord := strings.ToLower(word)\r\n\r\n\t// Ajouter des variantes avec et sans apostrophe\r\n\tif !strings.HasPrefix(lowercaseWord, \"l'\") \u0026\u0026 !strings.HasPrefix(lowercaseWord, \"d'\") {\r\n\t\tvariants = append(variants, \"l'\"+lowercaseWord, \"d'\"+lowercaseWord, \"l \"+lowercaseWord, \"d \"+lowercaseWord)\r\n\t}\r\n\r\n\t// Ajouter une variante sans underscore si le mot en contient\r\n\tif strings.Contains(word, \"_\") {\r\n\t\tspaceVariant := strings.ReplaceAll(word, \"_\", \" \")\r\n\t\tvariants = append(variants, spaceVariant)\r\n\t\tvariants = append(variants, \"l'\"+spaceVariant, \"d'\"+spaceVariant, \"l \"+spaceVariant, \"d \"+spaceVariant)\r\n\t}\r\n\r\n\treturn variants\r\n}\r\n\r\n// Fonction utilitaire pour dédupliquer une slice d'entiers\r\nfunc uniqueIntSlice(intSlice []int) []int {\r\n\tkeys := make(map[int]bool)\r\n\tvar list []int\r\n\tfor _, entry := range intSlice {\r\n\t\tif _, value := keys[entry]; !value {\r\n\t\t\tkeys[entry] = true\r\n\t\t\tlist = append(list, entry)\r\n\t\t}\r\n\t}\r\n\treturn list\r\n}\r\n",
    "size": 30641,
    "modTime": "2024-11-04T09:53:41.8439112+01:00",
    "path": "internal\\pipeline\\pipeline.go"
  },
  {
    "name": "prompt.go",
    "content": "package prompt\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"strings\"\r\n)\r\n\r\n// PromptTemplate représente un template de prompt\r\ntype PromptTemplate struct {\r\n\tTemplate string\r\n}\r\n\r\n// NewPromptTemplate crée un nouveau PromptTemplate\r\nfunc NewPromptTemplate(template string) *PromptTemplate {\r\n\treturn \u0026PromptTemplate{Template: template}\r\n}\r\n\r\n// Format remplit le template avec les valeurs fournies\r\nfunc (pt *PromptTemplate) Format(values map[string]string) string {\r\n    result := pt.Template\r\n    for key, value := range values {\r\n        result = strings.Replace(result, fmt.Sprintf(\"{%s}\", key), value, -1)\r\n    }\r\n    \r\n    // Ajouter le prompt supplémentaire s'il existe\r\n    if additionalPrompt, ok := values[\"additional_prompt\"]; ok \u0026\u0026 additionalPrompt != \"\" {\r\n        result += \"\\n\\nAdditional instructions:\\n\" + additionalPrompt\r\n    }\r\n    \r\n    return result\r\n}\r\n\r\n// Définition des templates de prompts\r\nvar (\r\n\tEntityExtractionPrompt = NewPromptTemplate(`\r\nAnalyze the following text and extract key entities (e.g., people, organizations, concepts) relevant to building an ontology:\r\n\r\n{text}\r\n\r\nFor each entity, provide:\r\n1. Entity name\r\n2. Entity type (e.g., Person, Organization, Concept)\r\n3. A brief description or context\r\n\r\nFormat your response as a list of entities, one per line, using tabs to separate fields like this:\r\nEntityName\\tEntityType\\tDescription/Context\r\n\r\nEnsure that:\r\nyour extractions are relevant to creating an ontology and avoid including irrelevant or trivial information.\r\nAll spaces in entity names are replaced with underscores\r\nYour extractions are relevant to creating an ontology\r\nYou avoid including irrelevant or trivial information\r\nYou use the original document language\r\nYou provide the output silently with no additional comments\r\n`)\r\n\r\n\tRelationExtractionPrompt = NewPromptTemplate(`\r\nBased on the following text and the list of entities provided, identify relationships between these entities that would be relevant for an ontology:\r\n\r\nText:\r\n{text}\r\n\r\nEntities:\r\n{entities}\r\n\r\nFor each relationship, provide:\r\n1. Source Entity\r\n2. Relationship Type\r\n3. Target Entity\r\n4. A brief description or context of the relationship\r\n\r\nFormat your response as a list of relationships, one per line, using tabs to separate fields like this:\r\nSourceEntity\\tRelationshipType\\tTargetEntity\\tDescription/Context\r\n\r\nFocus on meaningful relationships that contribute to the structure of the ontology. Avoid trivial or overly generic relationships.\r\nEnsure that:\r\n\r\nAll spaces in entity names are replaced with underscores\r\nYour extractions are relevant to creating an ontology\r\nYou avoid including irrelevant or trivial information\r\nYou use the original document language\r\nYou provide the output silently with no additional comments\r\n`)\r\n\r\n\tOntologyEnrichmentPrompt = NewPromptTemplate(`\r\nVous êtes un expert en ontologies chargé d'enrichir et de raffiner une ontologie existante. Voici l'ontologie actuelle et de nouvelles informations à intégrer :\r\n\r\nOntologie actuelle :\r\n{previous_result}\r\n\r\nNouveau texte à analyser :\r\n{text}\r\n\r\nContexte supplémentaire :\r\n{context}\r\n\r\nVotre tâche :\r\n1. Analyser le nouveau texte et le contexte.\r\n2. Identifier les nouvelles entités et relations pertinentes.\r\n3. Intégrer ces nouvelles informations dans l'ontologie existante.\r\n4. Raffiner les entités et relations existantes si nécessaire.\r\n5. Assurer la cohérence globale de l'ontologie.\r\n\r\nFournissez l'ontologie enrichie et raffinée dans le format suivant :\r\n- Pour les entités : Nom_Entité\\tType_Entité\\tDescription\r\n- Pour les relations : Entité_Source\\tType_Relation\\tEntité_Cible\\tDescription\r\n\r\nAssurez-vous que :\r\n\r\nTous les espaces dans les noms d'entités sont remplacés par des underscores.\r\nVos extractions sont pertinentes pour la création d'une ontologie\r\nVous évitez d'inclure des informations non pertinentes ou triviales\r\nVous utilisez la langue originale du document\r\nVous fournissez le résultat silencieusement sans commentaires supplémentaires\r\n`)\r\n\r\n\tOntologyMergePrompt = NewPromptTemplate(`\r\nVous êtes un expert en fusion d'ontologies. Votre tâche est de fusionner intelligemment une ontologie existante avec de nouvelles informations pour créer une ontologie enrichie et cohérente.\r\n\r\nOntologie existante :\r\n{previous_ontology}\r\n\r\nNouvelles informations à intégrer :\r\n{new_ontology}\r\n\r\nDirectives pour la fusion :\r\n1. Intégrez toutes les nouvelles entités et relations pertinentes de la nouvelle ontologie.\r\n2. En cas de conflit ou de duplication, identifie les concepts qui sont essentiellement identiques ou très proches sémantiquement. Pour chaque groupe de concepts similaires, choisis le nom le plus approprié et représentatif.\r\nFusionne les descriptions en une seule, plus complète. Combine toutes les positions textuelles en une seule liste, sans doublons, triée par ordre croissant.\r\n3. Assurez-vous que les relations entre les entités restent cohérentes.\r\n4. Si une nouvelle information contredit une ancienne, privilégiez la nouvelle mais notez la contradiction si elle est significative.\r\n5. Maintenez la structure et le format de l'ontologie existante.\r\n6. Évitez les redondances et les informations en double.\r\n\r\nVotre tâche :\r\n- Analysez attentivement les deux ensembles d'informations.\r\n- Fusionnez-les en une ontologie unique et cohérente.\r\n- Assurez-vous que le résultat final est complet, sans perte d'information importante.\r\n\r\nFormat de sortie :\r\nPrésentez l'ontologie fusionnée dans le même format que l'ontologie existante, avec une entité ou une relation par ligne.\r\nPour les entités : Nom_Entité\\tType_Entité\\tDescription\r\nPour les relations : Entité_Source\\tType_Relation\\tEntité_Cible\\tDescription\r\n\r\nProcédez à la fusion de manière silencieuse, sans ajouter de commentaires ou d'explications supplémentaires.\r\n`)\r\n)\r\n",
    "size": 5836,
    "modTime": "2024-10-30T22:49:14.3348105+01:00",
    "path": "internal\\prompt\\prompt.go"
  },
  {
    "name": "segmenter.go",
    "content": "package segmenter\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"errors\"\r\n\t\"fmt\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/pkoukk/tiktoken-go\"\r\n)\r\n\r\nvar (\r\n\tErrInvalidContent = errors.New(i18n.Messages.ErrInvalidContent)\r\n\tErrTokenization   = errors.New(i18n.Messages.ErrTokenization)\r\n)\r\n\r\nvar log = logger.GetLogger()\r\n\r\n// SegmentConfig holds the configuration for segmentation\r\ntype SegmentConfig struct {\r\n\tMaxTokens   int\r\n\tContextSize int\r\n\tModel       string\r\n}\r\n\r\n// Segment divides the content into segments of maxTokens\r\ntype SegmentInfo struct {\r\n\tContent []byte\r\n\tStart   int\r\n\tEnd     int\r\n}\r\n\r\nfunc Segment(content []byte, cfg SegmentConfig) ([]SegmentInfo, error) {\r\n\tlog.Debug(i18n.Messages.LogSegmentationStarted)\r\n\tlog.Debug(fmt.Sprintf(\"Segmentation config: MaxTokens=%d, ContextSize=%d, Model=%s\", cfg.MaxTokens, cfg.ContextSize, cfg.Model))\r\n\tlog.Debug(fmt.Sprintf(\"Content length: %d bytes\", len(content)))\r\n\r\n\tif len(content) == 0 {\r\n\t\treturn nil, ErrInvalidContent\r\n\t}\r\n\r\n\ttokenizer, err := getTokenizer(cfg.Model)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\tvar segments []SegmentInfo\r\n\tsentences := splitIntoSentences(content)\r\n\tcurrentSegment := new(bytes.Buffer)\r\n\tcurrentTokenCount := 0\r\n\tcurrentStart := 0\r\n\r\n\tfor _, sentence := range sentences {\r\n\t\tsentenceTokens := CountTokens(sentence, tokenizer)\r\n\r\n\t\tif currentTokenCount+sentenceTokens \u003e cfg.MaxTokens {\r\n\t\t\tif currentSegment.Len() \u003e 0 {\r\n\t\t\t\tsegmentContent := content[currentStart : currentStart+currentSegment.Len()]\r\n\t\t\t\tsegments = append(segments, SegmentInfo{\r\n\t\t\t\t\tContent: segmentContent,\r\n\t\t\t\t\tStart:   currentStart,\r\n\t\t\t\t\tEnd:     currentStart + currentSegment.Len(),\r\n\t\t\t\t})\r\n\t\t\t\tlog.Debug(fmt.Sprintf(\"Segment created. Start: %d, End: %d, Length: %d bytes, Tokens: %d, Preview: %s\",\r\n\t\t\t\t\tcurrentStart, currentStart+currentSegment.Len(), len(segmentContent), currentTokenCount,\r\n\t\t\t\t\ttruncateString(string(segmentContent), 100)))\r\n\t\t\t\tcurrentStart = currentStart + currentSegment.Len()\r\n\t\t\t\tcurrentSegment.Reset()\r\n\t\t\t\tcurrentTokenCount = 0\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tcurrentSegment.Write(sentence)\r\n\t\tcurrentTokenCount += sentenceTokens\r\n\r\n\t\tif currentTokenCount \u003e= cfg.MaxTokens {\r\n\t\t\tsegmentContent := content[currentStart : currentStart+currentSegment.Len()]\r\n\t\t\tsegments = append(segments, SegmentInfo{\r\n\t\t\t\tContent: segmentContent,\r\n\t\t\t\tStart:   currentStart,\r\n\t\t\t\tEnd:     currentStart + currentSegment.Len(),\r\n\t\t\t})\r\n\t\t\tlog.Debug(fmt.Sprintf(\"Segment created. Start: %d, End: %d, Length: %d bytes, Tokens: %d, Preview: %s\",\r\n\t\t\t\tcurrentStart, currentStart+currentSegment.Len(), len(segmentContent), currentTokenCount,\r\n\t\t\t\ttruncateString(string(segmentContent), 100)))\r\n\t\t\tcurrentStart = currentStart + currentSegment.Len()\r\n\t\t\tcurrentSegment.Reset()\r\n\t\t\tcurrentTokenCount = 0\r\n\t\t}\r\n\t}\r\n\r\n\tif currentSegment.Len() \u003e 0 {\r\n\t\tsegmentContent := content[currentStart : currentStart+currentSegment.Len()]\r\n\t\tsegments = append(segments, SegmentInfo{\r\n\t\t\tContent: segmentContent,\r\n\t\t\tStart:   currentStart,\r\n\t\t\tEnd:     currentStart + currentSegment.Len(),\r\n\t\t})\r\n\t\tlog.Debug(fmt.Sprintf(\"Final segment created. Start: %d, End: %d, Length: %d bytes, Tokens: %d, Preview: %s\",\r\n\t\t\tcurrentStart, currentStart+currentSegment.Len(), len(segmentContent), currentTokenCount,\r\n\t\t\ttruncateString(string(segmentContent), 100)))\r\n\t}\r\n\r\n\tlog.Info(fmt.Sprintf(i18n.Messages.LogSegmentationCompleted, len(segments)))\r\n\treturn segments, nil\r\n}\r\nfunc splitIntoSentences(content []byte) [][]byte {\r\n\tvar sentences [][]byte\r\n\tvar currentSentence []byte\r\n\r\n\tfor _, b := range content {\r\n\t\tcurrentSentence = append(currentSentence, b)\r\n\t\tif b == '.' || b == '!' || b == '?' {\r\n\t\t\tsentences = append(sentences, currentSentence)\r\n\t\t\tcurrentSentence = []byte{}\r\n\t\t}\r\n\t}\r\n\r\n\tif len(currentSentence) \u003e 0 {\r\n\t\tsentences = append(sentences, currentSentence)\r\n\t}\r\n\r\n\treturn sentences\r\n}\r\n\r\n// CountTokens returns the number of tokens in the content\r\nfunc CountTokens(content []byte, tokenizer *tiktoken.Tiktoken) int {\r\n\ttokens := tokenizer.Encode(string(content), nil, nil)\r\n\tcount := len(tokens)\r\n\treturn count\r\n}\r\n\r\n// getTokenizer returns a tokenizer for the specified model\r\nfunc getTokenizer(model string) (*tiktoken.Tiktoken, error) {\r\n\tencoding, err := tiktoken.GetEncoding(\"cl100k_base\")\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.Messages.ErrTokenizerInitialization, err)\r\n\t}\r\n\treturn encoding, nil\r\n}\r\n\r\n// CalibrateTokenCount adjusts the token count based on the LLM model\r\nfunc CalibrateTokenCount(count int, model string) int {\r\n\tlog.Debug(\"Calibrating token count for model %s. Original count: %d\", model, count)\r\n\t// Implement model-specific calibration logic here\r\n\t// For now, we'll just return the original count\r\n\tlog.Debug(\"Calibrated count: %d\", count)\r\n\treturn count\r\n}\r\n\r\n// GetContext returns the context of previous segments\r\nfunc GetContext(segments []SegmentInfo, currentIndex int, cfg SegmentConfig) string {\r\n\tlog.Debug(\"GetContext called for segment %d/%d\", currentIndex+1, len(segments))\r\n\r\n\tvar context bytes.Buffer\r\n\ttokenCount := 0\r\n\tsegmentsUsed := 0\r\n\r\n\ttokenizer, err := getTokenizer(cfg.Model)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to get tokenizer: %v\", err)\r\n\t\treturn \"\"\r\n\t}\r\n\r\n\tfor i := currentIndex - 1; i \u003e= 0 \u0026\u0026 tokenCount \u003c cfg.ContextSize; i-- {\r\n\t\tsegmentTokens := CountTokens(segments[i].Content, tokenizer)\r\n\t\tif tokenCount+segmentTokens \u003e cfg.ContextSize {\r\n\t\t\tbreak\r\n\t\t}\r\n\t\t// Prepend this segment to the context\r\n\t\ttemp := make([]byte, len(segments[i].Content)+1)\r\n\t\tcopy(temp[1:], segments[i].Content)\r\n\t\ttemp[0] = '\\n'\r\n\t\tcontext.Write(temp)\r\n\t\ttokenCount += segmentTokens\r\n\t\tsegmentsUsed++\r\n\t}\r\n\r\n\tlog.Debug(\"Generated context for segment %d/%d. Segments used: %d, Token count: %d, Context length: %d bytes\",\r\n\t\tcurrentIndex+1, len(segments), segmentsUsed, tokenCount, context.Len())\r\n\tlog.Debug(\"Context preview for segment %d: %s\",\r\n\t\tcurrentIndex+1, truncateString(context.String(), 100))\r\n\r\n\treturn context.String()\r\n}\r\n\r\nfunc truncateString(s string, maxLength int) string {\r\n\tif len(s) \u003c= maxLength {\r\n\t\treturn s\r\n\t}\r\n\treturn s[:maxLength] + \"...\"\r\n}\r\n",
    "size": 6140,
    "modTime": "2024-10-30T22:49:14.3368111+01:00",
    "path": "internal\\segmenter\\segmenter.go"
  },
  {
    "name": "tokenizer.go",
    "content": "package tokenizer\r\n\r\nimport (\r\n\t\"github.com/pkoukk/tiktoken-go\"\r\n)\r\n\r\n// CountTokens returns the number of tokens in the given text\r\nfunc CountTokens(text string) (int, error) {\r\n\tencoding, err := tiktoken.GetEncoding(\"cl100k_base\")\r\n\tif err != nil {\r\n\t\treturn 0, err\r\n\t}\r\n\ttokens := encoding.Encode(text, nil, nil)\r\n\treturn len(tokens), nil\r\n}\r\n",
    "size": 346,
    "modTime": "2024-10-20T15:16:21.4160242+02:00",
    "path": "internal\\tokenizer\\tokenizer.go"
  }
]