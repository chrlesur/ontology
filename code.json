[
  {
    "name": "main.go",
    "content": "// cmd/ontology/main.go\n\npackage main\n\nimport \"github.com/chrlesur/Ontology/internal/ontology\"\n\n// main is the entry point of the Ontology application.\n// It calls the Run function from the ontology package.\nfunc main() {\n\tontology.Execute()\n}\n",
    "size": 244,
    "modTime": "2024-10-21T20:24:11.756592+02:00",
    "path": "cmd\\ontology\\main.go"
  },
  {
    "name": "config.yaml",
    "content": "base_uri: \"http://www.wikidata.org/entity/\"\r\nopenai_api_url: \"https://api.openai.com/v1/chat/completions\"\r\nclaude_api_url: \"https://api.anthropic.com/v1/messages\"\r\nollama_api_url: \"http://localhost:11434/api/generate\"\r\nlog_directory: \"logs\"\r\nlog_level: \"info\"\r\nmax_tokens: 2000\r\ncontext_size: 4000\r\ndefault_llm: \"claude\"\r\ndefault_model: \"claude-3-5-sonnet-20240620\"",
    "size": 365,
    "modTime": "2024-10-21T23:24:45.2649414+02:00",
    "path": "config.yaml"
  },
  {
    "name": "config.go",
    "content": "package config\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"log\"\r\n\t\"os\"\r\n\t\"sync\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"gopkg.in/yaml.v2\"\r\n)\r\n\r\nvar (\r\n\tonce     sync.Once\r\n\tinstance *Config\r\n)\r\n\r\n// Config structure definition\r\ntype Config struct {\r\n\tBaseURI      string `yaml:\"base_uri\"`\r\n\tOpenAIAPIURL string `yaml:\"openai_api_url\"`\r\n\tClaudeAPIURL string `yaml:\"claude_api_url\"`\r\n\tOllamaAPIURL string `yaml:\"ollama_api_url\"`\r\n\tOpenAIAPIKey string `yaml:\"openai_api_key\"`\r\n\tClaudeAPIKey string `yaml:\"claude_api_key\"`\r\n\tLogDirectory string `yaml:\"log_directory\"`\r\n\tLogLevel     string `yaml:\"log_level\"`\r\n\tMaxTokens    int    `yaml:\"max_tokens\"`\r\n\tContextSize  int    `yaml:\"context_size\"`\r\n\tDefaultLLM   string `yaml:\"default_llm\"`\r\n\tDefaultModel string `yaml:\"default_model\"`\r\n\tOntologyName string `yaml:\"ontology_name\"`\r\n\tExportRDF    bool   `yaml:\"export_rdf\"`\r\n\tExportOWL    bool   `yaml:\"export_owl\"`\r\n\tInput        string `yaml:\"input\"`\r\n}\r\n\r\n// GetConfig returns the singleton instance of Config\r\nfunc GetConfig() *Config {\r\n\tonce.Do(func() {\r\n\t\tinstance = \u0026Config{\r\n\t\t\tOpenAIAPIURL: \"https://api.openai.com/v1/chat/completions\",\r\n\t\t\tClaudeAPIURL: \"https://api.anthropic.com/v1/messages\",\r\n\t\t\tOllamaAPIURL: \"http://localhost:11434/api/generate\",\r\n\t\t\tBaseURI:      \"http://www.wikidata.org/entity/\",\r\n\t\t\tLogDirectory: \"logs\",\r\n\t\t\tLogLevel:     \"info\",\r\n\t\t\tMaxTokens:    4000,\r\n\t\t\tContextSize:  500,\r\n\t\t\tDefaultLLM:   \"openai\",\r\n\t\t\tDefaultModel: \"gpt-3.5-turbo\",\r\n\t\t}\r\n\t\tinstance.loadConfigFile()\r\n\t\tinstance.loadEnvVariables()\r\n\t})\r\n\treturn instance\r\n}\r\n\r\n// loadConfigFile loads the configuration from a YAML file\r\nfunc (c *Config) loadConfigFile() {\r\n\tconfigPath := os.Getenv(\"ONTOLOGY_CONFIG_PATH\")\r\n\tif configPath == \"\" {\r\n\t\tconfigPath = \"config.yaml\"\r\n\t}\r\n\r\n\tdata, err := ioutil.ReadFile(configPath)\r\n\tif err != nil {\r\n\t\tlog.Printf(i18n.GetMessage(\"ErrReadConfigFile\"), err)\r\n\t\treturn\r\n\t}\r\n\r\n\terr = yaml.Unmarshal(data, c)\r\n\tif err != nil {\r\n\t\tlog.Printf(i18n.GetMessage(\"ErrParseConfigFile\"), err)\r\n\t}\r\n}\r\n\r\n// loadEnvVariables loads configuration from environment variables\r\nfunc (c *Config) loadEnvVariables() {\r\n\tif apiKey := os.Getenv(\"OPENAI_API_KEY\"); apiKey != \"\" {\r\n\t\tc.OpenAIAPIKey = apiKey\r\n\t}\r\n\tif apiKey := os.Getenv(\"CLAUDE_API_KEY\"); apiKey != \"\" {\r\n\t\tc.ClaudeAPIKey = apiKey\r\n\t}\r\n\t// Add more environment variables as needed\r\n}\r\n\r\n// ValidateConfig checks if the configuration is valid\r\nfunc (c *Config) ValidateConfig() error {\r\n\tif c.OpenAIAPIKey == \"\" \u0026\u0026 c.ClaudeAPIKey == \"\" {\r\n\t\treturn fmt.Errorf(i18n.GetMessage(\"ErrNoAPIKeys\"))\r\n\t}\r\n\t// Add more validation checks as needed\r\n\treturn nil\r\n}\r\n\r\n// Reload reloads the configuration from file and environment variables\r\nfunc (c *Config) Reload() error {\r\n\tc.loadConfigFile()\r\n\tc.loadEnvVariables()\r\n\treturn c.ValidateConfig()\r\n}\r\n",
    "size": 2817,
    "modTime": "2024-10-20T18:07:18.6889104+02:00",
    "path": "internal\\config\\config.go"
  },
  {
    "name": "convert.go",
    "content": "package converter\r\n\r\nimport (\r\n\t\"bufio\"\r\n\t\"bytes\"\r\n\t\"fmt\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\n// Convert converts a document segment into QuickStatement format\r\nfunc (qsc *QuickStatementConverter) Convert(segment []byte, context string, ontology string) (string, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ConvertStarted\"))\r\n\tlog.Debug(\"Input segment:\\n%s\", string(segment))\r\n\r\n\tstatements, err := qsc.parseSegment(segment)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.GetMessage(\"FailedToParseSegment\"), err)\r\n\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"FailedToParseSegment\"), err)\r\n\t}\r\n\r\n\tlog.Debug(\"Parsed %d statements\", len(statements))\r\n\r\n\tvar tsvBuilder strings.Builder\r\n\tfor _, stmt := range statements {\r\n\t\t// Écrire la ligne TSV\r\n\t\ttsvLine := fmt.Sprintf(\"%s\\t%s\\t%s\\n\", stmt.Subject.ID, stmt.Property.ID, stmt.Object)\r\n\t\ttsvBuilder.WriteString(tsvLine)\r\n\t}\r\n\r\n\tresult := tsvBuilder.String()\r\n\tlog.Debug(\"Generated TSV output:\\n%s\", result)\r\n\treturn result, nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) cleanAndNormalizeInput(input string) string {\r\n\tlines := strings.Split(input, \"\\n\")\r\n\tvar cleanedLines []string\r\n\tfor _, line := range lines {\r\n\t\ttrimmedLine := strings.TrimSpace(line)\r\n\t\tif trimmedLine != \"\" {\r\n\t\t\tcleanedLines = append(cleanedLines, trimmedLine)\r\n\t\t}\r\n\t}\r\n\treturn strings.Join(cleanedLines, \"\\n\")\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) parseSegment(segment []byte) ([]Statement, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParsingSegment\"))\r\n\tvar statements []Statement\r\n\tscanner := bufio.NewScanner(bytes.NewReader(segment))\r\n\tfor scanner.Scan() {\r\n\t\tline := scanner.Text()\r\n\t\t// Remplacer les doubles backslashes par un caractère temporaire\r\n\t\tline = strings.ReplaceAll(line, \"\\\\\\\\\", \"\\uFFFD\")\r\n\t\t// Remplacer les \\t par des tabulations réelles\r\n\t\tline = strings.ReplaceAll(line, \"\\\\t\", \"\\t\")\r\n\t\t// Restaurer les doubles backslashes\r\n\t\tline = strings.ReplaceAll(line, \"\\uFFFD\", \"\\\\\")\r\n\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tif len(parts) \u003c 3 {\r\n\t\t\tlog.Debug(\"Skipping invalid line: %s\", line)\r\n\t\t\tcontinue\r\n\t\t}\r\n\t\tstatement := Statement{\r\n\t\t\tSubject:  Entity{ID: strings.TrimSpace(parts[0])},\r\n\t\t\tProperty: Property{ID: strings.TrimSpace(parts[1])},\r\n\t\t\tObject:   strings.TrimSpace(strings.Join(parts[2:], \"\\t\")),\r\n\t\t}\r\n\t\tstatements = append(statements, statement)\r\n\t}\r\n\tif err := scanner.Err(); err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorScanningSegment\"), err)\r\n\t}\r\n\tif len(statements) == 0 {\r\n\t\treturn nil, fmt.Errorf(i18n.GetMessage(\"NoValidStatementsFound\"))\r\n\t}\r\n\treturn statements, nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) applyContextAndOntology(statements []Statement, context string, ontology string) ([]Statement, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ApplyingContextAndOntology\"))\r\n\t// This is a placeholder implementation. In a real-world scenario, this function would\r\n\t// use the context and ontology to enrich the statements.\r\n\tfor i := range statements {\r\n\t\tstatements[i].Subject.Label = fmt.Sprintf(\"%s (from context)\", statements[i].Subject.ID)\r\n\t}\r\n\treturn statements, nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) toQuickStatementTSV(statements []Statement) (string, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ConvertingToQuickStatementTSV\"))\r\n\tvar buffer bytes.Buffer\r\n\tfor _, stmt := range statements {\r\n\t\tline := fmt.Sprintf(\"%s\\t%s\\t%v\\n\", stmt.Subject.ID, stmt.Property.ID, stmt.Object)\r\n\t\t_, err := buffer.WriteString(line)\r\n\t\tif err != nil {\r\n\t\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorWritingQuickStatement\"), err)\r\n\t\t}\r\n\t}\r\n\treturn buffer.String(), nil\r\n}\r\n",
    "size": 3591,
    "modTime": "2024-10-21T23:06:48.3289982+02:00",
    "path": "internal\\converter\\convert.go"
  },
  {
    "name": "logger.go",
    "content": "package converter\r\n\r\nimport (\r\n    \"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()",
    "size": 116,
    "modTime": "2024-10-20T14:40:18.8000271+02:00",
    "path": "internal\\converter\\logger.go"
  },
  {
    "name": "owl.go",
    "content": "package converter\r\n\r\nimport (\r\n\t\"bufio\"\r\n\t\"bytes\"\r\n\t\"fmt\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\n// ConvertToOWL converts a QuickStatement to OWL format\r\nfunc (qsc *QuickStatementConverter) ConvertToOWL(quickstatement string) (string, error) {\r\n\tqsc.logger.Debug(i18n.GetMessage(\"ConvertToOWLStarted\"))\r\n\r\n\tstatements, err := qsc.parseQuickStatement(quickstatement)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"FailedToParseQuickStatement\"), err)\r\n\t}\r\n\r\n\tvar owlStatements []string\r\n\tfor _, stmt := range statements {\r\n\t\towlStmt, err := qsc.statementToOWL(stmt)\r\n\t\tif err != nil {\r\n\t\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"FailedToConvertStatementToOWL\"), err)\r\n\t\t}\r\n\t\towlStatements = append(owlStatements, owlStmt)\r\n\t}\r\n\r\n\tresult := qsc.generateOWLDocument(owlStatements)\r\n\r\n\tqsc.logger.Debug(i18n.GetMessage(\"ConvertToOWLFinished\"))\r\n\treturn result, nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) parseQuickStatement(quickstatement string) ([]Statement, error) {\r\n\tqsc.logger.Debug(i18n.GetMessage(\"ParsingQuickStatement\"))\r\n\tvar statements []Statement\r\n\tscanner := bufio.NewScanner(strings.NewReader(quickstatement))\r\n\tfor scanner.Scan() {\r\n\t\tline := scanner.Text()\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tif len(parts) \u003c 3 {\r\n\t\t\treturn nil, fmt.Errorf(i18n.GetMessage(\"InvalidQuickStatementFormat\"))\r\n\t\t}\r\n\t\tstatement := Statement{\r\n\t\t\tSubject:  Entity{ID: parts[0]},\r\n\t\t\tProperty: Property{ID: parts[1]},\r\n\t\t\tObject:   parts[2],\r\n\t\t}\r\n\t\tstatements = append(statements, statement)\r\n\t}\r\n\tif err := scanner.Err(); err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorScanningQuickStatement\"), err)\r\n\t}\r\n\treturn statements, nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) statementToOWL(statement Statement) (string, error) {\r\n\tqsc.logger.Debug(i18n.GetMessage(\"ConvertingStatementToOWL\"))\r\n\tsubjectURI := fmt.Sprintf(\":%s\", statement.Subject.ID)\r\n\tpropertyURI := fmt.Sprintf(\":%s\", statement.Property.ID)\r\n\tobjectValue := statement.Object.(string) // Type assertion, be cautious in real implementation\r\n\r\n\tvar owlStatement string\r\n\tif strings.HasPrefix(objectValue, \"Q\") {\r\n\t\t// If object is an entity\r\n\t\tobjectURI := fmt.Sprintf(\":%s\", objectValue)\r\n\t\towlStatement = fmt.Sprintf(\"ObjectPropertyAssertion(%s %s %s)\", propertyURI, subjectURI, objectURI)\r\n\t} else {\r\n\t\t// If object is a literal\r\n\t\towlStatement = fmt.Sprintf(\"DataPropertyAssertion(%s %s \\\"%s\\\"^^xsd:string)\", propertyURI, subjectURI, objectValue)\r\n\t}\r\n\r\n\treturn owlStatement, nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) generateOWLDocument(owlStatements []string) string {\r\n\tqsc.logger.Debug(i18n.GetMessage(\"GeneratingOWLDocument\"))\r\n\tvar buffer bytes.Buffer\r\n\r\n\tbuffer.WriteString(\"Prefix(:=\u003c\" + config.GetConfig().BaseURI + \"\u003e)\\n\")\r\n\tbuffer.WriteString(\"Prefix(owl:=\u003chttp://www.w3.org/2002/07/owl#\u003e)\\n\")\r\n\tbuffer.WriteString(\"Prefix(rdf:=\u003chttp://www.w3.org/1999/02/22-rdf-syntax-ns#\u003e)\\n\")\r\n\tbuffer.WriteString(\"Prefix(xml:=\u003chttp://www.w3.org/XML/1998/namespace\u003e)\\n\")\r\n\tbuffer.WriteString(\"Prefix(xsd:=\u003chttp://www.w3.org/2001/XMLSchema#\u003e)\\n\")\r\n\tbuffer.WriteString(\"Prefix(rdfs:=\u003chttp://www.w3.org/2000/01/rdf-schema#\u003e)\\n\\n\")\r\n\tbuffer.WriteString(\"Ontology(\u003c\" + config.GetConfig().BaseURI + \"\u003e\\n\\n\")\r\n\r\n\tfor _, stmt := range owlStatements {\r\n\t\tbuffer.WriteString(stmt + \"\\n\")\r\n\t}\r\n\r\n\tbuffer.WriteString(\")\")\r\n\r\n\treturn buffer.String()\r\n}\r\n",
    "size": 3438,
    "modTime": "2024-10-20T16:47:48.1038854+02:00",
    "path": "internal\\converter\\owl.go"
  },
  {
    "name": "parse.go",
    "content": "package converter\r\n\r\nimport (\r\n\t\"encoding/xml\"\r\n\t\"fmt\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/knakk/rdf\"\r\n)\r\n\r\n// ParseOntology parses an ontology string and returns a structured representation\r\nfunc ParseOntology(ontology string) (map[string]interface{}, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParseOntologyStarted\"))\r\n\r\n\tformat := detectOntologyFormat(ontology)\r\n\tvar result map[string]interface{}\r\n\tvar err error\r\n\r\n\tswitch format {\r\n\tcase \"QuickStatement\":\r\n\t\tresult, err = parseQuickStatementOntology(ontology)\r\n\tcase \"RDF\":\r\n\t\tresult, err = parseRDFOntology(ontology)\r\n\tcase \"OWL\":\r\n\t\tresult, err = parseOWLOntology(ontology)\r\n\tdefault:\r\n\t\treturn nil, fmt.Errorf(i18n.GetMessage(\"UnknownOntologyFormat\"))\r\n\t}\r\n\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"FailedToParseOntology\"), err)\r\n\t}\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ParseOntologyFinished\"))\r\n\treturn result, nil\r\n}\r\n\r\nfunc detectOntologyFormat(ontology string) string {\r\n\tif strings.Contains(ontology, \"Q\") \u0026\u0026 strings.Contains(ontology, \"P\") \u0026\u0026 strings.Contains(ontology, \"\\t\") {\r\n\t\treturn \"QuickStatement\"\r\n\t}\r\n\tif strings.Contains(ontology, \"\u003crdf:RDF\") {\r\n\t\treturn \"RDF\"\r\n\t}\r\n\tif strings.Contains(ontology, \"\u003cOntology\") {\r\n\t\treturn \"OWL\"\r\n\t}\r\n\treturn \"Unknown\"\r\n}\r\n\r\nfunc parseQuickStatementOntology(ontology string) (map[string]interface{}, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParseQuickStatementOntologyStarted\"))\r\n\r\n\tresult := make(map[string]interface{})\r\n\tentities := make(map[string]map[string]interface{})\r\n\r\n\tlines := strings.Split(ontology, \"\\n\")\r\n\tfor _, line := range lines {\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tif len(parts) != 3 {\r\n\t\t\treturn nil, fmt.Errorf(i18n.GetMessage(\"InvalidQuickStatementLine\"))\r\n\t\t}\r\n\r\n\t\tsubject, predicate, object := parts[0], parts[1], parts[2]\r\n\t\tif _, exists := entities[subject]; !exists {\r\n\t\t\tentities[subject] = make(map[string]interface{})\r\n\t\t}\r\n\t\tentities[subject][predicate] = object\r\n\t}\r\n\r\n\tresult[\"entities\"] = entities\r\n\tlog.Debug(i18n.GetMessage(\"ParseQuickStatementOntologyFinished\"))\r\n\treturn result, nil\r\n}\r\n\r\nfunc parseRDFOntology(ontology string) (map[string]interface{}, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParseRDFOntologyStarted\"))\r\n\r\n\tresult := make(map[string]interface{})\r\n\tentities := make(map[string]map[string]interface{})\r\n\r\n\tdec := rdf.NewTripleDecoder(strings.NewReader(ontology), rdf.RDFXML)\r\n\tfor {\r\n\t\ttriple, err := dec.Decode()\r\n\t\tif err != nil {\r\n\t\t\tif err.Error() == \"EOF\" {\r\n\t\t\t\tbreak\r\n\t\t\t}\r\n\t\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorDecodingRDF\"), err)\r\n\t\t}\r\n\r\n\t\tsubject := triple.Subj.String()\r\n\t\tpredicate := triple.Pred.String()\r\n\t\tobject := triple.Obj.String()\r\n\r\n\t\tif _, exists := entities[subject]; !exists {\r\n\t\t\tentities[subject] = make(map[string]interface{})\r\n\t\t}\r\n\t\tentities[subject][predicate] = object\r\n\t}\r\n\r\n\tresult[\"entities\"] = entities\r\n\tlog.Debug(i18n.GetMessage(\"ParseRDFOntologyFinished\"))\r\n\treturn result, nil\r\n}\r\n\r\nfunc parseOWLOntology(ontology string) (map[string]interface{}, error) {\r\n\tlog.Debug(i18n.GetMessage(\"ParseOWLOntologyStarted\"))\r\n\r\n\tresult := make(map[string]interface{})\r\n\r\n\t// Simplified OWL parsing\r\n\tvar owlData struct {\r\n\t\tXMLName xml.Name `xml:\"Ontology\"`\r\n\t\tClasses []struct {\r\n\t\t\tIRI   string `xml:\"IRI,attr\"`\r\n\t\t\tLabel string `xml:\"label,attr\"`\r\n\t\t} `xml:\"Declaration\u003eClass\"`\r\n\t}\r\n\r\n\terr := xml.Unmarshal([]byte(ontology), \u0026owlData)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorParsingOWL\"), err)\r\n\t}\r\n\r\n\tclasses := make(map[string]interface{})\r\n\tfor _, class := range owlData.Classes {\r\n\t\tclasses[class.IRI] = map[string]interface{}{\r\n\t\t\t\"label\": class.Label,\r\n\t\t}\r\n\t}\r\n\tresult[\"classes\"] = classes\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ParseOWLOntologyFinished\"))\r\n\treturn result, nil\r\n}\r\n",
    "size": 3787,
    "modTime": "2024-10-20T14:19:06.7515729+02:00",
    "path": "internal\\converter\\parse.go"
  },
  {
    "name": "quickstatement.go",
    "content": "// Package converter provides functionality for converting document segments\r\n// into QuickStatement format and other ontology representations.\r\npackage converter\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\n// Converter defines the interface for QuickStatement conversion\r\ntype Converter interface {\r\n\tConvert(segment []byte, context string, ontology string) (string, error)\r\n\tConvertToRDF(quickstatement string) (string, error)\r\n\tConvertToOWL(quickstatement string) (string, error)\r\n}\r\n\r\n// QuickStatementConverter implements the Converter interface\r\ntype QuickStatementConverter struct {\r\n\tlogger *logger.Logger\r\n}\r\n\r\n// NewQuickStatementConverter creates a new QuickStatementConverter\r\nfunc NewQuickStatementConverter(log *logger.Logger) *QuickStatementConverter {\r\n\treturn \u0026QuickStatementConverter{\r\n\t\tlogger: log,\r\n\t}\r\n}\r\n\r\n// Entity represents a Wikibase entity\r\ntype Entity struct {\r\n\tID    string\r\n\tLabel string\r\n}\r\n\r\n// Property represents a Wikibase property\r\ntype Property struct {\r\n\tID       string\r\n\tDataType string\r\n}\r\n\r\n// Statement represents a complete QuickStatement\r\ntype Statement struct {\r\n\tSubject  Entity\r\n\tProperty Property\r\n\tObject   interface{}\r\n}\r\n",
    "size": 1198,
    "modTime": "2024-10-20T16:53:22.8837516+02:00",
    "path": "internal\\converter\\quickstatement.go"
  },
  {
    "name": "rdf.go",
    "content": "package converter\r\n\r\nimport (\r\n\t\"bufio\"\r\n\t\"bytes\"\r\n\t\"fmt\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\n// ConvertToRDF converts a QuickStatement to RDF format\r\nfunc (qsc *QuickStatementConverter) ConvertToRDF(quickstatement string) (string, error) {\r\n\tqsc.logger.Debug(i18n.GetMessage(\"ConvertToRDFStarted\"))\r\n\r\n\tstatements, err := qsc.parseQuickStatement(quickstatement)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"FailedToParseQuickStatement\"), err)\r\n\t}\r\n\r\n\tvar rdfStatements []string\r\n\tfor _, stmt := range statements {\r\n\t\trdfStmt, err := qsc.statementToRDF(stmt)\r\n\t\tif err != nil {\r\n\t\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"FailedToConvertStatementToRDF\"), err)\r\n\t\t}\r\n\t\trdfStatements = append(rdfStatements, rdfStmt)\r\n\t}\r\n\r\n\tresult := qsc.generateRDFDocument(rdfStatements)\r\n\r\n\tqsc.logger.Debug(i18n.GetMessage(\"ConvertToRDFFinished\"))\r\n\treturn result, nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) parseQuickStatementForRDF(quickstatement string) ([]Statement, error) {\tqsc.logger.Debug(i18n.GetMessage(\"ParsingQuickStatement\"))\r\n\tvar statements []Statement\r\n\tscanner := bufio.NewScanner(strings.NewReader(quickstatement))\r\n\tfor scanner.Scan() {\r\n\t\tline := scanner.Text()\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tif len(parts) \u003c 3 {\r\n\t\t\treturn nil, fmt.Errorf(i18n.GetMessage(\"InvalidQuickStatementFormat\"))\r\n\t\t}\r\n\t\tstatement := Statement{\r\n\t\t\tSubject:  Entity{ID: parts[0]},\r\n\t\t\tProperty: Property{ID: parts[1]},\r\n\t\t\tObject:   parts[2],\r\n\t\t}\r\n\t\tstatements = append(statements, statement)\r\n\t}\r\n\tif err := scanner.Err(); err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorScanningQuickStatement\"), err)\r\n\t}\r\n\treturn statements, nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) statementToRDF(statement Statement) (string, error) {\r\n\tqsc.logger.Debug(i18n.GetMessage(\"ConvertingStatementToRDF\"))\r\n\tsubjectURI := fmt.Sprintf(\"\u003c%s%s\u003e\", config.GetConfig().BaseURI, statement.Subject.ID)\r\n\tpropertyURI := fmt.Sprintf(\"\u003c%s%s\u003e\", config.GetConfig().BaseURI, statement.Property.ID)\r\n\tobjectValue := statement.Object.(string) // Type assertion, be cautious in real implementation\r\n\r\n\tvar objectRDF string\r\n\tif strings.HasPrefix(objectValue, \"Q\") {\r\n\t\t// If object is an entity\r\n\t\tobjectRDF = fmt.Sprintf(\"\u003c%s%s\u003e\", config.GetConfig().BaseURI, objectValue)\r\n\t} else {\r\n\t\t// If object is a literal\r\n\t\tobjectRDF = fmt.Sprintf(\"\\\"%s\\\"\", objectValue)\r\n\t}\r\n\r\n\treturn fmt.Sprintf(\"%s %s %s .\", subjectURI, propertyURI, objectRDF), nil\r\n}\r\n\r\nfunc (qsc *QuickStatementConverter) generateRDFDocument(rdfStatements []string) string {\r\n\tqsc.logger.Debug(i18n.GetMessage(\"GeneratingRDFDocument\"))\r\n\tvar buffer bytes.Buffer\r\n\r\n\tbuffer.WriteString(\"@prefix wd: \u003c\" + config.GetConfig().BaseURI + \"\u003e .\\n\\n\")\r\n\tfor _, stmt := range rdfStatements {\r\n\t\tbuffer.WriteString(stmt + \"\\n\")\r\n\t}\r\n\r\n\treturn buffer.String()\r\n}\r\n",
    "size": 2918,
    "modTime": "2024-10-20T16:48:49.518701+02:00",
    "path": "internal\\converter\\rdf.go"
  },
  {
    "name": "utils.go",
    "content": "package converter\r\n\r\nimport (\r\n\t\"crypto/rand\"\r\n\t\"fmt\"\r\n\t\"net/url\"\r\n\t\"strings\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\n// EscapeString escapes special characters in a string for safe use in output formats\r\nfunc EscapeString(s string) string {\r\n\treturn strings.NewReplacer(\r\n\t\t`\"`, `\\\"`,\r\n\t\t`\\`, `\\\\`,\r\n\t\t`/`, `\\/`,\r\n\t\t\"\\b\", `\\b`,\r\n\t\t\"\\f\", `\\f`,\r\n\t\t\"\\n\", `\\n`,\r\n\t\t\"\\r\", `\\r`,\r\n\t\t\"\\t\", `\\t`,\r\n\t).Replace(s)\r\n}\r\n\r\n// IsValidURI checks if a string is a valid URI\r\nfunc IsValidURI(uri string) bool {\r\n\t_, err := url.ParseRequestURI(uri)\r\n\tif err != nil {\r\n\t\tlog.Debug(i18n.GetMessage(\"InvalidURI\")) // Cette ligne est correcte car log.Debug n'attend qu'un seul argument\r\n\t\treturn false\r\n\t}\r\n\treturn true\r\n}\r\n\r\n// FormatDate formats a date string to a standard format (ISO 8601)\r\nfunc FormatDate(date string) (string, error) {\r\n\tparsedDate, err := time.Parse(time.RFC3339, date)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidDateFormat\"), err) // Ajout de l'erreur comme second argument\r\n\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrorParsingDate\"), err)\r\n\t}\r\n\treturn parsedDate.Format(time.RFC3339), nil\r\n}\r\n\r\n// GenerateUniqueID generates a unique identifier for entities without one\r\nfunc GenerateUniqueID() string {\r\n\tb := make([]byte, 16)\r\n\t_, err := rand.Read(b)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.GetMessage(\"ErrorGeneratingUniqueID\"), err)\r\n\t\treturn \"\"\r\n\t}\r\n\treturn fmt.Sprintf(\"%x\", b)\r\n}\r\n\r\n// SplitIntoChunks divides a large dataset into smaller chunks for batch processing\r\nfunc SplitIntoChunks(data []byte, chunkSize int) [][]byte {\r\n\tvar chunks [][]byte\r\n\tfor i := 0; i \u003c len(data); i += chunkSize {\r\n\t\tend := i + chunkSize\r\n\t\tif end \u003e len(data) {\r\n\t\t\tend = len(data)\r\n\t\t}\r\n\t\tchunks = append(chunks, data[i:end])\r\n\t}\r\n\treturn chunks\r\n}\r\n\r\n// MergeMaps merges multiple maps into a single map\r\nfunc MergeMaps(maps ...map[string]interface{}) map[string]interface{} {\r\n\tresult := make(map[string]interface{})\r\n\tfor _, m := range maps {\r\n\t\tfor k, v := range m {\r\n\t\t\tresult[k] = v\r\n\t\t}\r\n\t}\r\n\treturn result\r\n}\r\n\r\n// TruncateString truncates a string to a specified length, adding an ellipsis if truncated\r\nfunc TruncateString(s string, maxLength int) string {\r\n\tif len(s) \u003c= maxLength {\r\n\t\treturn s\r\n\t}\r\n\treturn s[:maxLength-3] + \"...\"\r\n}\r\n\r\n// NormalizeWhitespace removes extra whitespace from a string\r\nfunc NormalizeWhitespace(s string) string {\r\n\treturn strings.Join(strings.Fields(s), \" \")\r\n}\r\n\r\n// IsNumeric checks if a string contains only numeric characters\r\nfunc IsNumeric(s string) bool {\r\n\tfor _, r := range s {\r\n\t\tif r \u003c '0' || r \u003e '9' {\r\n\t\t\treturn false\r\n\t\t}\r\n\t}\r\n\treturn true\r\n}\r\n",
    "size": 2639,
    "modTime": "2024-10-20T14:44:56.9267011+02:00",
    "path": "internal\\converter\\utils.go"
  },
  {
    "name": "validate.go",
    "content": "package converter\r\n\r\nimport (\r\n\t\"bufio\"\r\n\t\"fmt\"\r\n\t\"regexp\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\n// ValidateQuickStatement validates a QuickStatement string\r\nfunc ValidateQuickStatement(statement string) bool {\r\n\tlog.Debug(i18n.GetMessage(\"ValidateQuickStatementStarted\"))\r\n\r\n\terr := validateQuickStatementSyntax(statement)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidQuickStatementSyntax\"), err)\r\n\t\treturn false\r\n\t}\r\n\r\n\terr = checkEntityReferences(statement)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidEntityReferences\"), err)\r\n\t\treturn false\r\n\t}\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ValidateQuickStatementFinished\"))\r\n\treturn true\r\n}\r\n\r\n// ValidateRDF validates an RDF string\r\nfunc ValidateRDF(rdf string) bool {\r\n\tlog.Debug(i18n.GetMessage(\"ValidateRDFStarted\"))\r\n\r\n\terr := validateRDFSyntax(rdf)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidRDFSyntax\"), err)\r\n\t\treturn false\r\n\t}\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ValidateRDFFinished\"))\r\n\treturn true\r\n}\r\n\r\n// ValidateOWL validates an OWL string\r\nfunc ValidateOWL(owl string) bool {\r\n\tlog.Debug(i18n.GetMessage(\"ValidateOWLStarted\"))\r\n\r\n\terr := validateOWLSyntax(owl)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.GetMessage(\"InvalidOWLSyntax\"), err)\r\n\t\treturn false\r\n\t}\r\n\r\n\tlog.Debug(i18n.GetMessage(\"ValidateOWLFinished\"))\r\n\treturn true\r\n}\r\n\r\nfunc validateQuickStatementSyntax(statement string) error {\r\n\tscanner := bufio.NewScanner(strings.NewReader(statement))\r\n\tlineNum := 0\r\n\tfor scanner.Scan() {\r\n\t\tlineNum++\r\n\t\tline := scanner.Text()\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tif len(parts) != 3 {\r\n\t\t\treturn fmt.Errorf(i18n.GetMessage(\"InvalidQuickStatementLine\"), lineNum)\r\n\t\t}\r\n\t\tif !isValidEntity(parts[0]) || !isValidProperty(parts[1]) {\r\n\t\t\treturn fmt.Errorf(i18n.GetMessage(\"InvalidEntityOrProperty\"), lineNum)\r\n\t\t}\r\n\t}\r\n\treturn scanner.Err()\r\n}\r\n\r\nfunc validateRDFSyntax(rdf string) error {\r\n\tif !strings.Contains(rdf, \"\u003crdf:RDF\") || !strings.Contains(rdf, \"\u003c/rdf:RDF\u003e\") {\r\n\t\treturn fmt.Errorf(i18n.GetMessage(\"MissingRDFTags\"))\r\n\t}\r\n\tif !strings.Contains(rdf, \"xmlns:rdf=\\\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\\\"\") {\r\n\t\treturn fmt.Errorf(i18n.GetMessage(\"MissingRDFNamespace\"))\r\n\t}\r\n\treturn nil\r\n}\r\n\r\nfunc validateOWLSyntax(owl string) error {\r\n\tif !strings.Contains(owl, \"\u003cowl:Ontology\") || !strings.Contains(owl, \"\u003c/owl:Ontology\u003e\") {\r\n\t\treturn fmt.Errorf(i18n.GetMessage(\"MissingOWLTags\"))\r\n\t}\r\n\tif !strings.Contains(owl, \"xmlns:owl=\\\"http://www.w3.org/2002/07/owl#\\\"\") {\r\n\t\treturn fmt.Errorf(i18n.GetMessage(\"MissingOWLNamespace\"))\r\n\t}\r\n\treturn nil\r\n}\r\n\r\nfunc checkEntityReferences(statement string) error {\r\n\tscanner := bufio.NewScanner(strings.NewReader(statement))\r\n\tlineNum := 0\r\n\tfor scanner.Scan() {\r\n\t\tlineNum++\r\n\t\tline := scanner.Text()\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tif !isValidEntity(parts[0]) {\r\n\t\t\treturn fmt.Errorf(i18n.GetMessage(\"InvalidEntityReference\"), parts[0], lineNum)\r\n\t\t}\r\n\t}\r\n\treturn scanner.Err()\r\n}\r\n\r\nfunc isValidEntity(entity string) bool {\r\n\treturn regexp.MustCompile(`^Q\\d+$`).MatchString(entity)\r\n}\r\n\r\nfunc isValidProperty(property string) bool {\r\n\treturn regexp.MustCompile(`^P\\d+$`).MatchString(property)\r\n}\r\n\r\nfunc isValidURI(uri string) bool {\r\n\t// This is a simplified URI validation. In a real-world scenario, you might want to use a more comprehensive validation.\r\n\treturn regexp.MustCompile(`^(http|https)://`).MatchString(uri)\r\n}\r\n",
    "size": 3409,
    "modTime": "2024-10-20T14:43:44.4003488+02:00",
    "path": "internal\\converter\\validate.go"
  },
  {
    "name": "messages.go",
    "content": "package i18n\r\n\r\nimport (\r\n\t\"reflect\"\r\n\t\"strings\"\r\n)\r\n\r\n// Messages contient tous les messages de l'application\r\nvar Messages = struct {\r\n\tRootCmdShortDesc                  string\r\n\tRootCmdLongDesc                   string\r\n\tEnrichCmdShortDesc                string\r\n\tEnrichCmdLongDesc                 string\r\n\tConfigFlagUsage                   string\r\n\tDebugFlagUsage                    string\r\n\tSilentFlagUsage                   string\r\n\tInputFlagUsage                    string\r\n\tOutputFlagUsage                   string\r\n\tFormatFlagUsage                   string\r\n\tLLMFlagUsage                      string\r\n\tLLMModelFlagUsage                 string\r\n\tPassesFlagUsage                   string\r\n\tRDFFlagUsage                      string\r\n\tOWLFlagUsage                      string\r\n\tRecursiveFlagUsage                string\r\n\tInitializingApplication           string\r\n\tStartingEnrichProcess             string\r\n\tEnrichProcessCompleted            string\r\n\tExecutingPipeline                 string\r\n\tErrorExecutingRootCmd             string\r\n\tErrorExecutingPipeline            string\r\n\tErrorCreatingPipeline             string\r\n\tErrUnsupportedModel               string\r\n\tErrAPIKeyMissing                  string\r\n\tErrTranslationFailed              string\r\n\tErrInvalidLLMType                 string\r\n\tErrContextTooLong                 string\r\n\tTranslationStarted                string\r\n\tTranslationRetry                  string\r\n\tTranslationCompleted              string\r\n\tErrCreateLogDir                   string\r\n\tErrOpenLogFile                    string\r\n\tParseStarted                      string\r\n\tParseFailed                       string\r\n\tParseCompleted                    string\r\n\tMetadataExtractionFailed          string\r\n\tPageParseFailed                   string\r\n\tTextExtractionFailed              string\r\n\tErrInvalidContent                 string\r\n\tErrTokenization                   string\r\n\tErrReadingContent                 string\r\n\tErrTokenizerInitialization        string\r\n\tErrTokenCounting                  string\r\n\tLogSegmentationStarted            string\r\n\tLogSegmentationCompleted          string\r\n\tLogContextGeneration              string\r\n\tLogMergingSegments                string\r\n\tErrReadConfigFile                 string\r\n\tErrParseConfigFile                string\r\n\tErrNoAPIKeys                      string\r\n\tStartingQuickStatementConversion  string\r\n\tQuickStatementConversionCompleted string\r\n\tStartingRDFConversion             string\r\n\tRDFConversionCompleted            string\r\n\tStartingOWLConversion             string\r\n\tOWLConversionCompleted            string\r\n\tExistingOntologyFlagUsage         string\r\n\tErrAccessInput                    string\r\n\tErrProcessingPass                 string\r\n\tErrNoInputSpecified               string\r\n\tTranslationFailed                 string\r\n\tRateLimitExceeded                 string\r\n\tStartingPipeline                  string\r\n\tStartingPass                      string\r\n\tPipelineCompleted                 string\r\n\tSegmentProcessingError            string\r\n\tErrLoadExistingOntology           string\r\n\tErrSavingResult                   string\r\n\tErrSegmentContent                 string\r\n}{\r\n\tRootCmdShortDesc: \"Ontology enrichment tool\",\r\n\tRootCmdLongDesc: `Ontology is a command-line tool for enriching ontologies from various document formats.\r\nIt supports multiple input formats and can utilize different language models for analysis.`,\r\n\tEnrichCmdShortDesc: \"Enrich an ontology from input documents\",\r\n\tEnrichCmdLongDesc: `The enrich command processes input documents to create or update an ontology.\r\nIt can handle various input formats and use different language models for analysis.`,\r\n\tConfigFlagUsage:                   \"config file (default is $HOME/.ontology.yaml)\",\r\n\tDebugFlagUsage:                    \"enable debug mode\",\r\n\tSilentFlagUsage:                   \"silent mode, only show errors\",\r\n\tInputFlagUsage:                    \"input file or directory\",\r\n\tOutputFlagUsage:                   \"output file for the enriched ontology\",\r\n\tFormatFlagUsage:                   \"input format (auto-detected if not specified)\",\r\n\tLLMFlagUsage:                      \"language model to use for analysis\",\r\n\tLLMModelFlagUsage:                 \"specific model for the chosen LLM\",\r\n\tPassesFlagUsage:                   \"number of passes for ontology enrichment\",\r\n\tRDFFlagUsage:                      \"export ontology in RDF format\",\r\n\tOWLFlagUsage:                      \"export ontology in OWL format\",\r\n\tRecursiveFlagUsage:                \"process input directory recursively\",\r\n\tInitializingApplication:           \"Initializing Ontology application\",\r\n\tStartingEnrichProcess:             \"Starting ontology enrichment process\",\r\n\tEnrichProcessCompleted:            \"Ontology enrichment process completed\",\r\n\tExecutingPipeline:                 \"Executing ontology enrichment pipeline\",\r\n\tErrorExecutingRootCmd:             \"Error executing root command\",\r\n\tErrorExecutingPipeline:            \"Error executing pipeline\",\r\n\tErrorCreatingPipeline:             \"Error creating pipeline\",\r\n\tErrUnsupportedModel:               \"unsupported model\",\r\n\tErrAPIKeyMissing:                  \"API key is missing\",\r\n\tErrTranslationFailed:              \"translation failed\",\r\n\tErrInvalidLLMType:                 \"invalid LLM type\",\r\n\tErrContextTooLong:                 \"context is too long\",\r\n\tTranslationStarted:                \"Translation started\",\r\n\tTranslationRetry:                  \"Translation retry\",\r\n\tTranslationCompleted:              \"Translation completed\",\r\n\tErrCreateLogDir:                   \"Failed to create log directory\",\r\n\tErrOpenLogFile:                    \"Failed to open log file\",\r\n\tParseStarted:                      \"Parsing started\",\r\n\tParseFailed:                       \"Parsing failed\",\r\n\tParseCompleted:                    \"Parsing completed\",\r\n\tMetadataExtractionFailed:          \"Metadata extraction failed\",\r\n\tPageParseFailed:                   \"Failed to parse page\",\r\n\tTextExtractionFailed:              \"Failed to extract text from page\",\r\n\tErrInvalidContent:                 \"invalid content\",\r\n\tErrTokenization:                   \"tokenization error\",\r\n\tErrReadingContent:                 \"error reading content\",\r\n\tErrTokenizerInitialization:        \"error initializing tokenizer\",\r\n\tErrTokenCounting:                  \"error counting tokens\",\r\n\tLogSegmentationStarted:            \"Segmentation started\",\r\n\tLogSegmentationCompleted:          \"Segmentation completed: %d segments\",\r\n\tLogContextGeneration:              \"Generating context\",\r\n\tLogMergingSegments:                \"Merging segments\",\r\n\tErrReadConfigFile:                 \"Failed to read config file: %v\",\r\n\tErrParseConfigFile:                \"Failed to parse config file: %v\",\r\n\tErrNoAPIKeys:                      \"No API keys provided for any LLM service\",\r\n\tStartingQuickStatementConversion:  \"Starting conversion to QuickStatement format\",\r\n\tQuickStatementConversionCompleted: \"QuickStatement conversion completed\",\r\n\tStartingRDFConversion:             \"Starting conversion to RDF format\",\r\n\tRDFConversionCompleted:            \"RDF conversion completed\",\r\n\tStartingOWLConversion:             \"Starting conversion to OWL format\",\r\n\tOWLConversionCompleted:            \"OWL conversion completed\",\r\n\tExistingOntologyFlagUsage:         \"path to an existing ontology file to enrich\",\r\n\tErrAccessInput:                    \"Failed to access input: %v\",\r\n\tErrProcessingPass:                 \"Error processing pass: %v\",\r\n\tErrNoInputSpecified:               \"No input specified. Please use --input flag to specify an input file or directory.\",\r\n\tTranslationFailed:                 \"Translation failed after maximum retries: %v\",\r\n\tRateLimitExceeded:                 \"Rate limit exceeded. Waiting %v before retrying.\",\r\n\tStartingPipeline:                  \"Starting pipeline execution\",\r\n\tStartingPass:                      \"Starting pass %d\",\r\n\tPipelineCompleted:                 \"Pipeline execution completed successfully\",\r\n\tSegmentProcessingError:            \"Error processing segment %d: %v\",\r\n\tErrLoadExistingOntology:           \"Failed to load existing ontology\",\r\n\tErrSavingResult:                   \"Error saving result\",\r\n\tErrSegmentContent:                 \"Failed to segment content\",\r\n}\r\n\r\n// GetMessage retourne le message correspondant à la clé donnée\r\nfunc GetMessage(key string) string {\r\n\tv := reflect.ValueOf(Messages)\r\n\tf := v.FieldByName(key)\r\n\tif !f.IsValid() {\r\n\t\treturn key\r\n\t}\r\n\treturn strings.TrimSpace(f.String())\r\n}\r\n",
    "size": 8491,
    "modTime": "2024-10-20T23:24:58.4333011+02:00",
    "path": "internal\\i18n\\messages.go"
  },
  {
    "name": "claude.go",
    "content": "// internal/llm/claude.go\r\n\r\npackage llm\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"net/http\"\r\n\t\"strings\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n)\r\n\r\n// ClaudeClient implements the Client interface for Claude\r\ntype ClaudeClient struct {\r\n\tapiKey string\r\n\tmodel  string\r\n\tclient *http.Client\r\n\tconfig *config.Config\r\n}\r\n\r\n// supportedClaudeModels defines the list of supported Claude models\r\nvar supportedClaudeModels = map[string]bool{\r\n\t\"claude-3-5-sonnet-20240620\": true,\r\n\t\"claude-3-opus-20240229\":     true,\r\n\t\"claude-3-haiku-20240307\":    true,\r\n}\r\n\r\n// NewClaudeClient creates a new Claude client\r\nfunc NewClaudeClient(apiKey string, model string) (*ClaudeClient, error) {\r\n\tlog.Debug(\"Creating new Claude client with model: %s\", model)\r\n\tif apiKey == \"\" {\r\n\t\tlog.Error(\"API key is missing for Claude client\")\r\n\t\treturn nil, ErrAPIKeyMissing\r\n\t}\r\n\r\n\tif !supportedClaudeModels[model] {\r\n\t\tlog.Error(\"Unsupported Claude model: %s\", model)\r\n\t\treturn nil, fmt.Errorf(\"%w: %s\", ErrUnsupportedModel, model)\r\n\t}\r\n\r\n\treturn \u0026ClaudeClient{\r\n\t\tapiKey: apiKey,\r\n\t\tmodel:  model,\r\n\t\tclient: \u0026http.Client{Timeout: 60 * time.Second},\r\n\t\tconfig: config.GetConfig(),\r\n\t}, nil\r\n}\r\n\r\n// Translate sends a prompt to the Claude API and returns the response\r\nfunc (c *ClaudeClient) Translate(prompt string, context string) (string, error) {\r\n\tlog.Debug(i18n.Messages.TranslationStarted, \"Claude\", c.model)\r\n\tlog.Debug(\"Prompt length: %d, Context length: %d\", len(prompt), len(context))\r\n\r\n\tvar result string\r\n\tvar err error\r\n\tmaxRetries := 5\r\n\tbaseDelay := time.Second * 10\r\n\tmaxDelay := time.Minute * 2\r\n\r\n\tfor attempt := 0; attempt \u003c maxRetries; attempt++ {\r\n\t\tlog.Debug(\"Attempt %d of %d\", attempt+1, maxRetries)\r\n\t\tresult, err = c.makeRequest(prompt, context)\r\n\t\tif err == nil {\r\n\t\t\tlog.Debug(i18n.Messages.TranslationCompleted, \"Claude\", c.model)\r\n\t\t\treturn result, nil\r\n\t\t}\r\n\r\n\t\tif !isRateLimitError(err) {\r\n\t\t\tlog.Warning(i18n.Messages.TranslationRetry, attempt+1, err)\r\n\t\t\ttime.Sleep(time.Duration(attempt+1) * time.Second)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tdelay := baseDelay * time.Duration(1\u003c\u003cuint(attempt))\r\n\t\tif delay \u003e maxDelay {\r\n\t\t\tdelay = maxDelay\r\n\t\t}\r\n\t\tlog.Warning(i18n.Messages.RateLimitExceeded, delay)\r\n\t\ttime.Sleep(delay)\r\n\t}\r\n\r\n\tlog.Error(i18n.Messages.TranslationFailed, err)\r\n\treturn \"\", fmt.Errorf(\"%w: %v\", ErrTranslationFailed, err)\r\n}\r\n\r\nfunc isRateLimitError(err error) bool {\r\n\treturn strings.Contains(err.Error(), \"rate_limit_error\")\r\n}\r\n\r\nfunc (c *ClaudeClient) makeRequest(prompt string, context string) (string, error) {\r\n\tlog.Debug(\"Making request to Claude API\")\r\n\turl := config.GetConfig().ClaudeAPIURL\r\n\r\n\trequestBody, err := json.Marshal(map[string]interface{}{\r\n\t\t\"model\": c.model,\r\n\t\t\"messages\": []map[string]string{\r\n\t\t\t{\"role\": \"user\", \"content\": prompt},\r\n\t\t},\r\n\t\t\"system\":     context,\r\n\t\t\"max_tokens\": c.config.MaxTokens,\r\n\t})\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error marshalling request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error marshalling request: %w\", err)\r\n\t}\r\n\t//log.Debug(\"Claude API Request Body : %s\", requestBody)\r\n\treq, err := http.NewRequest(\"POST\", url, bytes.NewBuffer(requestBody))\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error creating request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error creating request: %w\", err)\r\n\t}\r\n\r\n\treq.Header.Set(\"Content-Type\", \"application/json\")\r\n\treq.Header.Set(\"x-api-key\", c.apiKey)\r\n\treq.Header.Set(\"anthropic-version\", \"2023-06-01\")\r\n\r\n\tlog.Debug(\"Sending request to Claude API\")\r\n\tresp, err := c.client.Do(req)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error sending request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error sending request: %w\", err)\r\n\t}\r\n\tdefer resp.Body.Close()\r\n\r\n\tbody, err := ioutil.ReadAll(resp.Body)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error reading response: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error reading response: %w\", err)\r\n\t}\r\n\r\n\tif resp.StatusCode != http.StatusOK {\r\n\t\tlog.Error(\"API request failed with status code %d: %s\", resp.StatusCode, string(body))\r\n\t\treturn \"\", fmt.Errorf(\"API request failed with status code %d: %s\", resp.StatusCode, string(body))\r\n\t}\r\n\r\n\tvar response struct {\r\n\t\tContent []struct {\r\n\t\t\tText string `json:\"text\"`\r\n\t\t} `json:\"content\"`\r\n\t}\r\n\r\n\terr = json.Unmarshal(body, \u0026response)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error unmarshalling response: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error unmarshalling response: %w\", err)\r\n\t}\r\n\r\n\tif len(response.Content) == 0 {\r\n\t\tlog.Error(\"No content in response\")\r\n\t\treturn \"\", fmt.Errorf(\"no content in response\")\r\n\t}\r\n\r\n\t//log.Debug(\"Successfully received and parsed response from Claude API : %s\", response.Content[0].Text)\r\n\treturn response.Content[0].Text, nil\r\n}\r\n\r\n// ProcessWithPrompt processes a prompt template with the given values and sends it to the Claude API\r\nfunc (c *ClaudeClient) ProcessWithPrompt(promptTemplate *prompt.PromptTemplate, values map[string]string) (string, error) {\r\n\tlog.Debug(\"Processing prompt with Claude\")\r\n\tformattedPrompt := promptTemplate.Format(values)\r\n\tlog.Debug(\"Formatted prompt: %s\", formattedPrompt)\r\n\r\n\t// Utilisez la méthode Translate existante pour envoyer le prompt formatté\r\n\treturn c.Translate(formattedPrompt, \"\")\r\n}\r\n",
    "size": 5253,
    "modTime": "2024-10-21T23:31:01.5428332+02:00",
    "path": "internal\\llm\\claude.go"
  },
  {
    "name": "client.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n)\r\n\r\n// Client defines the interface for LLM clients\r\ntype Client interface {\r\n\t// Translate takes a prompt and context, and returns the LLM's response\r\n\tTranslate(prompt string, context string) (string, error)\r\n\tProcessWithPrompt(promptTemplate *prompt.PromptTemplate, values map[string]string) (string, error)\r\n}\r\n",
    "size": 390,
    "modTime": "2024-10-21T00:09:55.4348073+02:00",
    "path": "internal\\llm\\client.go"
  },
  {
    "name": "constants.go",
    "content": "// internal/llm/constants.go\r\n\r\npackage llm\r\n\r\nimport \"time\"\r\n\r\n\r\nconst (\r\n    MaxRetries        = 5\r\n    InitialRetryDelay = 1 * time.Second\r\n    MaxRetryDelay     = 32 * time.Second\r\n)\r\n\r\nvar ModelContextLimits = map[string]int{\r\n    \"GPT-4o\":                 8192,\r\n    \"GPT-4o mini\":            4096,\r\n    \"o1-preview\":             16384,\r\n    \"o1-mini\":                2048,\r\n    \"claude-3-5-sonnet-20240620\": 200000,\r\n    \"claude-3-opus-20240229\":     200000,\r\n    \"claude-3-haiku-20240307\":    200000,\r\n    \"llama3.2:3B\":            4096,\r\n    \"llama3.1:8B\":            4096,\r\n    \"mistral-nemo:12B\":       8192,\r\n    \"mixtral:7B\":             32768,\r\n    \"mistral:7B\":             8192,\r\n    \"mistral-small:22B\":      16384,\r\n}",
    "size": 735,
    "modTime": "2024-10-20T15:18:13.4507413+02:00",
    "path": "internal\\llm\\constants.go"
  },
  {
    "name": "errors.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"errors\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\nvar (\r\n\tErrUnsupportedModel  = errors.New(i18n.Messages.ErrUnsupportedModel)\r\n\tErrAPIKeyMissing     = errors.New(i18n.Messages.ErrAPIKeyMissing)\r\n\tErrTranslationFailed = errors.New(i18n.Messages.ErrTranslationFailed)\r\n\tErrInvalidLLMType    = errors.New(i18n.Messages.ErrInvalidLLMType)\r\n\tErrContextTooLong    = errors.New(i18n.Messages.ErrContextTooLong)\r\n)\r\n",
    "size": 449,
    "modTime": "2024-10-20T20:00:43.9609939+02:00",
    "path": "internal\\llm\\errors.go"
  },
  {
    "name": "factory.go",
    "content": "// internal/llm/factory.go\r\n\r\npackage llm\r\n\r\nimport (\r\n\t\"fmt\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n)\r\n\r\nfunc GetClient(llmType string, model string) (Client, error) {\r\n\tcfg := config.GetConfig()\r\n\r\n\tswitch llmType {\r\n\tcase \"openai\":\r\n\t\treturn NewOpenAIClient(cfg.OpenAIAPIKey, model)\r\n\tcase \"claude\":\r\n\t\treturn NewClaudeClient(cfg.ClaudeAPIKey, model)\r\n\tcase \"ollama\":\r\n\t\treturn NewOllamaClient(model)\r\n\tdefault:\r\n\t\treturn nil, fmt.Errorf(\"%w: %s\", ErrInvalidLLMType, llmType)\r\n\t}\r\n}\r\n\r\ntype contextCheckingClient struct {\r\n\tbaseClient Client\r\n\tmodel      string\r\n}\r\n\r\nfunc (c *contextCheckingClient) Translate(prompt string, context string) (string, error) {\r\n\tif err := CheckContextLength(c.model, context); err != nil {\r\n\t\treturn \"\", err\r\n\t}\r\n\treturn c.baseClient.Translate(prompt, context)\r\n}\r\n",
    "size": 813,
    "modTime": "2024-10-20T15:27:23.170757+02:00",
    "path": "internal\\llm\\factory.go"
  },
  {
    "name": "logger.go",
    "content": "// internal/llm/logger.go\r\n\r\npackage llm\r\n\r\nimport (\r\n    \"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()",
    "size": 139,
    "modTime": "2024-10-20T15:51:39.514526+02:00",
    "path": "internal\\llm\\logger.go"
  },
  {
    "name": "ollama.go",
    "content": "// internal/llm/ollama.go\r\n\r\npackage llm\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"net/http\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n)\r\n\r\n// OllamaClient implements the Client interface for Ollama\r\ntype OllamaClient struct {\r\n\tmodel  string\r\n\tclient *http.Client\r\n\tconfig *config.Config\r\n}\r\n\r\n// supportedOllamaModels defines the list of supported Ollama models\r\nvar supportedOllamaModels = map[string]bool{\r\n\t\"llama3.2:3B\":       true,\r\n\t\"llama3.1:8B\":       true,\r\n\t\"mistral-nemo:12B\":  true,\r\n\t\"mixtral:7B\":        true,\r\n\t\"mistral:7B\":        true,\r\n\t\"mistral-small:22B\": true,\r\n}\r\n\r\n// NewOllamaClient creates a new Ollama client\r\nfunc NewOllamaClient(model string) (*OllamaClient, error) {\r\n\tlog.Debug(\"Creating new Ollama client with model: %s\", model)\r\n\tif !supportedOllamaModels[model] {\r\n\t\tlog.Error(\"Unsupported Ollama model: %s\", model)\r\n\t\treturn nil, fmt.Errorf(\"%w: %s\", ErrUnsupportedModel, model)\r\n\t}\r\n\r\n\treturn \u0026OllamaClient{\r\n\t\tmodel:  model,\r\n\t\tclient: \u0026http.Client{Timeout: 30 * time.Second},\r\n\t\tconfig: config.GetConfig(),\r\n\t}, nil\r\n}\r\n\r\n// Translate sends a prompt to the Ollama API and returns the response\r\nfunc (c *OllamaClient) Translate(prompt string, context string) (string, error) {\r\n\tlog.Debug(i18n.Messages.TranslationStarted, \"Ollama\", c.model)\r\n\tlog.Debug(\"Prompt length: %d, Context length: %d\", len(prompt), len(context))\r\n\r\n\tvar result string\r\n\tvar err error\r\n\tmaxRetries := 5\r\n\tbaseDelay := time.Second\r\n\r\n\tfor attempt := 1; attempt \u003c= maxRetries; attempt++ {\r\n\t\tlog.Debug(\"Attempt %d of %d\", attempt, maxRetries)\r\n\t\tresult, err = c.makeRequest(prompt, context)\r\n\t\tif err == nil {\r\n\t\t\tlog.Info(i18n.Messages.TranslationCompleted, \"Ollama\", c.model)\r\n\t\t\tlog.Debug(\"Translation successful, result length: %d\", len(result))\r\n\t\t\treturn result, nil\r\n\t\t}\r\n\r\n\t\tlog.Warning(i18n.Messages.TranslationRetry, attempt, err)\r\n\t\ttime.Sleep(time.Duration(attempt) * baseDelay)\r\n\t}\r\n\r\n\tlog.Error(i18n.Messages.TranslationFailed, err)\r\n\treturn \"\", fmt.Errorf(\"%w: %v\", ErrTranslationFailed, err)\r\n}\r\n\r\nfunc (c *OllamaClient) makeRequest(prompt string, context string) (string, error) {\r\n\tlog.Debug(\"Making request to Ollama API\")\r\n\turl := c.config.OllamaAPIURL\r\n\r\n\trequestBody, err := json.Marshal(map[string]interface{}{\r\n\t\t\"model\":  c.model,\r\n\t\t\"prompt\": prompt,\r\n\t\t\"system\": context,\r\n\t\t\"stream\": false,\r\n\t})\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error marshalling request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error marshalling request: %w\", err)\r\n\t}\r\n\r\n\tlog.Debug(\"Ollama API Request Body: %s\", requestBody)\r\n\treq, err := http.NewRequest(\"POST\", url, bytes.NewBuffer(requestBody))\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error creating request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error creating request: %w\", err)\r\n\t}\r\n\r\n\treq.Header.Set(\"Content-Type\", \"application/json\")\r\n\r\n\tlog.Debug(\"Sending request to Ollama API\")\r\n\tresp, err := c.client.Do(req)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error sending request: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error sending request: %w\", err)\r\n\t}\r\n\tdefer resp.Body.Close()\r\n\r\n\tbody, err := ioutil.ReadAll(resp.Body)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error reading response: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error reading response: %w\", err)\r\n\t}\r\n\r\n\tif resp.StatusCode != http.StatusOK {\r\n\t\tlog.Error(\"API request failed with status code %d: %s\", resp.StatusCode, string(body))\r\n\t\treturn \"\", fmt.Errorf(\"API request failed with status code %d: %s\", resp.StatusCode, string(body))\r\n\t}\r\n\r\n\tvar response struct {\r\n\t\tResponse string `json:\"response\"`\r\n\t}\r\n\r\n\terr = json.Unmarshal(body, \u0026response)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error unmarshalling response: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error unmarshalling response: %w\", err)\r\n\t}\r\n\r\n\tlog.Debug(\"Successfully received and parsed response from Ollama API: %s\", response.Response)\r\n\treturn response.Response, nil\r\n}\r\n\r\n// ProcessWithPrompt processes a prompt template with the given values and sends it to the Ollama API\r\nfunc (c *OllamaClient) ProcessWithPrompt(promptTemplate *prompt.PromptTemplate, values map[string]string) (string, error) {\r\n\tlog.Debug(\"Processing prompt with Ollama\")\r\n\tformattedPrompt := promptTemplate.Format(values)\r\n\tlog.Debug(\"Formatted prompt: %s\", formattedPrompt)\r\n\r\n\t// Utilisez la méthode Translate existante pour envoyer le prompt formatté\r\n\treturn c.Translate(formattedPrompt, \"\")\r\n}\r\n",
    "size": 4459,
    "modTime": "2024-10-21T00:20:55.1601541+02:00",
    "path": "internal\\llm\\ollama.go"
  },
  {
    "name": "openai.go",
    "content": "// internal/llm/openai.go\r\n\r\npackage llm\r\n\r\nimport (\r\n\t\"context\"\r\n\t\"fmt\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n\t\"github.com/sashabaranov/go-openai\"\r\n)\r\n\r\n// OpenAIClient implements the Client interface for OpenAI\r\ntype OpenAIClient struct {\r\n\tclient *openai.Client\r\n\tmodel  string\r\n\tconfig *config.Config\r\n}\r\n\r\n// supportedModels defines the list of supported OpenAI models\r\nvar supportedModels = map[string]bool{\r\n\t\"GPT-4o\":      true,\r\n\t\"GPT-4o mini\": true,\r\n\t\"o1-preview\":  true,\r\n\t\"o1-mini\":     true,\r\n}\r\n\r\n// NewOpenAIClient creates a new OpenAI client\r\nfunc NewOpenAIClient(apiKey string, model string) (*OpenAIClient, error) {\r\n\tlog.Debug(\"Creating new OpenAI client with model: %s\", model)\r\n\tif apiKey == \"\" {\r\n\t\tlog.Error(\"API key is missing for OpenAI client\")\r\n\t\treturn nil, ErrAPIKeyMissing\r\n\t}\r\n\r\n\tif !supportedModels[model] {\r\n\t\tlog.Error(\"Unsupported OpenAI model: %s\", model)\r\n\t\treturn nil, fmt.Errorf(\"%w: %s\", ErrUnsupportedModel, model)\r\n\t}\r\n\r\n\tclient := openai.NewClient(apiKey)\r\n\treturn \u0026OpenAIClient{\r\n\t\tclient: client,\r\n\t\tmodel:  model,\r\n\t\tconfig: config.GetConfig(),\r\n\t}, nil\r\n}\r\n\r\n// Translate sends a prompt to the OpenAI API and returns the response\r\nfunc (c *OpenAIClient) Translate(prompt string, context string) (string, error) {\r\n\tlog.Debug(i18n.Messages.TranslationStarted, \"OpenAI\", c.model)\r\n\tlog.Debug(\"Prompt length: %d, Context length: %d\", len(prompt), len(context))\r\n\r\n\tvar result string\r\n\tvar err error\r\n\tmaxRetries := 5\r\n\tbaseDelay := time.Second\r\n\r\n\tfor attempt := 1; attempt \u003c= maxRetries; attempt++ {\r\n\t\tlog.Debug(\"Attempt %d of %d\", attempt, maxRetries)\r\n\t\tresult, err = c.makeRequest(prompt, context)\r\n\t\tif err == nil {\r\n\t\t\tlog.Info(i18n.Messages.TranslationCompleted, \"OpenAI\", c.model)\r\n\t\t\tlog.Debug(\"Translation successful, result length: %d\", len(result))\r\n\t\t\treturn result, nil\r\n\t\t}\r\n\r\n\t\tlog.Warning(i18n.Messages.TranslationRetry, attempt, err)\r\n\t\ttime.Sleep(time.Duration(attempt) * baseDelay)\r\n\t}\r\n\r\n\tlog.Error(i18n.Messages.TranslationFailed, err)\r\n\treturn \"\", fmt.Errorf(\"%w: %v\", ErrTranslationFailed, err)\r\n}\r\n\r\nfunc (c *OpenAIClient) makeRequest(prompt string, systemContext string) (string, error) {\r\n\tlog.Debug(\"Making request to OpenAI API\")\r\n\r\n\treq := openai.ChatCompletionRequest{\r\n\t\tModel: c.model,\r\n\t\tMessages: []openai.ChatCompletionMessage{\r\n\t\t\t{\r\n\t\t\t\tRole:    openai.ChatMessageRoleSystem,\r\n\t\t\t\tContent: systemContext,\r\n\t\t\t},\r\n\t\t\t{\r\n\t\t\t\tRole:    openai.ChatMessageRoleUser,\r\n\t\t\t\tContent: prompt,\r\n\t\t\t},\r\n\t\t},\r\n\t\tMaxTokens: c.config.MaxTokens,\r\n\t}\r\n\r\n\tctx := context.Background()\r\n\tlog.Debug(\"Sending request to OpenAI API\")\r\n\tresp, err := c.client.CreateChatCompletion(ctx, req)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Error creating chat completion: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error creating chat completion: %w\", err)\r\n\t}\r\n\r\n\tif len(resp.Choices) == 0 {\r\n\t\tlog.Error(\"No choices in response\")\r\n\t\treturn \"\", fmt.Errorf(\"no choices in response\")\r\n\t}\r\n\r\n\tlog.Debug(\"Successfully received and parsed response from OpenAI API\")\r\n\treturn resp.Choices[0].Message.Content, nil\r\n}\r\n\r\n// ProcessWithPrompt processes a prompt template with the given values and sends it to the OpenAI API\r\nfunc (c *OpenAIClient) ProcessWithPrompt(promptTemplate *prompt.PromptTemplate, values map[string]string) (string, error) {\r\n\tlog.Debug(\"Processing prompt with OpenAI\")\r\n\tformattedPrompt := promptTemplate.Format(values)\r\n\tlog.Debug(\"Formatted prompt: %s\", formattedPrompt)\r\n\r\n\t// Utilisez la méthode Translate existante pour envoyer le prompt formatté\r\n\treturn c.Translate(formattedPrompt, \"\")\r\n}\r\n",
    "size": 3670,
    "modTime": "2024-10-21T00:19:29.4288627+02:00",
    "path": "internal\\llm\\openai.go"
  },
  {
    "name": "utils.go",
    "content": "package llm\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/tokenizer\"\r\n)\r\n\r\nfunc CheckContextLength(model string, context string) error {\r\n\tlimit, ok := ModelContextLimits[model]\r\n\tif !ok {\r\n\t\treturn ErrUnsupportedModel\r\n\t}\r\n\r\n\ttokenCount, err := tokenizer.CountTokens(context)\r\n\tif err != nil {\r\n\t\treturn err\r\n\t}\r\n\r\n\tif tokenCount \u003e limit {\r\n\t\treturn ErrContextTooLong\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n",
    "size": 399,
    "modTime": "2024-10-20T15:16:37.2625502+02:00",
    "path": "internal\\llm\\utils.go"
  },
  {
    "name": "logger.go",
    "content": "// internal/logger/logger.go\r\n\r\npackage logger\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"io\"\r\n\t\"log\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"runtime\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\n// LogLevel represents the severity of a log message\r\ntype LogLevel int\r\n\r\nconst (\r\n\t// DebugLevel is used for detailed system operations\r\n\tDebugLevel LogLevel = iota\r\n\t// InfoLevel is used for general operational entries\r\n\tInfoLevel\r\n\t// WarningLevel is used for non-critical issues\r\n\tWarningLevel\r\n\t// ErrorLevel is used for errors that need attention\r\n\tErrorLevel\r\n)\r\n\r\nvar (\r\n\tinstance *Logger\r\n\tonce     sync.Once\r\n)\r\n\r\n// Logger handles all logging operations\r\ntype Logger struct {\r\n\tlevel  LogLevel\r\n\tlogger *log.Logger\r\n\tfile   *os.File\r\n\tmu     sync.Mutex\r\n}\r\n\r\n// GetLogger returns the singleton instance of Logger\r\nfunc GetLogger() *Logger {\r\n\tonce.Do(func() {\r\n\t\tinstance = \u0026Logger{\r\n\t\t\tlevel:  InfoLevel,\r\n\t\t\tlogger: log.New(os.Stdout, \"\", log.Ldate|log.Ltime),\r\n\t\t}\r\n\t\tinstance.setupLogFile()\r\n\t})\r\n\treturn instance\r\n}\r\n\r\nfunc (l *Logger) setupLogFile() {\r\n\tlogDir := config.GetConfig().LogDirectory\r\n\tif logDir == \"\" {\r\n\t\tlogDir = \"logs\"\r\n\t}\r\n\r\n\tif err := os.MkdirAll(logDir, 0755); err != nil {\r\n\t\tl.Error(i18n.GetMessage(\"ErrCreateLogDir\"), err)\r\n\t\treturn\r\n\t}\r\n\r\n\tlogFile := filepath.Join(logDir, fmt.Sprintf(\"ontology_%s.log\", time.Now().Format(\"2006-01-02\")))\r\n\tfile, err := os.OpenFile(logFile, os.O_CREATE|os.O_WRONLY|os.O_APPEND, 0644)\r\n\tif err != nil {\r\n\t\tl.Error(i18n.GetMessage(\"ErrOpenLogFile\"), err)\r\n\t\treturn\r\n\t}\r\n\r\n\tl.file = file\r\n\tl.logger.SetOutput(io.MultiWriter(os.Stdout, file))\r\n}\r\n\r\n// SetLevel sets the current log level\r\nfunc (l *Logger) SetLevel(level LogLevel) {\r\n\tl.mu.Lock()\r\n\tdefer l.mu.Unlock()\r\n\tl.level = level\r\n}\r\n\r\nfunc (l *Logger) log(level LogLevel, message string, args ...interface{}) {\r\n\tif level \u003c l.level {\r\n\t\treturn\r\n\t}\r\n\r\n\tl.mu.Lock()\r\n\tdefer l.mu.Unlock()\r\n\r\n\t_, file, line, _ := runtime.Caller(2)\r\n\tlevelStr := [...]string{\"DEBUG\", \"INFO\", \"WARNING\", \"ERROR\"}[level]\r\n\tlogMessage := fmt.Sprintf(\"[%s] %s:%d - %s\", levelStr, filepath.Base(file), line, fmt.Sprintf(message, args...))\r\n\tl.logger.Println(logMessage)\r\n}\r\n\r\n// Debug logs a message at DebugLevel\r\nfunc (l *Logger) Debug(format string, args ...interface{}) {\r\n    if l.GetLevel() \u003c= DebugLevel {\r\n        l.log(DebugLevel, format, args...)\r\n    }\r\n}\r\n\r\n// Info logs a message at InfoLevel\r\nfunc (l *Logger) Info(message string, args ...interface{}) {\r\n\tl.log(InfoLevel, message, args...)\r\n}\r\n\r\n// Warning logs a message at WarningLevel\r\nfunc (l *Logger) Warning(message string, args ...interface{}) {\r\n\tl.log(WarningLevel, message, args...)\r\n}\r\n\r\n// Error logs a message at ErrorLevel\r\nfunc (l *Logger) Error(message string, args ...interface{}) {\r\n\tl.log(ErrorLevel, message, args...)\r\n}\r\n\r\n// Close closes the log file\r\nfunc (l *Logger) Close() {\r\n\tif l.file != nil {\r\n\t\tl.file.Close()\r\n\t}\r\n}\r\n\r\n// UpdateProgress updates the progress on the console\r\nfunc (l *Logger) UpdateProgress(current, total int) {\r\n\tfmt.Printf(\"\\rProgress: %d/%d\", current, total)\r\n}\r\n\r\n// RotateLogs archives old log files\r\nfunc (l *Logger) RotateLogs() error {\r\n\t// Implementation of log rotation\r\n\t// This is a placeholder and should be implemented based on specific requirements\r\n\treturn nil\r\n}\r\n\r\n// ParseLevel converts a string level to LogLevel\r\nfunc ParseLevel(level string) LogLevel {\r\n\tswitch strings.ToLower(level) {\r\n\tcase \"debug\":\r\n\t\treturn DebugLevel\r\n\tcase \"info\":\r\n\t\treturn InfoLevel\r\n\tcase \"warning\":\r\n\t\treturn WarningLevel\r\n\tcase \"error\":\r\n\t\treturn ErrorLevel\r\n\tdefault:\r\n\t\treturn InfoLevel\r\n\t}\r\n}\r\n\r\n// Ajoutez cette méthode\r\nfunc (l *Logger) GetLevel() LogLevel {\r\n\tl.mu.Lock()\r\n\tdefer l.mu.Unlock()\r\n\treturn l.level\r\n}\r\n",
    "size": 3790,
    "modTime": "2024-10-20T23:45:56.7432409+02:00",
    "path": "internal\\logger\\logger.go"
  },
  {
    "name": "enrich.go",
    "content": "package ontology\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/chrlesur/Ontology/internal/pipeline\"\r\n\t\"github.com/spf13/cobra\"\r\n)\r\n\r\nvar (\r\n\toutput           string\r\n\tformat           string\r\n\tllm              string\r\n\tllmModel         string\r\n\tpasses           int\r\n\trdf              bool\r\n\towl              bool\r\n\trecursive        bool\r\n\texistingOntology string\r\n)\r\n\r\n// enrichCmd represents the enrich command\r\nvar enrichCmd = \u0026cobra.Command{\r\n\tUse:   \"enrich [input]\",\r\n\tShort: i18n.Messages.EnrichCmdShortDesc,\r\n\tLong:  i18n.Messages.EnrichCmdLongDesc,\r\n\tArgs:  cobra.ExactArgs(1),\r\n\tRunE: func(cmd *cobra.Command, args []string) error {\r\n\t\tinput := args[0]\r\n\t\tlog := logger.GetLogger()\r\n\t\tlog.Info(i18n.Messages.StartingEnrichProcess)\r\n\r\n\t\t// Utiliser le chemin absolu pour l'entrée\r\n\t\tabsInput, err := filepath.Abs(input)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"error getting absolute path: %w\", err)\r\n\t\t}\r\n\r\n\t\t// Déterminer le nom de fichier de sortie si non spécifié\r\n\t\tif output == \"\" {\r\n\t\t\toutput = generateOutputFilename(absInput, owl, rdf)\r\n\t\t} else {\r\n\t\t\t// Si output est spécifié, s'assurer qu'il est absolu\r\n\t\t\toutput, err = filepath.Abs(output)\r\n\t\t\tif err != nil {\r\n\t\t\t\treturn fmt.Errorf(\"error getting absolute path for output: %w\", err)\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tp, err := pipeline.NewPipeline()\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.Messages.ErrorCreatingPipeline, err)\r\n\t\t}\r\n\r\n\t\tp.SetProgressCallback(func(info pipeline.ProgressInfo) {\r\n\t\t\tswitch info.CurrentStep {\r\n\t\t\tcase \"Starting Pass\":\r\n\t\t\t\tlog.Info(\"Starting pass %d of %d\", info.CurrentPass, info.TotalPasses)\r\n\t\t\tcase \"Segmenting\":\r\n\t\t\t\tlog.Info(\"Segmenting input into %d parts\", info.TotalSegments)\r\n\t\t\tcase \"Processing Segment\":\r\n\t\t\t\tlog.Debug(\"Processing segment %d of %d\", info.ProcessedSegments, info.TotalSegments)\r\n\t\t\t}\r\n\t\t})\r\n\r\n\t\terr = p.ExecutePipeline(absInput, output, passes, existingOntology)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.Messages.ErrorExecutingPipeline, err)\r\n\t\t}\r\n\r\n\t\tlog.Info(i18n.Messages.EnrichProcessCompleted)\r\n\t\tlog.Info(\"File processed: %s, output: %s\", absInput, output)\r\n\t\treturn nil\r\n\t},\r\n}\r\n\r\nfunc init() {\r\n\trootCmd.AddCommand(enrichCmd)\r\n\r\n\tenrichCmd.Flags().StringVar(\u0026output, \"output\", \"\", i18n.Messages.OutputFlagUsage)\r\n\tenrichCmd.Flags().StringVar(\u0026format, \"format\", \"\", i18n.Messages.FormatFlagUsage)\r\n\tenrichCmd.Flags().StringVar(\u0026llm, \"llm\", \"\", i18n.Messages.LLMFlagUsage)\r\n\tenrichCmd.Flags().StringVar(\u0026llmModel, \"llm-model\", \"\", i18n.Messages.LLMModelFlagUsage)\r\n\tenrichCmd.Flags().IntVar(\u0026passes, \"passes\", 1, i18n.Messages.PassesFlagUsage)\r\n\tenrichCmd.Flags().BoolVar(\u0026rdf, \"rdf\", false, i18n.Messages.RDFFlagUsage)\r\n\tenrichCmd.Flags().BoolVar(\u0026owl, \"owl\", false, i18n.Messages.OWLFlagUsage)\r\n\tenrichCmd.Flags().BoolVar(\u0026recursive, \"recursive\", false, i18n.Messages.RecursiveFlagUsage)\r\n\tenrichCmd.Flags().StringVar(\u0026existingOntology, \"existing-ontology\", \"\", i18n.Messages.ExistingOntologyFlagUsage)\r\n}\r\n\r\nfunc processDirectory(dirPath string) error {\r\n\treturn filepath.Walk(dirPath, func(path string, info os.FileInfo, err error) error {\r\n\t\tif err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t\tif info.IsDir() {\r\n\t\t\tif !recursive \u0026\u0026 path != dirPath {\r\n\t\t\t\treturn filepath.SkipDir\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t}\r\n\t\treturn processFile(path)\r\n\t})\r\n}\r\n\r\nfunc processFile(filePath string) error {\r\n\toutputPath := output\r\n\tif outputPath == \"\" {\r\n\t\t// Générer le nom de fichier de sortie dans le même répertoire que le fichier d'entrée\r\n\t\tdir := filepath.Dir(filePath)\r\n\t\tbaseName := filepath.Base(filePath)\r\n\t\tbaseName = strings.TrimSuffix(baseName, filepath.Ext(baseName))\r\n\r\n\t\tvar extension string\r\n\t\tif owl {\r\n\t\t\textension = \".owl\"\r\n\t\t} else if rdf {\r\n\t\t\textension = \".rdf\"\r\n\t\t} else {\r\n\t\t\textension = \".tsv\"\r\n\t\t}\r\n\r\n\t\toutputPath = filepath.Join(dir, baseName+extension)\r\n\t}\r\n\r\n\tp, err := pipeline.NewPipeline()\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.Messages.ErrorCreatingPipeline, err)\r\n\t}\r\n\r\n\t// Passer le chemin de sortie à ExecutePipeline\r\n\terr = p.ExecutePipeline(filePath, outputPath, passes, existingOntology)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.Messages.ErrorExecutingPipeline, err)\r\n\t}\r\n\r\n\tlog.Info(fmt.Sprintf(\"File processed: %s, output: %s\", filePath, outputPath))\r\n\treturn nil\r\n}\r\n\r\nfunc generateOutputFilename(input string, owl, rdf bool) string {\r\n\tdir := filepath.Dir(input)\r\n\tbaseName := filepath.Base(input)\r\n\tbaseName = strings.TrimSuffix(baseName, filepath.Ext(baseName))\r\n\r\n\tvar extension string\r\n\tif owl {\r\n\t\textension = \".owl\"\r\n\t} else if rdf {\r\n\t\textension = \".rdf\"\r\n\t} else {\r\n\t\textension = \".tsv\"\r\n\t}\r\n\r\n\treturn filepath.Join(dir, baseName+extension)\r\n}\r\n",
    "size": 4788,
    "modTime": "2024-10-21T23:38:39.40359+02:00",
    "path": "internal\\ontology\\enrich.go"
  },
  {
    "name": "logger.go",
    "content": "package ontology // ou le nom du package approprié\r\n\r\nimport (\r\n    \"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()",
    "size": 150,
    "modTime": "2024-10-20T17:29:08.5359463+02:00",
    "path": "internal\\ontology\\logger.go"
  },
  {
    "name": "root.go",
    "content": "package ontology\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"os\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/spf13/cobra\"\r\n)\r\n\r\nvar (\r\n\tcfgFile string\r\n\tdebug   bool\r\n\tsilent  bool\r\n)\r\n\r\n// rootCmd represents the base command when called without any subcommands\r\nvar rootCmd = \u0026cobra.Command{\r\n\tUse:   \"ontology\",\r\n\tShort: i18n.GetMessage(\"RootCmdShortDesc\"),\r\n\tLong:  i18n.GetMessage(\"RootCmdLongDesc\"),\r\n}\r\n\r\n// Execute adds all child commands to the root command and sets flags appropriately.\r\n// This is called by main.main(). It only needs to happen once to the rootCmd.\r\nfunc Execute() {\r\n\terr := rootCmd.Execute()\r\n\tif err != nil {\r\n\t\tos.Exit(1)\r\n\t}\r\n}\r\n\r\nfunc init() {\r\n\tcobra.OnInitialize(initConfig)\r\n\r\n\trootCmd.PersistentFlags().StringVar(\u0026cfgFile, \"config\", \"\", i18n.GetMessage(\"ConfigFlagUsage\"))\r\n\trootCmd.PersistentFlags().BoolVar(\u0026debug, \"debug\", false, i18n.GetMessage(\"DebugFlagUsage\"))\r\n\trootCmd.PersistentFlags().BoolVar(\u0026silent, \"silent\", false, i18n.GetMessage(\"SilentFlagUsage\"))\r\n\r\n\trootCmd.Run = rootCmd.HelpFunc()\r\n}\r\n\r\n// initConfig reads in config file and ENV variables if set.\r\nfunc initConfig() {\r\n\tif cfgFile != \"\" {\r\n\t\t// Set the config file path if specified\r\n\t\tos.Setenv(\"ONTOLOGY_CONFIG_PATH\", cfgFile)\r\n\t}\r\n\r\n\t// This will load the config from file and environment variables\r\n\tcfg := config.GetConfig()\r\n\r\n\t// Validate the config\r\n\tif err := cfg.ValidateConfig(); err != nil {\r\n\t\tfmt.Printf(\"Error in configuration: %v\\n\", err)\r\n\t\tos.Exit(1)\r\n\t}\r\n\r\n\tlog := logger.GetLogger()\r\n\r\n\t// Initialize logger based on debug and silent flags\r\n\tif debug {\r\n\t\tlog.SetLevel(logger.DebugLevel)\r\n\t} else if silent {\r\n\t\tlog.SetLevel(logger.ErrorLevel)\r\n\t} else {\r\n\t\tlogLevel := logger.ParseLevel(cfg.LogLevel)\r\n\t\tlog.SetLevel(logLevel)\r\n\t}\r\n\r\n\tlog.Info(i18n.GetMessage(\"InitializingApplication\"))\r\n}\r\n",
    "size": 1919,
    "modTime": "2024-10-21T20:31:04.012075+02:00",
    "path": "internal\\ontology\\root.go"
  },
  {
    "name": "docx.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"archive/zip\"\r\n\t\"encoding/xml\"\r\n\t\"io/ioutil\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\nfunc init() {\r\n\tRegisterParser(\".docx\", NewDOCXParser)\r\n}\r\n\r\n// DOCXParser implémente l'interface Parser pour les fichiers DOCX\r\ntype DOCXParser struct {\r\n\tmetadata map[string]string\r\n}\r\n\r\n// NewDOCXParser crée une nouvelle instance de DOCXParser\r\nfunc NewDOCXParser() Parser {\r\n\treturn \u0026DOCXParser{\r\n\t\tmetadata: make(map[string]string),\r\n\t}\r\n}\r\n\r\n// Parse extrait le contenu textuel d'un fichier DOCX\r\nfunc (p *DOCXParser) Parse(path string) ([]byte, error) {\r\n\tlog.Debug(i18n.Messages.ParseStarted, \"DOCX\", path)\r\n\r\n\treader, err := zip.OpenReader(path)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.Messages.ParseFailed, \"DOCX\", path, err)\r\n\t\treturn nil, err\r\n\t}\r\n\tdefer reader.Close()\r\n\r\n\tvar textContent strings.Builder\r\n\tfor _, file := range reader.File {\r\n\t\tif file.Name == \"word/document.xml\" {\r\n\t\t\trc, err := file.Open()\r\n\t\t\tif err != nil {\r\n\t\t\t\tlog.Error(i18n.Messages.ParseFailed, \"DOCX\", path, err)\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\t\t\tdefer rc.Close()\r\n\r\n\t\t\tcontent, err := ioutil.ReadAll(rc)\r\n\t\t\tif err != nil {\r\n\t\t\t\tlog.Error(i18n.Messages.ParseFailed, \"DOCX\", path, err)\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\r\n\t\t\tvar document struct {\r\n\t\t\t\tBody struct {\r\n\t\t\t\t\tParagraphs []struct {\r\n\t\t\t\t\t\tRuns []struct {\r\n\t\t\t\t\t\t\tText string `xml:\"t\"`\r\n\t\t\t\t\t\t} `xml:\"r\"`\r\n\t\t\t\t\t} `xml:\"p\"`\r\n\t\t\t\t}\r\n\t\t\t}\r\n\r\n\t\t\terr = xml.Unmarshal(content, \u0026document)\r\n\t\t\tif err != nil {\r\n\t\t\t\tlog.Error(i18n.Messages.ParseFailed, \"DOCX\", path, err)\r\n\t\t\t\treturn nil, err\r\n\t\t\t}\r\n\r\n\t\t\tfor _, paragraph := range document.Body.Paragraphs {\r\n\t\t\t\tfor _, run := range paragraph.Runs {\r\n\t\t\t\t\ttextContent.WriteString(run.Text)\r\n\t\t\t\t}\r\n\t\t\t\ttextContent.WriteString(\"\\n\")\r\n\t\t\t}\r\n\t\t\tbreak\r\n\t\t}\r\n\t}\r\n\r\n\tp.extractMetadata(reader)\r\n\tlog.Info(i18n.Messages.ParseCompleted, \"DOCX\", path)\r\n\treturn []byte(textContent.String()), nil\r\n}\r\n\r\n// GetMetadata retourne les métadonnées du fichier DOCX\r\nfunc (p *DOCXParser) GetMetadata() map[string]string {\r\n\treturn p.metadata\r\n}\r\n\r\n// extractMetadata extrait les métadonnées du DOCX\r\nfunc (p *DOCXParser) extractMetadata(reader *zip.ReadCloser) {\r\n\tfor _, file := range reader.File {\r\n\t\tif file.Name == \"docProps/core.xml\" {\r\n\t\t\trc, err := file.Open()\r\n\t\t\tif err != nil {\r\n\t\t\t\tlog.Warning(i18n.Messages.MetadataExtractionFailed, \"DOCX\", err)\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t\tdefer rc.Close()\r\n\r\n\t\t\tcontent, err := ioutil.ReadAll(rc)\r\n\t\t\tif err != nil {\r\n\t\t\t\tlog.Warning(i18n.Messages.MetadataExtractionFailed, \"DOCX\", err)\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\r\n\t\t\tvar coreProps struct {\r\n\t\t\t\tTitle          string `xml:\"title\"`\r\n\t\t\t\tSubject        string `xml:\"subject\"`\r\n\t\t\t\tCreator        string `xml:\"creator\"`\r\n\t\t\t\tKeywords       string `xml:\"keywords\"`\r\n\t\t\t\tDescription    string `xml:\"description\"`\r\n\t\t\t\tLastModifiedBy string `xml:\"lastModifiedBy\"`\r\n\t\t\t\tRevision       string `xml:\"revision\"`\r\n\t\t\t}\r\n\r\n\t\t\terr = xml.Unmarshal(content, \u0026coreProps)\r\n\t\t\tif err != nil {\r\n\t\t\t\tlog.Warning(i18n.Messages.MetadataExtractionFailed, \"DOCX\", err)\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\r\n\t\t\tp.metadata[\"title\"] = coreProps.Title\r\n\t\t\tp.metadata[\"subject\"] = coreProps.Subject\r\n\t\t\tp.metadata[\"creator\"] = coreProps.Creator\r\n\t\t\tp.metadata[\"keywords\"] = coreProps.Keywords\r\n\t\t\tp.metadata[\"description\"] = coreProps.Description\r\n\t\t\tp.metadata[\"lastModifiedBy\"] = coreProps.LastModifiedBy\r\n\t\t\tp.metadata[\"revision\"] = coreProps.Revision\r\n\t\t}\r\n\t}\r\n}\r\n",
    "size": 3415,
    "modTime": "2024-10-20T20:00:14.2181355+02:00",
    "path": "internal\\parser\\docx.go"
  },
  {
    "name": "html.go",
    "content": "package parser\r\n\r\nimport (\r\n    \"io/ioutil\"\r\n    \"strings\"\r\n\r\n    \"golang.org/x/net/html\"\r\n    \"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\nfunc init() {\r\n    RegisterParser(\".html\", NewHTMLParser)\r\n}\r\n\r\n// HTMLParser implémente l'interface Parser pour les fichiers HTML\r\ntype HTMLParser struct {\r\n    metadata map[string]string\r\n}\r\n\r\n// NewHTMLParser crée une nouvelle instance de HTMLParser\r\nfunc NewHTMLParser() Parser {\r\n    return \u0026HTMLParser{\r\n        metadata: make(map[string]string),\r\n    }\r\n}\r\n\r\n// Parse extrait le contenu textuel d'un fichier HTML\r\nfunc (p *HTMLParser) Parse(path string) ([]byte, error) {\r\n    log.Debug(i18n.Messages.ParseStarted, \"HTML\", path)\r\n    content, err := ioutil.ReadFile(path)\r\n    if err != nil {\r\n        log.Error(i18n.Messages.ParseFailed, \"HTML\", path, err)\r\n        return nil, err\r\n    }\r\n\r\n    doc, err := html.Parse(strings.NewReader(string(content)))\r\n    if err != nil {\r\n        log.Error(i18n.Messages.ParseFailed, \"HTML\", path, err)\r\n        return nil, err\r\n    }\r\n\r\n    var textContent strings.Builder\r\n    var extractText func(*html.Node)\r\n    extractText = func(n *html.Node) {\r\n        if n.Type == html.TextNode {\r\n            textContent.WriteString(n.Data)\r\n        }\r\n        for c := n.FirstChild; c != nil; c = c.NextSibling {\r\n            extractText(c)\r\n        }\r\n    }\r\n    extractText(doc)\r\n\r\n    p.extractMetadata(doc)\r\n    log.Info(i18n.Messages.ParseCompleted, \"HTML\", path)\r\n    return []byte(textContent.String()), nil\r\n}\r\n\r\n// GetMetadata retourne les métadonnées du fichier HTML\r\nfunc (p *HTMLParser) GetMetadata() map[string]string {\r\n    return p.metadata\r\n}\r\n\r\n// extractMetadata extrait les métadonnées du HTML\r\nfunc (p *HTMLParser) extractMetadata(doc *html.Node) {\r\n    var f func(*html.Node)\r\n    f = func(n *html.Node) {\r\n        if n.Type == html.ElementNode \u0026\u0026 n.Data == \"meta\" {\r\n            var name, content string\r\n            for _, a := range n.Attr {\r\n                if a.Key == \"name\" {\r\n                    name = a.Val\r\n                } else if a.Key == \"content\" {\r\n                    content = a.Val\r\n                }\r\n            }\r\n            if name != \"\" \u0026\u0026 content != \"\" {\r\n                p.metadata[name] = content\r\n            }\r\n        }\r\n        for c := n.FirstChild; c != nil; c = c.NextSibling {\r\n            f(c)\r\n        }\r\n    }\r\n    f(doc)\r\n}",
    "size": 2378,
    "modTime": "2024-10-20T20:00:18.1246764+02:00",
    "path": "internal\\parser\\html.go"
  },
  {
    "name": "logger.go",
    "content": "// internal/parser/logger.go\r\n\r\npackage parser\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()\r\n",
    "size": 144,
    "modTime": "2024-10-20T15:59:02.1149669+02:00",
    "path": "internal\\parser\\logger.go"
  },
  {
    "name": "markdown.go",
    "content": "package parser\r\n\r\nimport (\r\n    \"io/ioutil\"\r\n    \"os\"\r\n    \"path/filepath\"\r\n\t\"fmt\"\r\n    \"time\"\r\n\r\n    \"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\nfunc init() {\r\n    RegisterParser(\".md\", NewMarkdownParser)\r\n}\r\n\r\n// MarkdownParser implémente l'interface Parser pour les fichiers Markdown\r\ntype MarkdownParser struct {\r\n    metadata map[string]string\r\n}\r\n\r\n// NewMarkdownParser crée une nouvelle instance de MarkdownParser\r\nfunc NewMarkdownParser() Parser {\r\n    return \u0026MarkdownParser{\r\n        metadata: make(map[string]string),\r\n    }\r\n}\r\n\r\n// Parse lit le contenu d'un fichier Markdown\r\nfunc (p *MarkdownParser) Parse(path string) ([]byte, error) {\r\n    log.Debug(i18n.Messages.ParseStarted, \"Markdown\", path)\r\n    content, err := ioutil.ReadFile(path)\r\n    if err != nil {\r\n        log.Error(i18n.Messages.ParseFailed, \"Markdown\", path, err)\r\n        return nil, err\r\n    }\r\n    p.extractMetadata(path)\r\n    log.Info(i18n.Messages.ParseCompleted, \"Markdown\", path)\r\n    return content, nil\r\n}\r\n\r\n// GetMetadata retourne les métadonnées du fichier Markdown\r\nfunc (p *MarkdownParser) GetMetadata() map[string]string {\r\n    return p.metadata\r\n}\r\n\r\n// extractMetadata extrait les métadonnées basiques du fichier\r\nfunc (p *MarkdownParser) extractMetadata(path string) {\r\n    info, err := os.Stat(path)\r\n    if err != nil {\r\n        log.Warning(i18n.Messages.MetadataExtractionFailed, path, err)\r\n        return\r\n    }\r\n    p.metadata[\"filename\"] = filepath.Base(path)\r\n    p.metadata[\"size\"] = fmt.Sprintf(\"%d\", info.Size())\r\n    p.metadata[\"modified\"] = info.ModTime().Format(time.RFC3339)\r\n}",
    "size": 1604,
    "modTime": "2024-10-20T20:00:25.3760497+02:00",
    "path": "internal\\parser\\markdown.go"
  },
  {
    "name": "parser.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n)\r\n\r\n// Parser définit l'interface pour tous les analyseurs de documents\r\ntype Parser interface {\r\n\t// Parse prend le chemin d'un fichier et retourne son contenu en bytes\r\n\tParse(path string) ([]byte, error)\r\n\t// GetMetadata retourne les métadonnées du document sous forme de map\r\n\tGetMetadata() map[string]string\r\n}\r\n\r\n// FormatParser est une fonction qui crée un Parser spécifique à un format\r\ntype FormatParser func() Parser\r\n\r\n// formatParsers stocke les fonctions de création de Parser pour chaque format supporté\r\nvar formatParsers = make(map[string]FormatParser)\r\n\r\n// RegisterParser enregistre un nouveau parser pour un format donné\r\nfunc RegisterParser(format string, parser FormatParser) {\r\n\tformatParsers[format] = parser\r\n}\r\n\r\n// GetParser retourne le parser approprié basé sur le format spécifié\r\nfunc GetParser(format string) (Parser, error) {\r\n\tparserFunc, ok := formatParsers[format]\r\n\tif !ok {\r\n\t\treturn nil, fmt.Errorf(\"unsupported format: %s\", format)\r\n\t}\r\n\treturn parserFunc(), nil\r\n}\r\n\r\n// ParseDirectory parcourt un répertoire et parse tous les fichiers supportés\r\nfunc ParseDirectory(path string, recursive bool) ([][]byte, error) {\r\n\tvar results [][]byte\r\n\terr := filepath.Walk(path, func(filePath string, info os.FileInfo, err error) error {\r\n\t\tif err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t\tif info.IsDir() {\r\n\t\t\tif !recursive \u0026\u0026 filePath != path {\r\n\t\t\t\treturn filepath.SkipDir\r\n\t\t\t}\r\n\t\t\treturn nil\r\n\t\t}\r\n\t\text := strings.ToLower(filepath.Ext(filePath))\r\n\t\tparser, err := GetParser(ext)\r\n\t\tif err != nil {\r\n\t\t\treturn nil // Skip unsupported files\r\n\t\t}\r\n\t\tcontent, err := parser.Parse(filePath)\r\n\t\tif err != nil {\r\n\t\t\tlog.Warning(\"Failed to parse file: %s, error: %v\", filePath, err)\r\n\t\t\treturn nil\r\n\t\t}\r\n\t\tresults = append(results, content)\r\n\t\treturn nil\r\n\t})\r\n\treturn results, err\r\n}\r\n",
    "size": 1892,
    "modTime": "2024-10-20T16:02:54.2283253+02:00",
    "path": "internal\\parser\\parser.go"
  },
  {
    "name": "parser_test.go",
    "content": "package parser\r\n\r\nimport (\r\n    \"os\"\r\n    \"path/filepath\"\r\n    \"testing\"\r\n)\r\n\r\nfunc TestParsers(t *testing.T) {\r\n    testCases := []struct {\r\n        format string\r\n        content string\r\n    }{\r\n        {\".txt\", \"This is a test text file.\"},\r\n        {\".md\", \"# This is a Markdown file\\n\\nWith some content.\"},\r\n        {\".html\", \"\u003chtml\u003e\u003cbody\u003eThis is an HTML file\u003c/body\u003e\u003c/html\u003e\"},\r\n    }\r\n\r\n    for _, tc := range testCases {\r\n        t.Run(tc.format, func(t *testing.T) {\r\n            tempFile, err := createTempFile(tc.format, tc.content)\r\n            if err != nil {\r\n                t.Fatalf(\"Failed to create temp file: %v\", err)\r\n            }\r\n            defer os.Remove(tempFile)\r\n\r\n            parser, err := GetParser(tc.format)\r\n            if err != nil {\r\n                t.Fatalf(\"Failed to get parser: %v\", err)\r\n            }\r\n\r\n            content, err := parser.Parse(tempFile)\r\n            if err != nil {\r\n                t.Fatalf(\"Failed to parse file: %v\", err)\r\n            }\r\n\r\n            if string(content) != tc.content {\r\n                t.Errorf(\"Expected content %s, got %s\", tc.content, string(content))\r\n            }\r\n\r\n            metadata := parser.GetMetadata()\r\n            if len(metadata) == 0 {\r\n                t.Error(\"Expected non-empty metadata\")\r\n            }\r\n        })\r\n    }\r\n}\r\n\r\nfunc TestParseDirectory(t *testing.T) {\r\n    tempDir, err := os.MkdirTemp(\"\", \"parser_test\")\r\n    if err != nil {\r\n        t.Fatalf(\"Failed to create temp directory: %v\", err)\r\n    }\r\n    defer os.RemoveAll(tempDir)\r\n\r\n    files := []struct {\r\n        name string\r\n        content string\r\n    }{\r\n        {\"test1.txt\", \"Text file 1\"},\r\n        {\"test2.md\", \"# Markdown file\"},\r\n        {\"test3.html\", \"\u003chtml\u003e\u003cbody\u003eHTML file\u003c/body\u003e\u003c/html\u003e\"},\r\n    }\r\n\r\n    for _, f := range files {\r\n        err := os.WriteFile(filepath.Join(tempDir, f.name), []byte(f.content), 0644)\r\n        if err != nil {\r\n            t.Fatalf(\"Failed to create test file: %v\", err)\r\n        }\r\n    }\r\n\r\n    results, err := ParseDirectory(tempDir, false)\r\n    if err != nil {\r\n        t.Fatalf(\"ParseDirectory failed: %v\", err)\r\n    }\r\n\r\n    if len(results) != len(files) {\r\n        t.Errorf(\"Expected %d results, got %d\", len(files), len(results))\r\n    }\r\n}\r\n\r\nfunc createTempFile(ext, content string) (string, error) {\r\n    tmpfile, err := os.CreateTemp(\"\", \"test*\"+ext)\r\n    if err != nil {\r\n        return \"\", err\r\n    }\r\n    defer tmpfile.Close()\r\n\r\n    if _, err := tmpfile.Write([]byte(content)); err != nil {\r\n        return \"\", err\r\n    }\r\n\r\n    return tmpfile.Name(), nil\r\n}",
    "size": 2588,
    "modTime": "2024-10-20T12:32:19.3183805+02:00",
    "path": "internal\\parser\\parser_test.go"
  },
  {
    "name": "pdf.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"fmt\"\r\n\t\"io\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/ledongthuc/pdf\"\r\n)\r\n\r\nfunc init() {\r\n\tlog.Debug(\"Registering PDF parser\")\r\n\tRegisterParser(\".pdf\", NewPDFParser)\r\n}\r\n\r\ntype PDFParser struct {\r\n\tmetadata map[string]string\r\n}\r\n\r\nfunc NewPDFParser() Parser {\r\n\tlog.Debug(\"Creating new PDF parser\")\r\n\treturn \u0026PDFParser{\r\n\t\tmetadata: make(map[string]string),\r\n\t}\r\n}\r\n\r\nfunc (p *PDFParser) Parse(path string) ([]byte, error) {\r\n\tlog.Debug(\"Parsing PDF file: %s\", path)\r\n\tcontent, err := ParsePDF(path)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\tp.extractMetadata(path)\r\n\treturn content, nil\r\n}\r\n\r\nfunc (p *PDFParser) GetMetadata() map[string]string {\r\n\treturn p.metadata\r\n}\r\n\r\nfunc (p *PDFParser) extractMetadata(path string) {\r\n\tlog.Debug(\"Extracting metadata from PDF file: %s\", path)\r\n\tf, r, err := pdf.Open(path)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to open PDF: %v\", err)\r\n\t\treturn\r\n\t}\r\n\tdefer f.Close()\r\n\r\n\t// Extract basic information\r\n\tp.metadata[\"PageCount\"] = fmt.Sprintf(\"%d\", r.NumPage())\r\n\r\n\tlog.Debug(\"Extracted %d metadata items\", len(p.metadata))\r\n}\r\n\r\n// ParsePDF reads a PDF file and returns its content as a byte slice.\r\nfunc ParsePDF(path string) ([]byte, error) {\r\n\tlog.Debug(i18n.Messages.ParseStarted)\r\n\r\n\tf, r, err := pdf.Open(path)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.Messages.ParseFailed, err)\r\n\t\treturn nil, fmt.Errorf(\"failed to open PDF: %w\", err)\r\n\t}\r\n\tdefer f.Close()\r\n\r\n\tvar buf bytes.Buffer\r\n\tb, err := r.GetPlainText()\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to get plain text: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to get plain text: %w\", err)\r\n\t}\r\n\r\n\t_, err = io.Copy(\u0026buf, b)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to read content: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to read content: %w\", err)\r\n\t}\r\n\r\n\tcontent := buf.Bytes()\r\n\tlog.Info(i18n.Messages.ParseCompleted)\r\n\tlog.Debug(\"Total extracted content length: %d characters\", len(content))\r\n\treturn content, nil\r\n}\r\n",
    "size": 1980,
    "modTime": "2024-10-21T00:58:53.4312994+02:00",
    "path": "internal\\parser\\pdf.go"
  },
  {
    "name": "text.go",
    "content": "package parser\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n)\r\n\r\nfunc init() {\r\n\tRegisterParser(\".txt\", NewTextParser)\r\n}\r\n\r\n// TextParser implémente l'interface Parser pour les fichiers texte\r\ntype TextParser struct {\r\n\tmetadata map[string]string\r\n}\r\n\r\n// NewTextParser crée une nouvelle instance de TextParser\r\nfunc NewTextParser() Parser {\r\n\treturn \u0026TextParser{\r\n\t\tmetadata: make(map[string]string),\r\n\t}\r\n}\r\n\r\n// Parse lit le contenu d'un fichier texte\r\nfunc (p *TextParser) Parse(path string) ([]byte, error) {\r\n\tlog.Debug(i18n.Messages.ParseStarted, \"text\", path)\r\n\tcontent, err := ioutil.ReadFile(path)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.Messages.ParseFailed, \"text\", path, err)\r\n\t\treturn nil, err\r\n\t}\r\n\tp.extractMetadata(path)\r\n\tlog.Info(i18n.Messages.ParseCompleted, \"text\", path)\r\n\treturn content, nil\r\n}\r\n\r\n// GetMetadata retourne les métadonnées du fichier texte\r\nfunc (p *TextParser) GetMetadata() map[string]string {\r\n\treturn p.metadata\r\n}\r\n\r\n// extractMetadata extrait les métadonnées basiques du fichier\r\nfunc (p *TextParser) extractMetadata(path string) {\r\n\tinfo, err := os.Stat(path)\r\n\tif err != nil {\r\n\t\tlog.Warning(i18n.Messages.MetadataExtractionFailed, path, err)\r\n\t\treturn\r\n\t}\r\n\tp.metadata[\"filename\"] = filepath.Base(path)\r\n\tp.metadata[\"size\"] = fmt.Sprintf(\"%d\", info.Size())\r\n\tp.metadata[\"modified\"] = info.ModTime().Format(time.RFC3339)\r\n}\r\n",
    "size": 1447,
    "modTime": "2024-10-20T20:00:34.5625128+02:00",
    "path": "internal\\parser\\text.go"
  },
  {
    "name": "pipeline.go",
    "content": "package pipeline\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"os\"\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/converter\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/llm\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/chrlesur/Ontology/internal/parser\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n\t\"github.com/chrlesur/Ontology/internal/segmenter\"\r\n\r\n\t\"github.com/pkoukk/tiktoken-go\"\r\n)\r\n\r\ntype ProgressInfo struct {\r\n\tCurrentPass       int\r\n\tTotalPasses       int\r\n\tCurrentStep       string\r\n\tTotalSegments     int\r\n\tProcessedSegments int\r\n}\r\n\r\ntype ProgressCallback func(ProgressInfo)\r\n\r\n// Pipeline represents the main processing pipeline\r\ntype Pipeline struct {\r\n\tconfig           *config.Config\r\n\tlogger           *logger.Logger\r\n\tllm              llm.Client\r\n\tprogressCallback ProgressCallback\r\n}\r\n\r\n// NewPipeline creates a new instance of the processing pipeline\r\nfunc NewPipeline() (*Pipeline, error) {\r\n\tcfg := config.GetConfig()\r\n\tlog := logger.GetLogger()\r\n\r\n\tclient, err := llm.GetClient(cfg.DefaultLLM, cfg.DefaultModel)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrInitLLMClient\"), err)\r\n\t}\r\n\r\n\treturn \u0026Pipeline{\r\n\t\tconfig: cfg,\r\n\t\tlogger: log,\r\n\t\tllm:    client,\r\n\t}, nil\r\n}\r\n\r\n// SetProgressCallback sets the callback function for progress updates\r\nfunc (p *Pipeline) SetProgressCallback(callback ProgressCallback) {\r\n\tp.progressCallback = callback\r\n}\r\n\r\n// ExecutePipeline orchestrates the entire workflow\r\nfunc (p *Pipeline) ExecutePipeline(input string, output string, passes int, existingOntology string) error {\r\n\tp.logger.Info(i18n.GetMessage(\"StartingPipeline\"))\r\n\tp.logger.Debug(\"Input: %s, Output: %s, Passes: %d, Existing Ontology: %s\", input, output, passes, existingOntology)\r\n\r\n\tvar result string\r\n\tvar err error\r\n\r\n\t// Initialiser le tokenizer\r\n\ttke, err := tiktoken.GetEncoding(\"cl100k_base\")\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to initialize tokenizer: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to initialize tokenizer: %w\", err)\r\n\t}\r\n\r\n\tif existingOntology != \"\" {\r\n\t\tresult, err = p.loadExistingOntology(existingOntology)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Error(i18n.GetMessage(\"ErrLoadExistingOntology\"), err)\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrLoadExistingOntology\"), err)\r\n\t\t}\r\n\t\ttokenCount := len(tke.Encode(result, nil, nil))\r\n\t\tp.logger.Debug(\"Loaded existing ontology, token count: %d\", tokenCount)\r\n\t}\r\n\r\n\tfor i := 0; i \u003c passes; i++ {\r\n\t\tif p.progressCallback != nil {\r\n\t\t\tp.progressCallback(ProgressInfo{\r\n\t\t\t\tCurrentPass: i + 1,\r\n\t\t\t\tTotalPasses: passes,\r\n\t\t\t\tCurrentStep: \"Starting Pass\",\r\n\t\t\t})\r\n\t\t}\r\n\t\tinitialTokenCount := len(tke.Encode(result, nil, nil))\r\n\t\tp.logger.Info(\"Starting pass %d with initial result token count: %d\", i+1, initialTokenCount)\r\n\r\n\t\tresult, err = p.processSinglePass(input, result)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Error(i18n.GetMessage(\"ErrProcessingPass\"), i+1, err)\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrProcessingPass\"), err)\r\n\t\t}\r\n\r\n\t\tnewTokenCount := len(tke.Encode(result, nil, nil))\r\n\t\tp.logger.Info(\"Completed pass %d, new result token count: %d\", i+1, newTokenCount)\r\n\t\tp.logger.Info(\"Token count change in pass %d: %d\", i+1, newTokenCount-initialTokenCount)\r\n\r\n\t\t// Optionally, you can add a log to show a snippet of the result\r\n\t\t// p.logger.Debug(\"Result snippet after pass %d: %s\", i+1, truncateString(result, 100))\r\n\t}\r\n\r\n\terr = p.saveResult(result, output)\r\n\tif err != nil {\r\n\t\tp.logger.Error(i18n.GetMessage(\"ErrSavingResult\"), err)\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrSavingResult\"), err)\r\n\t}\r\n\r\n\tfinalTokenCount := len(tke.Encode(result, nil, nil))\r\n\tp.logger.Info(\"Pipeline completed. Final result token count: %d\", finalTokenCount)\r\n\treturn nil\r\n}\r\n\r\n// Helper function to truncate long strings for logging\r\nfunc truncateString(s string, maxLength int) string {\r\n\tif len(s) \u003c= maxLength {\r\n\t\treturn s\r\n\t}\r\n\treturn s[:maxLength] + \"...\"\r\n}\r\n\r\nfunc (p *Pipeline) processSinglePass(input string, previousResult string) (string, error) {\r\n\t// Initialiser le tokenizer\r\n\ttke, err := tiktoken.GetEncoding(\"cl100k_base\")\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to initialize tokenizer: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"failed to initialize tokenizer: %w\", err)\r\n\t}\r\n\r\n\tinputTokens := len(tke.Encode(input, nil, nil))\r\n\tpreviousResultTokens := len(tke.Encode(previousResult, nil, nil))\r\n\tp.logger.Info(\"Processing single pass, input tokens: %d, previous result tokens: %d\", inputTokens, previousResultTokens)\r\n\r\n\tcontent, err := p.parseInput(input)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to parse input: %v\", err)\r\n\t\treturn \"\", err\r\n\t}\r\n\tcontentTokens := len(tke.Encode(string(content), nil, nil))\r\n\tp.logger.Info(\"Parsed content tokens: %d\", contentTokens)\r\n\r\n\tsegments, err := segmenter.Segment(content, segmenter.SegmentConfig{\r\n\t\tMaxTokens:   p.config.MaxTokens,\r\n\t\tContextSize: p.config.ContextSize,\r\n\t\tModel:       p.config.DefaultModel,\r\n\t})\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to segment content: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrSegmentContent\"), err)\r\n\t}\r\n\tp.logger.Info(\"Number of segments: %d\", len(segments))\r\n\r\n\tif p.progressCallback != nil {\r\n\t\tp.progressCallback(ProgressInfo{\r\n\t\t\tCurrentStep:   \"Segmenting\",\r\n\t\t\tTotalSegments: len(segments),\r\n\t\t})\r\n\t}\r\n\r\n\tresults := make([]string, len(segments))\r\n\tvar wg sync.WaitGroup\r\n\tfor i, segment := range segments {\r\n\t\twg.Add(1)\r\n\t\tgo func(i int, seg []byte) {\r\n\t\t\tdefer wg.Done()\r\n\t\t\tsegmentTokens := len(tke.Encode(string(seg), nil, nil))\r\n\t\t\tp.logger.Debug(\"Processing segment %d/%d, tokens: %d\", i+1, len(segments), segmentTokens)\r\n\t\t\tcontext := segmenter.GetContext(segments, i, segmenter.SegmentConfig{\r\n\t\t\t\tMaxTokens:   p.config.MaxTokens,\r\n\t\t\t\tContextSize: p.config.ContextSize,\r\n\t\t\t\tModel:       p.config.DefaultModel,\r\n\t\t\t})\r\n\t\t\tresult, err := p.processSegment(seg, context, previousResult)\r\n\t\t\tif err != nil {\r\n\t\t\t\tp.logger.Error(i18n.GetMessage(\"SegmentProcessingError\"), i+1, err)\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t\tresultTokens := len(tke.Encode(result, nil, nil))\r\n\t\t\tresults[i] = result\r\n\t\t\tp.logger.Info(\"Segment %d processed successfully, result tokens: %d\", i+1, resultTokens)\r\n\t\t\tif p.progressCallback != nil {\r\n\t\t\t\tp.progressCallback(ProgressInfo{\r\n\t\t\t\t\tCurrentStep:       \"Processing Segment\",\r\n\t\t\t\t\tProcessedSegments: i + 1,\r\n\t\t\t\t\tTotalSegments:     len(segments),\r\n\t\t\t\t})\r\n\t\t\t}\r\n\t\t}(i, segment)\r\n\t}\r\n\twg.Wait()\r\n\r\n\t// Fusionner les résultats de tous les segments\r\n\tmergedResult, err := p.mergeResults(previousResult, results)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to merge segment results: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"failed to merge segment results: %w\", err)\r\n\t}\r\n\r\n\tmergedResultTokens := len(tke.Encode(mergedResult, nil, nil))\r\n\tp.logger.Info(\"Merged result tokens: %d\", mergedResultTokens)\r\n\r\n\treturn mergedResult, nil\r\n}\r\n\r\nfunc (p *Pipeline) parseInput(input string) ([]byte, error) {\r\n\tinfo, err := os.Stat(input)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrAccessInput\"), err)\r\n\t}\r\n\r\n\tif info.IsDir() {\r\n\t\treturn p.parseDirectory(input)\r\n\t}\r\n\r\n\text := filepath.Ext(input)\r\n\tparser, err := parser.GetParser(ext)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrUnsupportedFormat\"), err)\r\n\t}\r\n\r\n\tcontent, err := parser.Parse(input)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrParseFile\"), err)\r\n\t}\r\n\r\n\treturn content, nil\r\n}\r\n\r\nfunc (p *Pipeline) parseDirectory(dir string) ([]byte, error) {\r\n\tvar content []byte\r\n\terr := filepath.Walk(dir, func(path string, info os.FileInfo, err error) error {\r\n\t\tif err != nil {\r\n\t\t\treturn err\r\n\t\t}\r\n\t\tif !info.IsDir() {\r\n\t\t\text := filepath.Ext(path)\r\n\t\t\tparser, err := parser.GetParser(ext)\r\n\t\t\tif err != nil {\r\n\t\t\t\tp.logger.Warning(i18n.GetMessage(\"ErrUnsupportedFormat\"), path)\r\n\t\t\t\treturn nil\r\n\t\t\t}\r\n\t\t\tfileContent, err := parser.Parse(path)\r\n\t\t\tif err != nil {\r\n\t\t\t\tp.logger.Warning(i18n.GetMessage(\"ErrParseFile\"), path, err)\r\n\t\t\t\treturn nil\r\n\t\t\t}\r\n\t\t\tcontent = append(content, fileContent...)\r\n\t\t}\r\n\t\treturn nil\r\n\t})\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrParseDirectory\"), err)\r\n\t}\r\n\treturn content, nil\r\n}\r\n\r\nfunc (p *Pipeline) processSegment(segment []byte, context string, previousResult string) (string, error) {\r\n\tp.logger.Debug(\"Processing segment of length %d\", len(segment))\r\n\r\n\tenrichmentValues := map[string]string{\r\n\t\t\"text\":            string(segment),\r\n\t\t\"context\":         context,\r\n\t\t\"previous_result\": previousResult,\r\n\t}\r\n\r\n\tenrichedResult, err := p.llm.ProcessWithPrompt(prompt.OntologyEnrichmentPrompt, enrichmentValues)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"ontology enrichment failed: %w\", err)\r\n\t}\r\n\r\n\tp.logger.Debug(\"Enriched ontology:\\n%s\", enrichedResult)\r\n\treturn enrichedResult, nil\r\n}\r\n\r\nfunc (p *Pipeline) mergeResults(previousResult string, newResults []string) (string, error) {\r\n\t// Combiner tous les nouveaux résultats\r\n\tcombinedNewResults := strings.Join(newResults, \"\\n\")\r\n\r\n\t// Préparer les valeurs pour le prompt de fusion\r\n\tmergeValues := map[string]string{\r\n\t\t\"previous_ontology\": previousResult,\r\n\t\t\"new_ontology\":      combinedNewResults,\r\n\t}\r\n\r\n\t// Utiliser le LLM pour fusionner les résultats\r\n\tmergedResult, err := p.llm.ProcessWithPrompt(prompt.OntologyMergePrompt, mergeValues)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"ontology merge failed: %w\", err)\r\n\t}\r\n\r\n\treturn mergedResult, nil\r\n}\r\n\r\nfunc (p *Pipeline) combineResults(results []string) (string, error) {\r\n\tcombined := strings.Join(results, \"\\n\")\r\n\treturn combined, nil\r\n}\r\n\r\nfunc (p *Pipeline) loadExistingOntology(path string) (string, error) {\r\n\tcontent, err := ioutil.ReadFile(path)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrReadExistingOntology\"), err)\r\n\t}\r\n\treturn string(content), nil\r\n}\r\n\r\nfunc (p *Pipeline) saveResult(result string, outputPath string) error {\r\n\tqsc := converter.NewQuickStatementConverter(p.logger)\r\n\r\n\tqs, err := qsc.Convert([]byte(result), \"\", \"\") // Nous utilisons des chaînes vides pour context et ontology pour l'instant\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrConvertQuickStatement\"), err)\r\n\t}\r\n\r\n\tdir := filepath.Dir(outputPath)\r\n\tbaseName := filepath.Base(outputPath)\r\n\text := filepath.Ext(baseName)\r\n\tnameWithoutExt := strings.TrimSuffix(baseName, ext)\r\n\r\n\ttsvPath := filepath.Join(dir, nameWithoutExt+\".tsv\")\r\n\terr = ioutil.WriteFile(tsvPath, []byte(qs), 0644)\r\n\tif err != nil {\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrWriteOutput\"), err)\r\n\t}\r\n\r\n\tif p.config.ExportRDF {\r\n\t\trdf, err := qsc.ConvertToRDF(qs)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrConvertRDF\"), err)\r\n\t\t}\r\n\t\trdfPath := filepath.Join(dir, nameWithoutExt+\".rdf\")\r\n\t\terr = ioutil.WriteFile(rdfPath, []byte(rdf), 0644)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrWriteRDF\"), err)\r\n\t\t}\r\n\t}\r\n\r\n\tif p.config.ExportOWL {\r\n\t\towl, err := qsc.ConvertToOWL(qs)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrConvertOWL\"), err)\r\n\t\t}\r\n\t\towlPath := filepath.Join(dir, nameWithoutExt+\".owl\")\r\n\t\terr = ioutil.WriteFile(owlPath, []byte(owl), 0644)\r\n\t\tif err != nil {\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrWriteOWL\"), err)\r\n\t\t}\r\n\t}\r\n\r\n\treturn nil\r\n}\r\n",
    "size": 11435,
    "modTime": "2024-10-21T23:48:08.1628723+02:00",
    "path": "internal\\pipeline\\pipeline.go"
  },
  {
    "name": "prompt.go",
    "content": "package prompt\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"strings\"\r\n)\r\n\r\n// PromptTemplate représente un template de prompt\r\ntype PromptTemplate struct {\r\n\tTemplate string\r\n}\r\n\r\n// NewPromptTemplate crée un nouveau PromptTemplate\r\nfunc NewPromptTemplate(template string) *PromptTemplate {\r\n\treturn \u0026PromptTemplate{Template: template}\r\n}\r\n\r\n// Format remplit le template avec les valeurs fournies\r\nfunc (pt *PromptTemplate) Format(values map[string]string) string {\r\n\tresult := pt.Template\r\n\tfor key, value := range values {\r\n\t\tresult = strings.Replace(result, fmt.Sprintf(\"{%s}\", key), value, -1)\r\n\t}\r\n\treturn result\r\n}\r\n\r\n// Définition des templates de prompts\r\nvar (\r\n\tEntityExtractionPrompt = NewPromptTemplate(`\r\nAnalyze the following text and extract key entities (e.g., people, organizations, concepts) relevant to building an ontology:\r\n\r\n{text}\r\n\r\nFor each entity, provide:\r\n1. Entity name\r\n2. Entity type (e.g., Person, Organization, Concept)\r\n3. A brief description or context\r\n\r\nFormat your response as a list of entities, one per line, using tabs to separate fields like this:\r\nEntityName\\tEntityType\\tDescription/Context\r\n\r\nEnsure that your extractions are relevant to creating an ontology and avoid including irrelevant or trivial information.\r\nUse the original document language. Do it silently with no comment.\r\n`)\r\n\r\n\tRelationExtractionPrompt = NewPromptTemplate(`\r\nBased on the following text and the list of entities provided, identify relationships between these entities that would be relevant for an ontology:\r\n\r\nText:\r\n{text}\r\n\r\nEntities:\r\n{entities}\r\n\r\nFor each relationship, provide:\r\n1. Source Entity\r\n2. Relationship Type\r\n3. Target Entity\r\n4. A brief description or context of the relationship\r\n\r\nFormat your response as a list of relationships, one per line, using tabs to separate fields like this:\r\nSourceEntity\\tRelationshipType\\tTargetEntity\\tDescription/Context\r\n\r\nFocus on meaningful relationships that contribute to the structure of the ontology. Avoid trivial or overly generic relationships.\r\nUse the original document language. Do it silently with no comment.\r\n`)\r\n\r\n\tOntologyEnrichmentPrompt = NewPromptTemplate(`\r\nVous êtes un expert en ontologies chargé d'enrichir et de raffiner une ontologie existante. Voici l'ontologie actuelle et de nouvelles informations à intégrer :\r\n\r\nOntologie actuelle :\r\n{previous_result}\r\n\r\nNouveau texte à analyser :\r\n{text}\r\n\r\nContexte supplémentaire :\r\n{context}\r\n\r\nVotre tâche :\r\n1. Analyser le nouveau texte et le contexte.\r\n2. Identifier les nouvelles entités et relations pertinentes.\r\n3. Intégrer ces nouvelles informations dans l'ontologie existante.\r\n4. Raffiner les entités et relations existantes si nécessaire.\r\n5. Assurer la cohérence globale de l'ontologie.\r\n\r\nFournissez l'ontologie enrichie et raffinée dans le format suivant :\r\n- Pour les entités : Nom_Entité\\tType_Entité\\tDescription\r\n- Pour les relations : Entité_Source\\tType_Relation\\tEntité_Cible\\tDescription\r\n\r\nAssurez-vous que chaque élément de l'ontologie est pertinent et contribue à une représentation complète et cohérente du domaine.\r\nUtilisez la langue originale du document. Répondez silencieusement, sans commentaires additionnels.\r\n`)\r\n\r\n\tOntologyMergePrompt = NewPromptTemplate(`\r\nVous êtes un expert en fusion d'ontologies. Votre tâche est de fusionner intelligemment une ontologie existante avec de nouvelles informations pour créer une ontologie enrichie et cohérente.\r\n\r\nOntologie existante :\r\n{previous_ontology}\r\n\r\nNouvelles informations à intégrer :\r\n{new_ontology}\r\n\r\nDirectives pour la fusion :\r\n1. Intégrez toutes les nouvelles entités et relations pertinentes de la nouvelle ontologie.\r\n2. En cas de conflit ou de duplication, conservez l'information la plus complète ou la plus à jour.\r\n3. Assurez-vous que les relations entre les entités restent cohérentes.\r\n4. Si une nouvelle information contredit une ancienne, privilégiez la nouvelle mais notez la contradiction si elle est significative.\r\n5. Maintenez la structure et le format de l'ontologie existante.\r\n6. Évitez les redondances et les informations en double.\r\n\r\nVotre tâche :\r\n- Analysez attentivement les deux ensembles d'informations.\r\n- Fusionnez-les en une ontologie unique et cohérente.\r\n- Assurez-vous que le résultat final est complet, sans perte d'information importante.\r\n\r\nFormat de sortie :\r\nPrésentez l'ontologie fusionnée dans le même format que l'ontologie existante, avec une entité ou une relation par ligne.\r\nPour les entités : Nom_Entité\\tType_Entité\\tDescription\r\nPour les relations : Entité_Source\\tType_Relation\\tEntité_Cible\\tDescription\r\n\r\nProcédez à la fusion de manière silencieuse, sans ajouter de commentaires ou d'explications supplémentaires.\r\n`)\r\n)\r\n",
    "size": 4747,
    "modTime": "2024-10-21T23:21:42.296373+02:00",
    "path": "internal\\prompt\\prompt.go"
  },
  {
    "name": "segmenter.go",
    "content": "package segmenter\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"errors\"\r\n\t\"fmt\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/pkoukk/tiktoken-go\"\r\n)\r\n\r\nvar (\r\n\tErrInvalidContent = errors.New(i18n.Messages.ErrInvalidContent)\r\n\tErrTokenization   = errors.New(i18n.Messages.ErrTokenization)\r\n)\r\n\r\nvar log = logger.GetLogger()\r\n\r\n// SegmentConfig holds the configuration for segmentation\r\ntype SegmentConfig struct {\r\n\tMaxTokens   int\r\n\tContextSize int\r\n\tModel       string\r\n}\r\n\r\n// Segment divides the content into segments of maxTokens\r\nfunc Segment(content []byte, cfg SegmentConfig) ([][]byte, error) {\r\n\tlog.Debug(i18n.Messages.LogSegmentationStarted)\r\n\tlog.Debug(fmt.Sprintf(\"Segmentation config: MaxTokens=%d, ContextSize=%d, Model=%s\", cfg.MaxTokens, cfg.ContextSize, cfg.Model))\r\n\tlog.Debug(fmt.Sprintf(\"Content length: %d bytes\", len(content)))\r\n\r\n\tif len(content) == 0 {\r\n\t\treturn nil, ErrInvalidContent\r\n\t}\r\n\r\n\ttokenizer, err := getTokenizer(cfg.Model)\r\n\tif err != nil {\r\n\t\treturn nil, err\r\n\t}\r\n\r\n\tvar segments [][]byte\r\n\tsentences := splitIntoSentences(content)\r\n\tcurrentSegment := new(bytes.Buffer)\r\n\tcurrentTokenCount := 0\r\n\r\n\tfor _, sentence := range sentences {\r\n\t\tsentenceTokens := CountTokens(sentence, tokenizer)\r\n\r\n\t\tif currentTokenCount+sentenceTokens \u003e cfg.MaxTokens {\r\n\t\t\tif currentSegment.Len() \u003e 0 {\r\n\t\t\t\tsegments = append(segments, currentSegment.Bytes())\r\n\t\t\t\tlog.Debug(fmt.Sprintf(\"Segment created with %d tokens\", currentTokenCount))\r\n\t\t\t\tcurrentSegment.Reset()\r\n\t\t\t\tcurrentTokenCount = 0\r\n\t\t\t}\r\n\t\t}\r\n\r\n\t\tcurrentSegment.Write(sentence)\r\n\t\tcurrentTokenCount += sentenceTokens\r\n\r\n\t\tif currentTokenCount \u003e= cfg.MaxTokens {\r\n\t\t\tsegments = append(segments, currentSegment.Bytes())\r\n\t\t\tlog.Debug(fmt.Sprintf(\"Segment created with %d tokens\", currentTokenCount))\r\n\t\t\tcurrentSegment.Reset()\r\n\t\t\tcurrentTokenCount = 0\r\n\t\t}\r\n\t}\r\n\r\n\tif currentSegment.Len() \u003e 0 {\r\n\t\tsegments = append(segments, currentSegment.Bytes())\r\n\t\tlog.Debug(fmt.Sprintf(\"Final segment created with %d tokens\", currentTokenCount))\r\n\t}\r\n\r\n\tlog.Info(fmt.Sprintf(i18n.Messages.LogSegmentationCompleted, len(segments)))\r\n\treturn segments, nil\r\n}\r\n\r\nfunc splitIntoSentences(content []byte) [][]byte {\r\n\tvar sentences [][]byte\r\n\tvar currentSentence []byte\r\n\r\n\tfor _, b := range content {\r\n\t\tcurrentSentence = append(currentSentence, b)\r\n\t\tif b == '.' || b == '!' || b == '?' {\r\n\t\t\tsentences = append(sentences, currentSentence)\r\n\t\t\tcurrentSentence = []byte{}\r\n\t\t}\r\n\t}\r\n\r\n\tif len(currentSentence) \u003e 0 {\r\n\t\tsentences = append(sentences, currentSentence)\r\n\t}\r\n\r\n\treturn sentences\r\n}\r\n\r\n// CountTokens returns the number of tokens in the content\r\nfunc CountTokens(content []byte, tokenizer *tiktoken.Tiktoken) int {\r\n\ttokens := tokenizer.Encode(string(content), nil, nil)\r\n\tcount := len(tokens)\r\n\treturn count\r\n}\r\n\r\n// getTokenizer returns a tokenizer for the specified model\r\nfunc getTokenizer(model string) (*tiktoken.Tiktoken, error) {\r\n\tencoding, err := tiktoken.GetEncoding(\"cl100k_base\")\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.Messages.ErrTokenizerInitialization, err)\r\n\t}\r\n\treturn encoding, nil\r\n}\r\n\r\n// CalibrateTokenCount adjusts the token count based on the LLM model\r\nfunc CalibrateTokenCount(count int, model string) int {\r\n\tlog.Debug(\"Calibrating token count for model %s. Original count: %d\", model, count)\r\n\t// Implement model-specific calibration logic here\r\n\t// For now, we'll just return the original count\r\n\tlog.Debug(\"Calibrated count: %d\", count)\r\n\treturn count\r\n}\r\n\r\n// GetContext returns the context of previous segments\r\nfunc GetContext(segments [][]byte, currentIndex int, cfg SegmentConfig) string {\r\n\tlog.Debug(i18n.Messages.LogContextGeneration)\r\n\tif currentIndex == 0 {\r\n\t\treturn \"\"\r\n\t}\r\n\r\n\ttokenizer, err := getTokenizer(cfg.Model)\r\n\tif err != nil {\r\n\t\tlog.Error(i18n.Messages.ErrTokenizerInitialization, err)\r\n\t\treturn \"\"\r\n\t}\r\n\r\n\tvar context bytes.Buffer\r\n\ttokenCount := 0\r\n\r\n\tfor i := currentIndex - 1; i \u003e= 0; i-- {\r\n\t\tsegmentTokens := CountTokens(segments[i], tokenizer)\r\n\r\n\t\tif tokenCount+segmentTokens \u003e cfg.ContextSize {\r\n\t\t\tbreak\r\n\t\t}\r\n\r\n\t\t// Prepend this segment to the context\r\n\t\ttemp := make([]byte, len(segments[i]))\r\n\t\tcopy(temp, segments[i])\r\n\t\tcontext.Write(temp)\r\n\t\tcontext.WriteByte('\\n')\r\n\r\n\t\ttokenCount += segmentTokens\r\n\t}\r\n\r\n\tlog.Debug(fmt.Sprintf(\"Generated context with %d tokens\", tokenCount))\r\n\treturn context.String()\r\n}\r\n",
    "size": 4420,
    "modTime": "2024-10-21T00:28:58.8026397+02:00",
    "path": "internal\\segmenter\\segmenter.go"
  },
  {
    "name": "tokenizer.go",
    "content": "package tokenizer\r\n\r\nimport (\r\n\t\"github.com/pkoukk/tiktoken-go\"\r\n)\r\n\r\n// CountTokens returns the number of tokens in the given text\r\nfunc CountTokens(text string) (int, error) {\r\n\tencoding, err := tiktoken.GetEncoding(\"cl100k_base\")\r\n\tif err != nil {\r\n\t\treturn 0, err\r\n\t}\r\n\ttokens := encoding.Encode(text, nil, nil)\r\n\treturn len(tokens), nil\r\n}\r\n",
    "size": 346,
    "modTime": "2024-10-20T15:16:21.4160242+02:00",
    "path": "internal\\tokenizer\\tokenizer.go"
  }
]