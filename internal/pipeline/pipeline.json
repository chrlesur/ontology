[
  {
    "name": "context.go",
    "content": "package pipeline\r\n\r\nimport (\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n)\r\n\r\nvar log = logger.GetLogger()\r\n\r\n// ContextEntry représente le contexte pour une position spécifique dans le document\r\ntype ContextEntry struct {\r\n\tPosition int      `json:\"position\"`\r\n\tBefore   []string `json:\"before\"`\r\n\tAfter    []string `json:\"after\"`\r\n\tElement  string   `json:\"element\"`\r\n\tLength   int      `json:\"length\"`\r\n}\r\n",
    "size": 417,
    "modTime": "2024-11-16T14:50:56.872504+01:00",
    "path": "context.go"
  },
  {
    "name": "context_generation.go",
    "content": "// context_generation.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"strings\"\r\n)\r\n\r\n// GenerateContextJSON génère un JSON contenant le contexte pour chaque position donnée\r\nfunc GenerateContextJSON(content []byte, positions []int, contextWords int, positionRanges []PositionRange) (string, error) {\r\n\tlog.Debug(\"Starting GenerateContextJSON\")\r\n\tlog.Debug(\"Number of positions: %d, Context words: %d\", len(positions), contextWords)\r\n\r\n\twords := strings.Fields(string(content))\r\n\tlog.Debug(\"Total words in content: %d\", len(words))\r\n\r\n\tvar entries []ContextEntry\r\n\tfor _, pr := range positionRanges {\r\n\t\tstart := pr.Start\r\n\t\tend := pr.End\r\n\t\telement := pr.Element\r\n\r\n\t\tif start \u003c 0 || end \u003e= len(words) {\r\n\t\t\tlog.Warning(\"Invalid position range for element %s: [%d, %d]\", element, start, end)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tbeforeStart := max(0, start-contextWords)\r\n\t\tafterEnd := min(len(words), end+contextWords+1)\r\n\r\n\t\tentry := ContextEntry{\r\n\t\t\tPosition: start,\r\n\t\t\tBefore:   words[beforeStart:start],\r\n\t\t\tAfter:    words[end+1 : afterEnd],\r\n\t\t\tElement:  element,\r\n\t\t\tLength:   end - start + 1,\r\n\t\t}\r\n\t\tentries = append(entries, entry)\r\n\t\tlog.Debug(\"Generated context for element %s at position %d\", element, start)\r\n\t}\r\n\r\n\tlog.Debug(\"Number of context entries generated: %d\", len(entries))\r\n\r\n\tif len(entries) == 0 {\r\n\t\tlog.Warning(\"No context entries generated\")\r\n\t\treturn \"[]\", nil\r\n\t}\r\n\r\n\t// Utiliser un encoder JSON personnalisé\r\n\tvar buf strings.Builder\r\n\tencoder := json.NewEncoder(\u0026buf)\r\n\tencoder.SetEscapeHTML(false)\r\n\tencoder.SetIndent(\"\", \"  \") // Deux espaces pour l'indentation\r\n\r\n\tif err := encoder.Encode(entries); err != nil {\r\n\t\tlog.Error(\"Error marshaling context JSON: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"error marshaling context JSON: %w\", err)\r\n\t}\r\n\r\n\toutput := buf.String()\r\n\tlog.Debug(\"JSON data generated successfully. Length: %d bytes\", len(output))\r\n\r\n\treturn output, nil\r\n}\r\n\r\n// getContextWords récupère les mots de contexte avant et après une position donnée\r\nfunc getContextWords(words []string, position, contextSize int) ([]string, []string) {\r\n\tstart := max(0, position-contextSize)\r\n\tend := min(len(words), position+contextSize+1)\r\n\r\n\tvar before, after []string\r\n\tif position \u003e start {\r\n\t\tbefore = words[start:position]\r\n\t}\r\n\tif position+1 \u003c end {\r\n\t\tafter = words[position+1 : end]\r\n\t}\r\n\r\n\treturn before, after\r\n}\r\n\r\n// formatContextJSON formate le JSON de contexte pour une meilleure lisibilité\r\n\r\nfunc formatContextJSON(jsonString string) string {\r\n\tvar buf bytes.Buffer\r\n\terr := json.Indent(\u0026buf, []byte(jsonString), \"\", \"  \")\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to format JSON: %v\", err)\r\n\t\treturn jsonString // Retourne le JSON non formaté en cas d'erreur\r\n\t}\r\n\treturn buf.String()\r\n}\r\n",
    "size": 2760,
    "modTime": "2024-11-16T14:50:56.8735053+01:00",
    "path": "context_generation.go"
  },
  {
    "name": "db.go",
    "content": "package pipeline\r\n\r\nimport (\r\n\t\"database/sql\"\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/model\"\r\n\t_ \"modernc.org/sqlite\"\r\n)\r\n\r\nfunc initDB() (*sql.DB, error) {\r\n\tlog.Debug(\"Initializing in-memory SQLite database\")\r\n\tdb, err := sql.Open(\"sqlite\", \":memory:\")\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to open database: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"échec de l'ouverture de la base de données : %w\", err)\r\n\t}\r\n\r\n\t// Création de la table des entités et de l'index\r\n\t_, err = db.Exec(`\r\n        CREATE TABLE IF NOT EXISTS entities (\r\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\r\n            name TEXT NOT NULL UNIQUE,\r\n            type TEXT NOT NULL,\r\n            description TEXT,\r\n            positions TEXT,  \r\n            created_at DATETIME,\r\n            updated_at DATETIME,\r\n            source TEXT\r\n        );\r\n        CREATE INDEX IF NOT EXISTS idx_entities_name ON entities(name);\r\n    `)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to create entities table or index: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"échec de la création de la table entities ou de l'index : %w\", err)\r\n\t}\r\n\tlog.Debug(\"Entities table and index created successfully\")\r\n\r\n\t// Création de la table des relations et des index\r\n\t_, err = db.Exec(`\r\n        CREATE TABLE IF NOT EXISTS relations (\r\n            id INTEGER PRIMARY KEY AUTOINCREMENT,\r\n            source TEXT NOT NULL,\r\n            type TEXT NOT NULL,\r\n            target TEXT NOT NULL,\r\n            description TEXT,\r\n            weight REAL,\r\n            direction TEXT,\r\n            created_at DATETIME,\r\n            updated_at DATETIME,\r\n            UNIQUE(source, type, target)\r\n        );\r\n        CREATE INDEX IF NOT EXISTS idx_relations_source ON relations(source);\r\n        CREATE INDEX IF NOT EXISTS idx_relations_target ON relations(target);\r\n    `)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to create relations table or indexes: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"échec de la création de la table relations ou des index : %w\", err)\r\n\t}\r\n\tlog.Debug(\"Relations table and indexes created successfully\")\r\n\r\n\tlog.Debug(\"Database initialized successfully\")\r\n\treturn db, nil\r\n}\r\n\r\nfunc UpsertEntity(db *sql.DB, entity *model.OntologyElement) error {\r\n    log.Debug(\"Upserting entity: %+v\", entity)\r\n    positionsJSON, err := json.Marshal(entity.Positions)\r\n    if err != nil {\r\n        return fmt.Errorf(\"failed to marshal positions: %w\", err)\r\n    }\r\n    _, err = db.Exec(`\r\n        INSERT INTO entities (name, type, description, positions, created_at, updated_at, source)\r\n        VALUES (?, ?, ?, ?, ?, ?, ?)\r\n        ON CONFLICT(name) DO UPDATE SET\r\n        type = ?,\r\n        description = ?,\r\n        positions = ?,\r\n        updated_at = ?,\r\n        source = ?\r\n    `, entity.Name, entity.Type, entity.Description, positionsJSON, entity.CreatedAt, entity.UpdatedAt, entity.Source,\r\n       entity.Type, entity.Description, positionsJSON, entity.UpdatedAt, entity.Source)\r\n    if err != nil {\r\n        log.Error(\"Failed to upsert entity: %v\", err)\r\n        return fmt.Errorf(\"failed to upsert entity: %w\", err)\r\n    }\r\n    log.Debug(\"Entity upserted successfully with positions: %v\", entity.Positions)\r\n    return nil\r\n}\r\n\r\nfunc UpsertRelation(db *sql.DB, relation *model.Relation) error {\r\n    log.Debug(\"Upserting relation: %+v\", relation)\r\n    _, err := db.Exec(`\r\n        INSERT INTO relations (source, type, target, description, weight, direction, created_at, updated_at)\r\n        VALUES (?, ?, ?, ?, ?, ?, ?, ?)\r\n        ON CONFLICT(source, type, target) DO UPDATE SET\r\n        description = ?,\r\n        weight = ?,\r\n        direction = ?,\r\n        updated_at = ?\r\n    `, relation.Source, relation.Type, relation.Target, relation.Description, relation.Weight, relation.Direction,\r\n        relation.CreatedAt, relation.UpdatedAt,\r\n        relation.Description, relation.Weight, relation.Direction, relation.UpdatedAt)\r\n    if err != nil {\r\n        log.Error(\"Failed to upsert relation: %v\", err)\r\n        return fmt.Errorf(\"failed to upsert relation: %w\", err)\r\n    }\r\n    return nil\r\n}\r\n\r\nfunc GetAllEntities(db *sql.DB) ([]*model.OntologyElement, error) {\r\n\tlog.Debug(\"Starting GetAllEntities\")\r\n\r\n\trows, err := db.Query(\"SELECT name, type, description, positions, created_at, updated_at, source FROM entities\")\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to query entities: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"échec de la récupération des entités : %w\", err)\r\n\t}\r\n\tdefer rows.Close()\r\n\r\n\tvar entities []*model.OntologyElement\r\n\tfor rows.Next() {\r\n\t\tvar e model.OntologyElement\r\n\t\tvar positionsJSON []byte\r\n        err := rows.Scan(\u0026e.Name, \u0026e.Type, \u0026e.Description, \u0026positionsJSON, \u0026e.CreatedAt, \u0026e.UpdatedAt, \u0026e.Source)\r\n\t\tif err != nil {\r\n\t\t\tlog.Error(\"Failed to scan entity: %v\", err)\r\n\t\t\treturn nil, fmt.Errorf(\"échec du scan d'une entité : %w\", err)\r\n\t\t}\r\n\r\n\t\tlog.Debug(\"Retrieved entity from DB: Name=%s, Type=%s, Description=%s, PositionsJSON=%s\",\r\n\t\t\te.Name, e.Type, e.Description, string(positionsJSON))\r\n\r\n\t\tif len(positionsJSON) \u003e 0 {\r\n\t\t\terr = json.Unmarshal(positionsJSON, \u0026e.Positions)\r\n\t\t\tif err != nil {\r\n\t\t\t\tlog.Error(\"Failed to unmarshal positions for entity %s: %v\", e.Name, err)\r\n\t\t\t\treturn nil, fmt.Errorf(\"échec du décodage des positions pour l'entité %s : %w\", e.Name, err)\r\n\t\t\t}\r\n\t\t\tlog.Debug(\"Unmarshalled positions for entity %s: %v\", e.Name, e.Positions)\r\n\t\t} else {\r\n\t\t\tlog.Debug(\"No positions found for entity %s\", e.Name)\r\n\t\t}\r\n\r\n\t\tentities = append(entities, \u0026e)\r\n\t}\r\n\r\n\tlog.Debug(\"Retrieved %d entities in total\", len(entities))\r\n\treturn entities, nil\r\n}\r\n\r\nfunc GetAllRelations(db *sql.DB) ([]*model.Relation, error) {\r\n\tlog.Debug(\"Getting all relations\")\r\n\trows, err := db.Query(\"SELECT source, type, target, description, weight, direction, created_at, updated_at FROM relations\")\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to query relations: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"échec de la récupération des relations : %w\", err)\r\n\t}\r\n\tdefer rows.Close()\r\n\r\n\tvar relations []*model.Relation\r\n\tfor rows.Next() {\r\n\t\tvar r model.Relation\r\n\t\terr := rows.Scan(\u0026r.Source, \u0026r.Type, \u0026r.Target, \u0026r.Description, \u0026r.Weight, \u0026r.Direction, \u0026r.CreatedAt, \u0026r.UpdatedAt)\r\n\t\tif err != nil {\r\n\t\t\tlog.Error(\"Failed to scan relation: %v\", err)\r\n\t\t\treturn nil, fmt.Errorf(\"échec du scan d'une relation : %w\", err)\r\n\t\t}\r\n\t\tlog.Debug(\"Retrieved relation: %+v\", r)\r\n\t\trelations = append(relations, \u0026r)\r\n\t}\r\n\tlog.Debug(\"Total relations retrieved: %d\", len(relations))\r\n\treturn relations, nil\r\n}\r\n",
    "size": 6502,
    "modTime": "2024-11-16T14:50:56.8745057+01:00",
    "path": "db.go"
  },
  {
    "name": "enrichment.go",
    "content": "// enrichment.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/model\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n)\r\n\r\n// processSegment traite un segment individuel du contenu\r\nfunc (p *Pipeline) processSegment(segment []byte, context string, previousResult string, positionIndex map[string][]int, includePositions bool, offset int) (string, error) {\r\n\tlog.Debug(\"Processing segment of length %d, context length %d, previous result length %d, offset %d\", len(segment), len(context), len(previousResult), offset)\r\n\tlog.Debug(\"Segment content preview: %s\", truncateString(string(segment), 200))\r\n\tlog.Debug(\"Context preview: %s\", truncateString(context, 200))\r\n\r\n\tenrichmentValues := map[string]string{\r\n\t\t\"text\":            string(segment),\r\n\t\t\"context\":         context,\r\n\t\t\"previous_result\": previousResult,\r\n\t}\r\n\r\n\tvar enrichmentPrompt *prompt.PromptTemplate\r\n\tif p.enrichmentPromptFile != \"\" {\r\n\t\tcustomPrompt, err := p.readPromptFile(p.enrichmentPromptFile)\r\n\t\tif err != nil {\r\n\t\t\tlog.Error(\"Failed to read custom prompt file: %v\", err)\r\n\t\t\treturn \"\", fmt.Errorf(\"failed to read custom prompt file: %w\", err)\r\n\t\t}\r\n\t\tenrichmentPrompt = prompt.NewCustomPromptTemplate(customPrompt)\r\n\t\tlog.Debug(\"Using custom enrichment prompt from file: %s\", p.enrichmentPromptFile)\r\n\t} else {\r\n\t\tenrichmentPrompt = prompt.OntologyEnrichmentPrompt\r\n\t\tlog.Debug(\"Using default enrichment prompt\")\r\n\t}\r\n\r\n\tlog.Debug(\"Calling LLM with OntologyEnrichmentPrompt\")\r\n\r\n\tenrichedResult, err := p.llm.ProcessWithPrompt(enrichmentPrompt, enrichmentValues)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Ontology enrichment failed: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"ontology enrichment failed: %w\", err)\r\n\t}\r\n\r\n\t// Normaliser le résultat enrichi\r\n\tnormalizedResult := normalizeTSV(enrichedResult)\r\n\r\n\tlog.Debug(\"Enriched result length: %d, preview: %s\", len(normalizedResult), truncateString(normalizedResult, 100))\r\n\r\n\tp.enrichOntologyWithPositions(normalizedResult, positionIndex, includePositions, string(segment), offset)\r\n\r\n\treturn normalizedResult, nil\r\n}\r\n\r\n// enrichOntologyWithPositions enrichit l'ontologie avec les positions des éléments\r\nfunc (p *Pipeline) enrichOntologyWithPositions(enrichedResult string, positionIndex map[string][]int, includePositions bool, content string, offset int) {\r\n\tlog.Debug(\"Starting enrichOntologyWithPositions with offset %d\", offset)\r\n\tlog.Debug(\"Include positions: %v\", includePositions)\r\n\tlog.Debug(\"Position index size: %d\", len(positionIndex))\r\n\r\n\tlines := strings.Split(enrichedResult, \"\\n\")\r\n\tlog.Debug(\"Number of lines to process: %d\", len(lines))\r\n\r\n\tfor i, line := range lines {\r\n\t\tlog.Debug(\"Processing line %d: %s\", i, line)\r\n\t\tparts := strings.Fields(line)\r\n\t\tif len(parts) \u003e= 3 {\r\n\t\t\tname := parts[0]\r\n\t\t\telementType := parts[1]\r\n\t\t\tdescription := strings.Join(parts[2:], \" \")\r\n\r\n\t\t\telement := p.ontology.GetElementByName(name)\r\n\t\t\tif element == nil {\r\n\t\t\t\telement = model.NewOntologyElement(name, elementType)\r\n\t\t\t\tp.ontology.AddElement(element)\r\n\t\t\t\tlog.Debug(\"Added new element: %v\", element)\r\n\t\t\t} else {\r\n\t\t\t\tlog.Debug(\"Updated existing element: %v\", element)\r\n\t\t\t}\r\n\t\t\telement.Description = description\r\n\r\n\t\t\tif includePositions {\r\n\t\t\t\tlog.Debug(\"Searching for positions of entity: %s\", name)\r\n\t\t\t\tallPositions := p.findPositions(name, positionIndex, string(p.fullContent))\r\n\t\t\t\tlog.Debug(\"Found %d positions for entity %s: %v\", len(allPositions), name, allPositions)\r\n\t\t\t\tif len(allPositions) \u003e 0 {\r\n\t\t\t\t\t// Garder toutes les positions trouvées\r\n\t\t\t\t\telement.SetPositions(uniquePositions(allPositions))\r\n\t\t\t\t\tlog.Debug(\"Set %d positions for element %s: %v\", len(allPositions), name, allPositions)\r\n\t\t\t\t} else {\r\n\t\t\t\t\tlog.Debug(\"No positions found for element %s\", name)\r\n\t\t\t\t}\r\n\t\t\t}\r\n\r\n\t\t\t// Traitement des relations (si nécessaire)\r\n\t\t\tif len(parts) \u003e= 4 {\r\n\t\t\t\t// Code pour traiter les relations...\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\tlog.Warning(\"Skipping invalid line: %s\", line)\r\n\t\t}\r\n\t}\r\n\r\n\tlog.Debug(\"Ontology after enrichment:\")\r\n\tfor _, element := range p.ontology.Elements {\r\n\t\tlog.Debug(\"Element: %s, Type: %s, Description: %s, Positions: %v\",\r\n\t\t\telement.Name, element.Type, element.Description, element.Positions)\r\n\t}\r\n\tlog.Debug(\"Final ontology state - Elements: %d, Relations: %d\",\r\n\t\tlen(p.ontology.Elements), len(p.ontology.Relations))\r\n}\r\n\r\n// uniquePositions supprime les doublons dans une slice d'entiers\r\nfunc uniquePositions(positions []int) []int {\r\n\tkeys := make(map[int]bool)\r\n\tlist := []int{}\r\n\tfor _, entry := range positions {\r\n\t\tif _, value := keys[entry]; !value {\r\n\t\t\tkeys[entry] = true\r\n\t\t\tlist = append(list, entry)\r\n\t\t}\r\n\t}\r\n\treturn list\r\n}\r\n\r\n// normalizeTSV normalise une chaîne TSV\r\nfunc normalizeTSV(input string) string {\r\n\tlines := strings.Split(input, \"\\n\")\r\n\tvar normalizedLines []string\r\n\tfor _, line := range lines {\r\n\t\tline = strings.ReplaceAll(line, \"\\\\t\", \" \")\r\n\t\tline = strings.ReplaceAll(line, \"\\t\", \" \")\r\n\t\tfields := strings.Fields(line)\r\n\t\tif len(fields) \u003e= 3 {\r\n\t\t\tnormalizedLine := strings.Join(fields[:2], \"\\t\") + \"\\t\" + strings.Join(fields[2:], \" \")\r\n\t\t\tnormalizedLines = append(normalizedLines, normalizedLine)\r\n\t\t}\r\n\t}\r\n\treturn strings.Join(normalizedLines, \"\\n\")\r\n}\r\n",
    "size": 5202,
    "modTime": "2024-11-16T14:50:56.875507+01:00",
    "path": "enrichment.go"
  },
  {
    "name": "input_processing.go",
    "content": "// input_processing.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"fmt\"\r\n\t\"path/filepath\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/parser\"\r\n\t\"github.com/chrlesur/Ontology/internal/storage\"\r\n)\r\n\r\n// parseInput traite le fichier ou le répertoire d'entrée\r\nfunc (p *Pipeline) parseInput(input string) ([]byte, error) {\r\n\tp.logger.Debug(\"Starting parseInput for: %s\", input)\r\n\tif p.storage == nil {\r\n\t\treturn nil, fmt.Errorf(\"storage is not initialized\")\r\n\t}\r\n\r\n\tstorageType := storage.DetectStorageType(input)\r\n\tp.logger.Debug(\"Detected storage type: %s for input: %s\", storageType, input)\r\n\r\n\tswitch storageType {\r\n\tcase storage.S3StorageType:\r\n\t\ts3Storage, ok := p.storage.(*storage.S3Storage)\r\n\t\tif !ok {\r\n\t\t\treturn nil, fmt.Errorf(\"S3 storage is not initialized\")\r\n\t\t}\r\n\t\tbucket, key, err := storage.ParseS3URI(input)\r\n\t\tif err != nil {\r\n\t\t\treturn nil, fmt.Errorf(\"failed to parse S3 URI: %w\", err)\r\n\t\t}\r\n\t\treturn s3Storage.ReadFromBucket(bucket, key)\r\n\tcase storage.LocalStorageType:\r\n\t\treturn p.storage.Read(input)\r\n\tdefault:\r\n\t\treturn nil, fmt.Errorf(\"unsupported storage type: %s\", storageType)\r\n\t}\r\n}\r\n\r\n// parseDirectory traite récursivement un répertoire d'entrée\r\nfunc (p *Pipeline) parseDirectory(dir string) ([]byte, error) {\r\n\tp.logger.Debug(\"Starting parseDirectory for: %s\", dir)\r\n\r\n\tvar content []byte\r\n\tfiles, err := p.storage.List(dir)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Error listing directory contents: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrParseDirectory\"), err)\r\n\t}\r\n\r\n\tfor _, file := range files {\r\n\t\tfilePath := filepath.Join(dir, file)\r\n\t\tisDir, err := p.storage.IsDirectory(filePath)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Warning(\"Error checking if path is directory: %v\", err)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tif !isDir {\r\n\t\t\text := filepath.Ext(file)\r\n\t\t\tparser, err := parser.GetParser(ext)\r\n\t\t\tif err != nil {\r\n\t\t\t\tp.logger.Warning(\"Unsupported file format for %s: %v\", file, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\r\n\t\t\tfileContent, err := p.storage.Read(filePath)\r\n\t\t\tif err != nil {\r\n\t\t\t\tp.logger.Warning(\"Error reading file %s: %v\", file, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\r\n\t\t\t// Créer un io.Reader à partir du contenu du fichier\r\n\t\t\treader := bytes.NewReader(fileContent)\r\n\r\n\t\t\tparsed, err := parser.Parse(reader)\r\n\t\t\tif err != nil {\r\n\t\t\t\tp.logger.Warning(\"Error parsing file %s: %v\", file, err)\r\n\t\t\t\tcontinue\r\n\t\t\t}\r\n\r\n\t\t\tcontent = append(content, parsed...)\r\n\t\t\tp.logger.Debug(\"Parsed file %s, content length: %d bytes\", file, len(parsed))\r\n\t\t}\r\n\t}\r\n\r\n\tp.logger.Debug(\"Finished parsing directory, total content length: %d bytes\", len(content))\r\n\treturn content, nil\r\n}\r\n\r\n// loadExistingOntology charge une ontologie existante à partir d'un fichier\r\nfunc (p *Pipeline) loadExistingOntology(path string) (string, error) {\r\n\tp.logger.Debug(\"Loading existing ontology from: %s\", path)\r\n\r\n\tcontent, err := p.storage.Read(path)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Error reading existing ontology: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrReadExistingOntology\"), err)\r\n\t}\r\n\r\n\tp.logger.Debug(\"Successfully loaded existing ontology, content length: %d bytes\", len(content))\r\n\treturn string(content), nil\r\n}\r\n",
    "size": 3211,
    "modTime": "2024-11-16T14:50:56.875507+01:00",
    "path": "input_processing.go"
  },
  {
    "name": "output_generation.go",
    "content": "// output_generation.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n\t\"encoding/json\"\r\n\t\"fmt\"\r\n\t\"path/filepath\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/metadata\"\r\n)\r\n\r\n// saveResult sauvegarde les résultats de l'ontologie et génère les fichiers de sortie\r\nfunc (p *Pipeline) saveResult(result string, outputPath string, newContent []byte) error {\r\n\tp.logger.Debug(\"Starting saveResult\")\r\n\tp.logger.Info(\"Number of elements in ontology: %d\", len(p.ontology.Elements))\r\n\tp.logger.Info(\"Number of relations in ontology: %d\", len(p.ontology.Relations))\r\n\tp.logger.Debug(\"Writing TSV to: %s\", outputPath)\r\n\r\n\t// Générer le contenu TSV\r\n\ttsvContent := p.generateTSVContent()\r\n\r\n\t// Sauvegarder le fichier TSV\r\n\terr := p.storage.Write(outputPath, []byte(tsvContent))\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to write TSV file: %v\", err)\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrWriteOutput\"), err)\r\n\t}\r\n\tp.logger.Debug(\"TSV file written: %s\", outputPath)\r\n\r\n\tif p.contextOutput {\r\n\t\tcontextFile := strings.TrimSuffix(outputPath, filepath.Ext(outputPath)) + \"_context.json\"\r\n\t\tentities, err := GetAllEntities(p.db)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Error(\"Failed to get all entities: %v\", err)\r\n\t\t\treturn fmt.Errorf(\"failed to get all entities: %w\", err)\r\n\t\t}\r\n\t\tp.ontology.Elements = entities\r\n\t\tp.logger.Debug(\"Updated ontology with %d entities from database\", len(entities))\r\n\r\n\t\tpositionRanges := p.getAllPositionsFromNewContent()\r\n\t\tp.logger.Debug(\"Got %d position ranges from new content\", len(positionRanges))\r\n\r\n\t\tmergedPositions := mergeOverlappingPositions(positionRanges)\r\n\t\tp.logger.Debug(\"Merged to %d position ranges\", len(mergedPositions))\r\n\r\n\t\tpositions := make([]int, len(mergedPositions))\r\n\t\tfor i, pr := range mergedPositions {\r\n\t\t\tpositions[i] = pr.Start\r\n\t\t}\r\n\r\n\t\tcontextJSON, err := GenerateContextJSON(newContent, positions, p.contextWords, mergedPositions)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Error(\"Failed to generate context JSON: %v\", err)\r\n\t\t\treturn fmt.Errorf(\"failed to generate context JSON: %w\", err)\r\n\t\t}\r\n\r\n\t\terr = p.storage.Write(contextFile, []byte(contextJSON))\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Error(\"Failed to write context JSON file: %v\", err)\r\n\t\t\treturn fmt.Errorf(\"failed to write context JSON file: %w\", err)\r\n\t\t}\r\n\t\tp.logger.Info(\"Context JSON saved to: %s\", contextFile)\r\n\t} else {\r\n\t\tp.logger.Debug(\"Context output is disabled. Skipping context JSON generation.\")\r\n\t}\r\n\r\n\t// Générer et sauvegarder les métadonnées\r\n\tmetadataGen := metadata.NewGenerator(p.storage)\r\n\tontologyFile := filepath.Base(outputPath)\r\n\tmeta, err := metadataGen.GenerateMetadata(p.inputPath, ontologyFile, outputPath)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to generate metadata: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to generate metadata: %w\", err)\r\n\t}\r\n\r\n\tmetaContent, err := json.MarshalIndent(meta, \"\", \"  \")\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to marshal metadata: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to marshal metadata: %w\", err)\r\n\t}\r\n\r\n\tmetaFilePath := strings.TrimSuffix(outputPath, filepath.Ext(outputPath)) + \"_meta.json\"\r\n\r\n\terr = p.storage.Write(metaFilePath, metaContent)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to save metadata: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to save metadata: %w\", err)\r\n\t}\r\n\r\n\tp.logger.Info(\"Metadata saved to: %s\", metaFilePath)\r\n\tp.logger.Debug(\"Finished saveResult\")\r\n\treturn nil\r\n}\r\n\r\n// generateTSVContent génère le contenu TSV à partir de l'ontologie\r\nfunc (p *Pipeline) generateTSVContent() string {\r\n\tvar tsvBuilder strings.Builder\r\n\r\n\tp.logger.Debug(\"Starting generateTSVContent\")\r\n\r\n\t// Écrire les éléments\r\n\tfor _, element := range p.ontology.Elements {\r\n\t\tpositions := strings.Trim(strings.Join(strings.Fields(fmt.Sprint(element.Positions)), \",\"), \"[]\")\r\n\t\tline := fmt.Sprintf(\"%s\\t%s\\t%s\\t%s\\n\", element.Name, element.Type, element.Description, positions)\r\n\t\ttsvBuilder.WriteString(line)\r\n\t\tp.logger.Debug(\"Added element to TSV: Name=%s, Type=%s, Description=%s, Positions=%s\",\r\n\t\t\telement.Name, element.Type, element.Description, positions)\r\n\t}\r\n\r\n\t// Écrire les relations\r\n\tfor _, relation := range p.ontology.Relations {\r\n\t\tline := fmt.Sprintf(\"%s\\t%s:%d\\t%s\\t%s\\n\",\r\n\t\t\trelation.Source,\r\n\t\t\trelation.Type,\r\n\t\t\trelation.Weight,\r\n\t\t\trelation.Target,\r\n\t\t\trelation.Description)\r\n\t\ttsvBuilder.WriteString(line)\r\n\t\tp.logger.Debug(\"Added relation to TSV: Source=%s, Type=%s, Weight=%d, Target=%s, Description=%s\",\r\n\t\t\trelation.Source, relation.Type, relation.Weight, relation.Target, relation.Description)\r\n\t}\r\n\r\n\tresult := tsvBuilder.String()\r\n\tp.logger.Debug(\"TSV content generation completed. Total lines: %d\", strings.Count(result, \"\\n\"))\r\n\treturn result\r\n}\r\n",
    "size": 4718,
    "modTime": "2024-11-16T14:50:56.8765076+01:00",
    "path": "output_generation.go"
  },
  {
    "name": "pipeline.go",
    "content": "// pipeline.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n\t\"database/sql\"\r\n\t\"fmt\"\r\n\t\"io/ioutil\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/llm\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/chrlesur/Ontology/internal/model\"\r\n\t\"github.com/chrlesur/Ontology/internal/storage\"\r\n\r\n\t\"github.com/pkoukk/tiktoken-go\"\r\n)\r\n\r\n// ProgressInfo contient les informations sur la progression du traitement\r\ntype ProgressInfo struct {\r\n\tCurrentPass       int\r\n\tTotalPasses       int\r\n\tCurrentStep       string\r\n\tTotalSegments     int\r\n\tProcessedSegments int\r\n}\r\n\r\n// ProgressCallback est une fonction de rappel pour mettre à jour la progression\r\ntype ProgressCallback func(ProgressInfo)\r\n\r\n// Pipeline représente le pipeline de traitement principal\r\ntype Pipeline struct {\r\n\tconfig                   *config.Config\r\n\tlogger                   *logger.Logger\r\n\tllm                      llm.Client\r\n\tprogressCallback         ProgressCallback\r\n\tontology                 *model.Ontology\r\n\tincludePositions         bool\r\n\tcontextOutput            bool\r\n\tcontextWords             int\r\n\tinputPath                string\r\n\tentityExtractionPrompt   string\r\n\trelationExtractionPrompt string\r\n\tontologyEnrichmentPrompt string\r\n\tontologyMergePrompt      string\r\n\tstorage                  storage.Storage\r\n\tmaxConcurrentThreads     int\r\n\tenrichmentPromptFile     string\r\n\tpositionIndex            map[string][]int\r\n\tfullContent              []byte // stocker le contenu complet du document.\r\n\tsegmentOffsets           []int  // stocker les offsets de début de chaque segment.\r\n\tdb                       *sql.DB\r\n}\r\n\r\n// NewPipeline crée une nouvelle instance du pipeline de traitement\r\nfunc NewPipeline(includePositions bool, contextOutput bool, contextWords int, entityPrompt, relationPrompt, enrichmentPrompt, mergePrompt, llmType, llmModel, inputPath string, maxConcurrentThreads int, aiyouAssistantID string, enrichmentPromptFile string) (*Pipeline, error) {\r\n\tcfg := config.GetConfig()\r\n\tlog := logger.GetLogger()\r\n\r\n\tlog.Debug(\"Creating new pipeline instance\")\r\n\r\n\t// Sélection du LLM\r\n\tselectedLLM := cfg.DefaultLLM\r\n\tselectedModel := cfg.DefaultModel\r\n\tif llmType != \"\" {\r\n\t\tselectedLLM = llmType\r\n\t}\r\n\tif llmModel != \"\" {\r\n\t\tselectedModel = llmModel\r\n\t}\r\n\r\n\t// Logique spécifique pour AIYOU\r\n\tif selectedLLM == \"aiyou\" \u0026\u0026 aiyouAssistantID != \"\" {\r\n\t\tselectedModel = aiyouAssistantID\r\n\t\tlog.Debug(\"Using AIYOU with assistant ID: %s\", selectedModel)\r\n\t}\r\n\r\n\tlog.Info(\"Selected LLM: %s, Model: %s\", selectedLLM, selectedModel)\r\n\r\n\t// Initialisation du client LLM\r\n\tclient, err := llm.GetClient(selectedLLM, selectedModel)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to initialize LLM client: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrInitLLMClient\"), err)\r\n\t}\r\n\r\n\t// Initialisation du stockage\r\n\tstorageInstance, err := storage.NewStorage(cfg, inputPath)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Failed to initialize storage: %v\", err)\r\n\t\treturn nil, fmt.Errorf(\"failed to initialize storage: %w\", err)\r\n\t}\r\n\r\n\t// Initialisation de la base sql in memory\r\n\r\n\tdb, err := initDB()\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to initialize database: %w\", err)\r\n\t}\r\n\r\n\tlog.Info(\"Pipeline instance created successfully\")\r\n\r\n\treturn \u0026Pipeline{\r\n\t\tconfig:                   cfg,\r\n\t\tlogger:                   log,\r\n\t\tllm:                      client,\r\n\t\tontology:                 model.NewOntology(),\r\n\t\tincludePositions:         includePositions,\r\n\t\tcontextOutput:            contextOutput,\r\n\t\tcontextWords:             contextWords,\r\n\t\tentityExtractionPrompt:   entityPrompt,\r\n\t\trelationExtractionPrompt: relationPrompt,\r\n\t\tontologyEnrichmentPrompt: enrichmentPrompt,\r\n\t\tontologyMergePrompt:      mergePrompt,\r\n\t\tstorage:                  storageInstance,\r\n\t\tmaxConcurrentThreads:     maxConcurrentThreads,\r\n\t\tenrichmentPromptFile:     enrichmentPromptFile,\r\n\t\tdb:                       db,\r\n\t\tpositionIndex:            make(map[string][]int),\r\n\t}, nil\r\n}\r\n\r\n// SetProgressCallback définit la fonction de rappel pour les mises à jour de progression\r\nfunc (p *Pipeline) SetProgressCallback(callback ProgressCallback) {\r\n\tp.logger.Debug(\"Setting progress callback\")\r\n\tp.progressCallback = callback\r\n}\r\n\r\n// ExecutePipeline orchestre l'ensemble du flux de travail\r\nfunc (p *Pipeline) ExecutePipeline(input string, output string, passes int, existingOntology string, ontology *model.Ontology) error {\r\n\tp.inputPath = input\r\n\tp.logger.Info(i18n.GetMessage(\"StartingPipeline\"))\r\n\tp.logger.Debug(\"Input: %s, Output: %s, Passes: %d, Existing Ontology: %s\", input, output, passes, existingOntology)\r\n\r\n\tvar result string\r\n\tvar err error\r\n\tvar finalContent []byte\r\n\r\n\t// Initialiser le tokenizer\r\n\ttke, err := tiktoken.GetEncoding(\"cl100k_base\")\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to initialize tokenizer: %v\", err)\r\n\t\treturn fmt.Errorf(\"failed to initialize tokenizer: %w\", err)\r\n\t}\r\n\r\n\t// Charger l'ontologie existante si spécifiée\r\n\tif existingOntology != \"\" {\r\n\t\tcontent, err := p.storage.Read(existingOntology)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Error(\"Failed to read existing ontology: %v\", err)\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrLoadExistingOntology\"), err)\r\n\t\t}\r\n\t\tresult = string(content)\r\n\t\ttokenCount := len(tke.Encode(result, nil, nil))\r\n\t\tp.logger.Debug(\"Loaded existing ontology, token count: %d\", tokenCount)\r\n\t}\r\n\r\n\t// Effectuer les passes de traitement\r\n\tfor i := 0; i \u003c passes; i++ {\r\n\t\tif p.progressCallback != nil {\r\n\t\t\tp.progressCallback(ProgressInfo{\r\n\t\t\t\tCurrentPass: i + 1,\r\n\t\t\t\tTotalPasses: passes,\r\n\t\t\t\tCurrentStep: \"Starting Pass\",\r\n\t\t\t})\r\n\t\t}\r\n\t\tinitialTokenCount := len(tke.Encode(result, nil, nil))\r\n\t\tp.logger.Info(\"Starting pass %d with initial result token count: %d\", i+1, initialTokenCount)\r\n\r\n\t\tresult, finalContent, err = p.processSinglePass(input, result, p.includePositions)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Error(i18n.GetMessage(\"ErrProcessingPass\"), i+1, err)\r\n\t\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrProcessingPass\"), err)\r\n\t\t}\r\n\r\n\t\tnewTokenCount := len(tke.Encode(result, nil, nil))\r\n\t\tp.logger.Info(\"Completed pass %d, new result token count: %d\", i+1, newTokenCount)\r\n\t\tp.logger.Info(\"Token count change in pass %d: %d\", i+1, newTokenCount-initialTokenCount)\r\n\t}\r\n\r\n\t// Sauvegarder les résultats\r\n\terr = p.saveResult(result, output, finalContent)\r\n\tif err != nil {\r\n\t\tp.logger.Error(i18n.GetMessage(\"ErrSavingResult\"), err)\r\n\t\treturn fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrSavingResult\"), err)\r\n\t}\r\n\r\n\tp.logger.Info(\"Pipeline execution completed successfully\")\r\n\treturn nil\r\n}\r\n\r\nfunc (p *Pipeline) readPromptFile(filePath string) (string, error) {\r\n\r\n\tif strings.HasPrefix(filePath, \"s3://\") {\r\n\t\t// Lecture depuis S3\r\n\t\tcontent, err := p.storage.Read(filePath)\r\n\t\tif err != nil {\r\n\t\t\treturn \"\", fmt.Errorf(\"failed to read S3 prompt file: %w\", err)\r\n\t\t}\r\n\t\treturn string(content), nil\r\n\t}\r\n\r\n\t// Lecture depuis le système de fichiers local\r\n\tcontent, err := ioutil.ReadFile(filePath)\r\n\tif err != nil {\r\n\t\treturn \"\", fmt.Errorf(\"failed to read local prompt file: %w\", err)\r\n\t}\r\n\treturn string(content), nil\r\n}\r\n\r\nfunc (p *Pipeline) Close() error {\r\n    if p.db != nil {\r\n        return p.db.Close()\r\n    }\r\n    return nil\r\n}",
    "size": 7308,
    "modTime": "2024-11-16T14:50:56.8785094+01:00",
    "path": "pipeline.go"
  },
  {
    "name": "position_utils.go",
    "content": "// position_utils.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n\t\"bytes\"\r\n\t\"sort\"\r\n\t\"strings\"\r\n\r\n\t\"github.com/mozillazg/go-unidecode\"\r\n)\r\n\r\n// PositionRange représente une plage de positions pour un élément\r\ntype PositionRange struct {\r\n\tStart   int\r\n\tEnd     int\r\n\tElement string\r\n}\r\n\r\n// createPositionIndex crée un index des positions pour chaque mot dans le contenu\r\nfunc (p *Pipeline) createPositionIndex(content []byte) map[string][]int {\r\n\tlog.Debug(\"Starting createPositionIndex\")\r\n\tindex := make(map[string][]int)\r\n\twords := bytes.Fields(content)\r\n\r\n\tfor i, word := range words {\r\n\t\tvariants := generateArticleVariants(string(word))\r\n\t\tfor _, variant := range variants {\r\n\t\t\tnormalizedVariant := normalizeWord(variant)\r\n\t\t\tindex[normalizedVariant] = append(index[normalizedVariant], i)\r\n\t\t}\r\n\t}\r\n\r\n\t// Indexer les paires et triplets de mots\r\n\tfor i := 0; i \u003c len(words)-1; i++ {\r\n\t\tpair := normalizeWord(string(words[i])) + \" \" + normalizeWord(string(words[i+1]))\r\n\t\tindex[pair] = append(index[pair], i)\r\n\r\n\t\tif i \u003c len(words)-2 {\r\n\t\t\ttriplet := pair + \" \" + normalizeWord(string(words[i+2]))\r\n\t\t\tindex[triplet] = append(index[triplet], i)\r\n\t\t}\r\n\t}\r\n\r\n\tlog.Debug(\"Finished createPositionIndex. Total indexed terms: %d\", len(index))\r\n\treturn index\r\n}\r\n\r\n// findPositions trouve toutes les positions d'un mot ou d'une phrase dans le contenu\r\nfunc (p *Pipeline) findPositions(entityName string, index map[string][]int, fullContent string) []int {\r\n\tp.logger.Debug(\"Starting findPositions for entity: %s\", entityName)\r\n\r\n\tcontentWords := strings.Fields(fullContent)\r\n\tvar allPositions []int\r\n\r\n\t// Générer les variantes de l'entité\r\n\tvariants := generateEntityVariants(entityName)\r\n\tp.logger.Debug(\"Generated variants for %s: %v\", entityName, variants)\r\n\r\n\tnormalizedFullContent := normalizeString(fullContent)\r\n\r\n\tfor _, variant := range variants {\r\n\t\tnormalizedVariant := normalizeString(variant)\r\n\t\tp.logger.Debug(\"Processing normalized variant: %s\", normalizedVariant)\r\n\r\n\t\t// Recherche exacte en utilisant l'index\r\n\t\tif positions, ok := index[normalizedVariant]; ok {\r\n\t\t\tp.logger.Debug(\"Found exact match positions for %s: %v\", variant, positions)\r\n\t\t\tallPositions = append(allPositions, positions...)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\t// Recherche exacte dans le contenu normalisé si non trouvé dans l'index\r\n\t\tvariantWords := strings.Fields(normalizedVariant)\r\n\t\tfor i := 0; i \u003c= len(contentWords)-len(variantWords); i++ {\r\n\t\t\tmatch := true\r\n\t\t\tfor j, word := range variantWords {\r\n\t\t\t\tif !strings.Contains(normalizeString(contentWords[i+j]), word) {\r\n\t\t\t\t\tmatch = false\r\n\t\t\t\t\tbreak\r\n\t\t\t\t}\r\n\t\t\t}\r\n\t\t\tif match {\r\n\t\t\t\tallPositions = append(allPositions, i)\r\n\t\t\t\tp.logger.Debug(\"Found exact match for %s at position %d\", variant, i)\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t// Si aucune correspondance exacte n'est trouvée, essayer la recherche approximative\r\n\tif len(allPositions) == 0 {\r\n\t\tp.logger.Debug(\"No exact matches found, trying approximate search for: %s\", entityName)\r\n\t\tapproximatePositions := p.findApproximatePositions(entityName, normalizedFullContent)\r\n\t\tallPositions = append(allPositions, approximatePositions...)\r\n\t}\r\n\r\n\t// Dédupliquer et trier les positions trouvées\r\n\tuniquePositions := uniqueIntSlice(allPositions)\r\n\tsort.Ints(uniquePositions)\r\n\r\n\tif len(uniquePositions) \u003e 0 {\r\n\t\tp.logger.Debug(\"Found total unique positions for %s: %v\", entityName, uniquePositions)\r\n\t} else {\r\n\t\tp.logger.Debug(\"No positions found for entity: %s\", entityName)\r\n\t}\r\n\r\n\treturn uniquePositions\r\n}\r\n\r\nfunc generateEntityVariants(entityName string) []string {\r\n\tvariants := []string{entityName}\r\n\tlowercaseEntity := strings.ToLower(entityName)\r\n\r\n\t// Ajouter des variantes avec et sans tirets/underscores\r\n\tif strings.Contains(lowercaseEntity, \"-\") || strings.Contains(lowercaseEntity, \"_\") {\r\n\t\tvariants = append(variants,\r\n\t\t\tstrings.ReplaceAll(lowercaseEntity, \"-\", \" \"),\r\n\t\t\tstrings.ReplaceAll(lowercaseEntity, \"_\", \" \"))\r\n\t}\r\n\r\n\t// Ajouter des variantes avec articles\r\n\tvariants = append(variants,\r\n\t\t\"l'\"+lowercaseEntity,\r\n\t\t\"d'\"+lowercaseEntity,\r\n\t\t\"le \"+lowercaseEntity,\r\n\t\t\"la \"+lowercaseEntity,\r\n\t\t\"les \"+lowercaseEntity)\r\n\r\n\treturn variants\r\n}\r\n\r\n// findApproximatePositions trouve les positions approximatives d'un mot ou d'une phrase\r\nfunc (p *Pipeline) findApproximatePositions(entityName, fullContent string) []int {\r\n\twords := strings.Fields(strings.ToLower(entityName))\r\n\tcontentLower := strings.ToLower(fullContent)\r\n\tvar positions []int\r\n\r\n\t// Recherche exacte de la phrase complète\r\n\tif index := strings.Index(contentLower, strings.Join(words, \" \")); index != -1 {\r\n\t\tpositions = append(positions, index)\r\n\t\tlog.Debug(\"Found exact match for %s at position %d\", entityName, index)\r\n\t\treturn positions\r\n\t}\r\n\r\n\t// Recherche approximative\r\n\tcontentWords := strings.Fields(contentLower)\r\n\tmaxDistance := 10 // Nombre maximum de mots entre les termes recherchés\r\n\r\n\tfor i := 0; i \u003c len(contentWords); i++ {\r\n\t\t//log.Debug(\"Checking approximate match for %s at maximum distance %d\", words, maxDistance)\r\n\t\tif matchFound, endPos := p.checkApproximateMatch(words, contentWords[i:], maxDistance); matchFound {\r\n\t\t\tpositions = append(positions, i)\r\n\t\t\tmatchedPhrase := strings.Join(contentWords[i:i+endPos+1], \" \")\r\n\t\t\tlog.Debug(\"Found approximate match for %s at position %d: %s\", entityName, i, matchedPhrase)\r\n\t\t}\r\n\t}\r\n\r\n\treturn positions\r\n}\r\n\r\n// checkApproximateMatch vérifie si une correspondance approximative est trouvée\r\nfunc (p *Pipeline) checkApproximateMatch(searchWords, contentWords []string, maxDistance int) (bool, int) {\r\n\t//p.logger.Debug(\"Starting checkApproximateMatch with searchWords: %v, maxDistance: %d\", searchWords, maxDistance)\r\n\r\n\t// Normaliser les mots de recherche\r\n\tnormalizedSearchWords := make([]string, len(searchWords))\r\n\tfor i, word := range searchWords {\r\n\t\tnormalizedSearchWords[i] = normalizeString(word)\r\n\t}\r\n\r\n\twordIndex := 0\r\n\tdistanceCount := 0\r\n\tfor i, word := range contentWords {\r\n\t\tnormalizedWord := normalizeString(word)\r\n\t\t//p.logger.Debug(\"Checking normalized word: '%s' at position %d\", normalizedWord, i)\r\n\r\n\t\tif strings.Contains(normalizedWord, normalizedSearchWords[wordIndex]) {\r\n\t\t\t//p.logger.Debug(\"Match found for '%s' in '%s'\", normalizedSearchWords[wordIndex], normalizedWord)\r\n\t\t\twordIndex++\r\n\t\t\tdistanceCount = 0\r\n\t\t\t//p.logger.Debug(\"wordIndex incremented to %d, distanceCount reset to 0\", wordIndex)\r\n\r\n\t\t\tif wordIndex == len(normalizedSearchWords) {\r\n\t\t\t\tp.logger.Debug(\"All search words matched. Returning true with end position %d\", i)\r\n\t\t\t\treturn true, i\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\tdistanceCount++\r\n\t\t\t// p.logger.Debug(\"No match. distanceCount incremented to %d\", distanceCount)\r\n\r\n\t\t\tif distanceCount \u003e maxDistance {\r\n\t\t\t\t//p.logger.Debug(\"Max distance exceeded. Returning false\")\r\n\t\t\t\treturn false, -1\r\n\t\t\t}\r\n\t\t}\r\n\t}\r\n\r\n\t//p.logger.Debug(\"End of content words reached without full match. Returning false\")\r\n\treturn false, -1\r\n}\r\n\r\n// getAllPositionsFromNewContent récupère toutes les positions des éléments dans le nouveau contenu\r\nfunc (p *Pipeline) getAllPositionsFromNewContent() []PositionRange {\r\n\tvar allPositions []PositionRange\r\n\tfor _, element := range p.ontology.Elements {\r\n\t\tpositions := element.Positions\r\n\t\tfor _, pos := range positions {\r\n\t\t\tallPositions = append(allPositions, PositionRange{\r\n\t\t\t\tStart:   pos,\r\n\t\t\t\tEnd:     pos + len(strings.Fields(element.Name)) - 1,\r\n\t\t\t\tElement: element.Name,\r\n\t\t\t})\r\n\t\t}\r\n\t}\r\n\tlog.Debug(\"Total position ranges collected from ontology: %d\", len(allPositions))\r\n\treturn allPositions\r\n}\r\n\r\n// mergeOverlappingPositions fusionne les plages de positions qui se chevauchent\r\nfunc mergeOverlappingPositions(positions []PositionRange) []PositionRange {\r\n\tif len(positions) == 0 {\r\n\t\treturn positions\r\n\t}\r\n\r\n\tsort.Slice(positions, func(i, j int) bool {\r\n\t\treturn positions[i].Start \u003c positions[j].Start\r\n\t})\r\n\r\n\tmerged := []PositionRange{positions[0]}\r\n\r\n\tfor _, current := range positions[1:] {\r\n\t\tlast := \u0026merged[len(merged)-1]\r\n\t\tif current.Start \u003c= last.End+1 {\r\n\t\t\tif current.End \u003e last.End {\r\n\t\t\t\tlast.End = current.End\r\n\t\t\t}\r\n\t\t\tif len(current.Element) \u003e len(last.Element) {\r\n\t\t\t\tlast.Element = current.Element\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\tmerged = append(merged, current)\r\n\t\t}\r\n\t}\r\n\r\n\treturn merged\r\n}\r\n\r\nfunc normalizeString(s string) string {\r\n\treturn strings.ToLower(unidecode.Unidecode(s))\r\n}\r\n",
    "size": 8311,
    "modTime": "2024-11-16T14:50:56.8785094+01:00",
    "path": "position_utils.go"
  },
  {
    "name": "segmentation.go",
    "content": "// segmentation.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n\t\"database/sql\"\r\n\t\"fmt\"\r\n\t\"path/filepath\"\r\n\t\"strconv\"\r\n\t\"strings\"\r\n\t\"sync\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/i18n\"\r\n\t\"github.com/chrlesur/Ontology/internal/model\"\r\n\t\"github.com/chrlesur/Ontology/internal/parser\"\r\n\t\"github.com/chrlesur/Ontology/internal/prompt\"\r\n\t\"github.com/chrlesur/Ontology/internal/segmenter\"\r\n\r\n\t\"github.com/pkoukk/tiktoken-go\"\r\n)\r\n\r\n// processSinglePass traite une seule passe de l'ensemble du contenu\r\n\r\nfunc (p *Pipeline) processSinglePass(input string, previousResult string, includePositions bool) (string, []byte, error) {\r\n\tp.logger.Debug(\"Démarrage du traitement d'une passe unique pour l'entrée : %s\", input)\r\n\r\n\tisDir, err := p.storage.IsDirectory(input)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Échec de la vérification si l'entrée est un répertoire : %v\", err)\r\n\t\treturn \"\", nil, fmt.Errorf(\"échec de la vérification si l'entrée est un répertoire : %w\", err)\r\n\t}\r\n\r\n\tvar content []byte\r\n\tif isDir {\r\n\t\tcontent, err = p.readDirectory(input)\r\n\t} else {\r\n\t\tcontent, err = p.readFile(input)\r\n\t}\r\n\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Échec de la lecture de l'entrée : %v\", err)\r\n\t\treturn \"\", nil, fmt.Errorf(\"échec de la lecture de l'entrée : %w\", err)\r\n\t}\r\n\r\n\tif len(content) == 0 {\r\n\t\tp.logger.Error(\"Aucun contenu trouvé dans l'entrée : %s\", input)\r\n\t\treturn \"\", nil, fmt.Errorf(\"aucun contenu trouvé dans l'entrée\")\r\n\t}\r\n\r\n\tp.fullContent = content\r\n\tp.positionIndex = p.createPositionIndex(p.fullContent)\r\n\tp.logger.Debug(\"Position index created. Number of entries: %d\", len(p.positionIndex))\r\n\r\n\t// Initialisation du tokenizer\r\n\ttke, err := tiktoken.GetEncoding(\"cl100k_base\")\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Échec de l'initialisation du tokenizer : %v\", err)\r\n\t\treturn \"\", nil, fmt.Errorf(\"échec de l'initialisation du tokenizer : %w\", err)\r\n\t}\r\n\r\n\tcontentTokens := len(tke.Encode(string(content), nil, nil))\r\n\tp.logger.Info(\"Nombre de tokens du contenu d'entrée : %d\", contentTokens)\r\n\r\n\tp.fullContent = content\r\n\tpositionIndex := p.createPositionIndex(p.fullContent)\r\n\tp.logger.Debug(\"Index de position créé. Nombre d'entrées : %d\", len(positionIndex))\r\n\r\n\tsegments, offsets, err := segmenter.Segment(content, segmenter.SegmentConfig{\r\n\t\tMaxTokens:   p.config.MaxTokens,\r\n\t\tContextSize: p.config.ContextSize,\r\n\t\tModel:       p.config.DefaultModel,\r\n\t})\r\n\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Échec de la segmentation du contenu : %v\", err)\r\n\t\treturn \"\", nil, fmt.Errorf(\"%s: %w\", i18n.GetMessage(\"ErrSegmentContent\"), err)\r\n\t}\r\n\r\n\tp.segmentOffsets = offsets\r\n\r\n\tp.logger.Info(\"Nombre de segments : %d\", len(segments))\r\n\r\n\tif p.progressCallback != nil {\r\n\t\tp.progressCallback(ProgressInfo{\r\n\t\t\tCurrentStep:   \"Segmentation\",\r\n\t\t\tTotalSegments: len(segments),\r\n\t\t})\r\n\t}\r\n\r\n\tresults := make([]string, len(segments))\r\n\tvar wg sync.WaitGroup\r\n\tsem := make(chan struct{}, p.maxConcurrentThreads)\r\n\r\n\tfor i, segment := range segments {\r\n\t\twg.Add(1)\r\n\t\tgo func(i int, seg segmenter.SegmentInfo) {\r\n\t\t\tdefer wg.Done()\r\n\r\n\t\t\tsem \u003c- struct{}{}\r\n\t\t\tdefer func() { \u003c-sem }()\r\n\r\n\t\t\tsegmentTokens := len(tke.Encode(string(seg.Content), nil, nil))\r\n\t\t\tp.logger.Debug(\"Traitement du segment %d/%d, Début : %d, Fin : %d, Longueur : %d octets, Tokens : %d\",\r\n\t\t\t\ti+1, len(segments), seg.Start, seg.End, len(seg.Content), segmentTokens)\r\n\r\n\t\t\tcontext := segmenter.GetContext(segments, i, segmenter.SegmentConfig{\r\n\t\t\t\tMaxTokens:   p.config.MaxTokens,\r\n\t\t\t\tContextSize: p.config.ContextSize,\r\n\t\t\t\tModel:       p.config.DefaultModel,\r\n\t\t\t})\r\n\t\t\tp.logger.Debug(\"Contexte pour le segment %d/%d, Longueur : %d octets\", i+1, len(segments), len(context))\r\n\r\n\t\t\tresult, err := p.processSegment(seg.Content, context, previousResult, positionIndex, includePositions, seg.Start)\r\n\t\t\tif err != nil {\r\n\t\t\t\tp.logger.Error(i18n.GetMessage(\"SegmentProcessingError\"), i+1, err)\r\n\t\t\t\treturn\r\n\t\t\t}\r\n\t\t\tresultTokens := len(tke.Encode(result, nil, nil))\r\n\t\t\tresults[i] = result\r\n\t\t\tp.logger.Info(\"Segment %d traité avec succès, nombre de tokens du résultat : %d\", i+1, resultTokens)\r\n\t\t\tif p.progressCallback != nil {\r\n\t\t\t\tp.progressCallback(ProgressInfo{\r\n\t\t\t\t\tCurrentStep:       \"Traitement du Segment\",\r\n\t\t\t\t\tProcessedSegments: i + 1,\r\n\t\t\t\t\tTotalSegments:     len(segments),\r\n\t\t\t\t})\r\n\t\t\t}\r\n\t\t}(i, segment)\r\n\t}\r\n\twg.Wait()\r\n\r\n\t// Utilisation de la nouvelle fonction mergeResultsWithDB\r\n\tmergedResult, err := p.mergeResultsWithDB(previousResult, results)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Échec de la fusion des résultats : %v\", err)\r\n\t\treturn \"\", nil, fmt.Errorf(\"échec de la fusion des résultats : %w\", err)\r\n\t}\r\n\r\n\tmergedResultTokens := len(tke.Encode(mergedResult, nil, nil))\r\n\tp.logger.Info(\"Nombre de tokens du résultat fusionné : %d\", mergedResultTokens)\r\n\tp.logger.Debug(\"Traitement de la passe unique terminé. Longueur du résultat fusionné : %d\", len(mergedResult))\r\n\treturn mergedResult, content, nil\r\n}\r\n\r\n// mergeResults fusionne les résultats de tous les segments\r\nfunc (p *Pipeline) mergeResults(previousResult string, newResults []string) (string, error) {\r\n\tlog.Info(\"Starting mergeResults. Previous result length: %d, Number of new results: %d\", len(previousResult), len(newResults))\r\n\r\n\t// Combiner tous les nouveaux résultats\r\n\tcombinedNewResults := strings.Join(newResults, \"\\n\")\r\n\tlog.Debug(\"Combined new results length: %d\", len(combinedNewResults))\r\n\tlog.Debug(\"--- Combined Results ---\")\r\n\tlog.Debug(\"%s\", combinedNewResults)\r\n\tlog.Debug(\"--- ---\")\r\n\r\n\t// Préparer les valeurs pour le prompt de fusion\r\n\tmergeValues := map[string]string{\r\n\t\t\"previous_ontology\": previousResult,\r\n\t\t\"new_ontology\":      combinedNewResults,\r\n\t\t\"additional_prompt\": p.ontologyMergePrompt,\r\n\t}\r\n\r\n\t// Utiliser le LLM pour fusionner les résultats\r\n\tlog.Debug(\"Calling LLM with OntologyMergePrompt\")\r\n\r\n\tmergedResult, err := p.llm.ProcessWithPrompt(prompt.OntologyMergePrompt, mergeValues)\r\n\tif err != nil {\r\n\t\tlog.Error(\"Ontology merge failed: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"ontology merge failed: %w\", err)\r\n\t}\r\n\r\n\t// Normaliser le résultat fusionné\r\n\tnormalizedMergedResult := normalizeTSV(mergedResult)\r\n\tlog.Debug(\"--- Normalized Results ---\")\r\n\tlog.Debug(\"%s\", normalizedMergedResult)\r\n\tlog.Debug(\"--- ---\")\r\n\r\n\tlog.Debug(\"Merged result length: %d\", len(normalizedMergedResult))\r\n\treturn normalizedMergedResult, nil\r\n}\r\n\r\nfunc (p *Pipeline) processMetadata(metadata map[string]string) {\r\n\tp.logger.Debug(\"Processing metadata\")\r\n\tfor key, value := range metadata {\r\n\t\t// Logique pour traiter chaque métadonnée\r\n\t\tp.logger.Debug(\"Metadata: %s = %s\", key, value)\r\n\t}\r\n}\r\n\r\nfunc (p *Pipeline) readDirectory(dirPath string) ([]byte, error) {\r\n\tp.logger.Debug(\"Reading directory: %s\", dirPath)\r\n\r\n\tfiles, err := p.storage.List(dirPath)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to list directory contents: %w\", err)\r\n\t}\r\n\r\n\tvar allContent []byte\r\n\tfor _, filePath := range files {\r\n\t\t// Utiliser le chemin tel quel, sans le joindre à dirPath\r\n\t\tcontent, err := p.readFile(filePath)\r\n\t\tif err != nil {\r\n\t\t\tp.logger.Warning(\"Failed to read file %s: %v\", filePath, err)\r\n\t\t\tcontinue\r\n\t\t}\r\n\r\n\t\tallContent = append(allContent, content...)\r\n\t\tallContent = append(allContent, '\\n') // Add separator between files\r\n\t}\r\n\r\n\tif len(allContent) == 0 {\r\n\t\treturn nil, fmt.Errorf(\"no content found in directory: %s\", dirPath)\r\n\t}\r\n\r\n\treturn allContent, nil\r\n}\r\n\r\nfunc (p *Pipeline) readFile(filePath string) ([]byte, error) {\r\n\text := filepath.Ext(filePath)\r\n\tparser, err := parser.GetParser(ext)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to get parser for file %s: %w\", filePath, err)\r\n\t}\r\n\r\n\treader, err := p.storage.GetReader(filePath)\r\n\tif err != nil {\r\n\t\treturn nil, fmt.Errorf(\"failed to get reader for file %s: %w\", filePath, err)\r\n\t}\r\n\tdefer reader.Close()\r\n\r\n\treturn parser.Parse(reader)\r\n}\r\n\r\n// mergeResultsWithDB fusionne les résultats précédents avec les nouveaux résultats en utilisant une base de données temporaire.\r\nfunc (p *Pipeline) mergeResultsWithDB(previousResult string, newResults []string) (string, error) {\r\n\tp.logger.Debug(\"Starting mergeResultsWithDB\")\r\n\tp.logger.Debug(\"Inserting previous result: %s\", previousResult)\r\n\tif err := p.insertResults(p.db, previousResult); err != nil {\r\n\t\tp.logger.Error(\"Failed to insert previous result: %v\", err)\r\n\t\treturn \"\", err\r\n\t}\r\n\r\n\tfor i, result := range newResults {\r\n\t\tp.logger.Debug(\"Inserting new result %d: %s\", i, result)\r\n\t\tif err := p.insertResults(p.db, result); err != nil {\r\n\t\t\tp.logger.Error(\"Failed to insert new result %d: %v\", i, err)\r\n\t\t\treturn \"\", err\r\n\t\t}\r\n\t}\r\n\r\n\tmergedResult, err := p.getMergedResults(p.db)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to get merged results: %v\", err)\r\n\t\treturn \"\", err\r\n\t}\r\n\r\n\tif mergedResult == \"\" {\r\n\t\tp.logger.Warning(\"Merged result is empty\")\r\n\t} else {\r\n\t\tp.logger.Debug(\"Merged result: %s\", mergedResult)\r\n\t}\r\n\r\n\treturn mergedResult, nil\r\n}\r\n\r\n// insertResults insère les résultats dans la base de données.\r\nfunc (p *Pipeline) insertResults(db *sql.DB, result string) error {\r\n\tlines := strings.Split(result, \"\\n\")\r\n\tfor _, line := range lines {\r\n\t\tline = strings.TrimSpace(line)\r\n\t\tif line == \"\" {\r\n\t\t\tcontinue // Skip empty lines\r\n\t\t}\r\n\t\tparts := strings.Split(line, \"\\t\")\r\n\t\tp.logger.Debug(\"Processing line with %d parts: %v\", len(parts), parts)\r\n\r\n\t\tif len(parts) \u003e= 3 {\r\n\t\t\t// Vérifier si c'est une relation ou une entité\r\n\t\t\tif strings.Contains(parts[1], \":\") {\r\n\t\t\t\t// C'est une relation\r\n\t\t\t\trelationParts := strings.SplitN(parts[1], \":\", 2)\r\n\t\t\t\tif len(relationParts) != 2 {\r\n\t\t\t\t\tp.logger.Warning(\"Invalid relation format: %v\", parts)\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t\trelationType := relationParts[0]\r\n\t\t\t\t// Extraire l'entier de la chaîne formatée\r\n\t\t\t\tstrengthStr := relationParts[1]\r\n\t\t\t\tstrengthStr = strings.TrimPrefix(strengthStr, \"%!f(int=\")\r\n\t\t\t\tstrengthStr = strings.TrimSuffix(strengthStr, \")\")\r\n\r\n\t\t\t\tstrength, err := strconv.Atoi(strengthStr)\r\n\t\t\t\tif err != nil {\r\n\t\t\t\t\tp.logger.Warning(\"Invalid strength value: %v\", err)\r\n\t\t\t\t\tcontinue\r\n\t\t\t\t}\r\n\t\t\t\t\r\n\t\t\t\ttarget := parts[2]\r\n\t\t\t\tdescription := strings.Join(parts[3:], \" \")\r\n\r\n\t\t\t\trelation := \u0026model.Relation{\r\n\t\t\t\t\tSource:      strings.TrimSpace(parts[0]),\r\n\t\t\t\t\tType:        strings.TrimSpace(relationType),\r\n\t\t\t\t\tTarget:      strings.TrimSpace(target),\r\n\t\t\t\t\tDescription: strings.TrimSpace(description),\r\n\t\t\t\t\tWeight:      strength,\r\n\t\t\t\t\tCreatedAt:   time.Now(),\r\n\t\t\t\t\tUpdatedAt:   time.Now(),\r\n\t\t\t\t}\r\n\t\t\t\tp.logger.Debug(\"Relation object before upsert: %+v\", relation)\r\n\t\t\t\tif err := UpsertRelation(db, relation); err != nil {\r\n\t\t\t\t\tp.logger.Error(\"Failed to upsert relation: %v\", err)\r\n\t\t\t\t\treturn err\r\n\t\t\t\t}\r\n\r\n\t\t\t\tp.logger.Debug(\"Relation upserted successfully\")\r\n\t\t\t} else {\r\n\t\t\t\t// C'est une entité\r\n\t\t\t\tdescription := \"\"\r\n\t\t\t\tif len(parts) \u003e 3 {\r\n\t\t\t\t\tdescription = strings.Join(parts[3:], \" \")\r\n\t\t\t\t} else if len(parts) == 3 {\r\n\t\t\t\t\tdescription = parts[2]\r\n\t\t\t\t}\r\n\t\t\t\tentity := \u0026model.OntologyElement{\r\n\t\t\t\t\tName:        strings.TrimSpace(parts[0]),\r\n\t\t\t\t\tType:        strings.TrimSpace(parts[1]),\r\n\t\t\t\t\tDescription: strings.TrimSpace(description),\r\n\t\t\t\t\tPositions:   p.findPositions(strings.TrimSpace(parts[0]), p.positionIndex, string(p.fullContent)),\r\n\t\t\t\t\tCreatedAt:   time.Now(),\r\n\t\t\t\t\tUpdatedAt:   time.Now(),\r\n\t\t\t\t}\r\n\t\t\t\tp.logger.Debug(\"Entity object before upsert: %+v\", entity)\r\n\t\t\t\tif err := UpsertEntity(db, entity); err != nil {\r\n\t\t\t\t\tp.logger.Error(\"Failed to upsert entity: %v\", err)\r\n\t\t\t\t\treturn err\r\n\t\t\t\t}\r\n\t\t\t\tp.logger.Debug(\"Entity upserted successfully\")\r\n\t\t\t}\r\n\t\t} else {\r\n\t\t\tp.logger.Warning(\"Skipping invalid line: %v\", parts)\r\n\t\t}\r\n\t}\r\n\treturn nil\r\n}\r\n\r\n// getMergedResults récupère les résultats fusionnés de la base de données.\r\nfunc (p *Pipeline) getMergedResults(db *sql.DB) (string, error) {\r\n\tp.logger.Debug(\"Starting getMergedResults\")\r\n\tvar result strings.Builder\r\n\r\n\t// Récupérer et écrire les entités\r\n\tentities, err := GetAllEntities(db)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to get all entities: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"failed to get all entities: %w\", err)\r\n\t}\r\n\tp.logger.Debug(\"Retrieved %d entities\", len(entities))\r\n\r\n\tfor _, entity := range entities {\r\n\t\tpositionsStr := strings.Trim(strings.Join(strings.Fields(fmt.Sprint(entity.Positions)), \",\"), \"[]\")\r\n\t\tline := fmt.Sprintf(\"%s\\t%s\\t%s\\t%s\\n\",\r\n\t\t\tentity.Name,\r\n\t\t\tentity.Type,\r\n\t\t\tentity.Description,\r\n\t\t\tpositionsStr)\r\n\t\tresult.WriteString(line)\r\n\t\tp.logger.Debug(\"Added entity to result: %s\", strings.TrimSpace(line))\r\n\t}\r\n\r\n\t// Récupérer et écrire les relations\r\n\trelations, err := GetAllRelations(db)\r\n\tif err != nil {\r\n\t\tp.logger.Error(\"Failed to get all relations: %v\", err)\r\n\t\treturn \"\", fmt.Errorf(\"failed to get all relations: %w\", err)\r\n\t}\r\n\tp.logger.Debug(\"Retrieved %d relations\", len(relations))\r\n\r\n\tfor _, relation := range relations {\r\n\t\tline := fmt.Sprintf(\"%s\\t%s:%d\\t%s\\t%s\\n\",\r\n\t\t\trelation.Source,\r\n\t\t\trelation.Type,\r\n\t\t\tint(relation.Weight),\r\n\t\t\trelation.Target,\r\n\t\t\trelation.Description)\r\n\t\tresult.WriteString(line)\r\n\t\tp.logger.Debug(\"Added relation to result: %s\", strings.TrimSpace(line))\r\n\t}\r\n\r\n\t// Mettre à jour l'ontologie\r\n\tp.ontology.Elements = entities\r\n\tp.ontology.Relations = relations\r\n\tp.logger.Info(\"Updated ontology with %d elements and %d relations\", len(entities), len(relations))\r\n\r\n\tfinalResult := result.String()\r\n\tp.logger.Debug(\"Final merged result length: %d characters\", len(finalResult))\r\n\treturn finalResult, nil\r\n}\r\n",
    "size": 13436,
    "modTime": "2024-11-16T14:50:56.8795104+01:00",
    "path": "segmentation.go"
  },
  {
    "name": "segmentation_test.go",
    "content": "// pipeline/segmentation_test.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n\t\"fmt\"\r\n\t\"testing\"\r\n\t\"time\"\r\n\r\n\t\"github.com/chrlesur/Ontology/internal/config\"\r\n\t\"github.com/chrlesur/Ontology/internal/logger\"\r\n\t\"github.com/stretchr/testify/assert\"\r\n)\r\n\r\n// newTestPipeline crée une instance de Pipeline pour les tests\r\nfunc newTestPipeline() *Pipeline {\r\n\tcfg := \u0026config.Config{\r\n\t\t// Ajoutez ici les configurations nécessaires pour le test\r\n\t\tMaxTokens:    1000,\r\n\t\tContextSize:  2000,\r\n\t\tDefaultLLM:   \"test-llm\",\r\n\t\tDefaultModel: \"test-model\",\r\n\t}\r\n\r\n\treturn \u0026Pipeline{\r\n\t\tlogger: logger.GetLogger(),\r\n\t\tconfig: cfg,\r\n\t\t// Ajoutez d'autres champs nécessaires pour le test\r\n\t}\r\n}\r\n\r\nfunc TestMergeResultsWithDB(t *testing.T) {\r\n\tp := newTestPipeline()\r\n\r\n\tnow := time.Now().UTC()\r\n\tnowStr := now.Format(time.RFC3339)\r\n\tpreviousResult := fmt.Sprintf(\"Entity1\\tType1\\tDescription1\\t%s\\t%s\\tSource1\\n\", nowStr, nowStr)\r\n\tnewResults := []string{\r\n\t\tfmt.Sprintf(\"Entity2\\tType2\\tUpdated Description2\\t%s\\t%s\\tSource2\\n\", nowStr, nowStr),\r\n\t\tfmt.Sprintf(\"Entity3\\tType3\\tDescription3\\t%s\\t%s\\tSource3\\n\", nowStr, nowStr),\r\n\t\tfmt.Sprintf(\"Relation1\\tRelType\\tEntity1\\tEntity2\\tDescription\\t0.50\\tforward\\t%s\\t%s\\n\", nowStr, nowStr),\r\n\t}\r\n\r\n\tt.Logf(\"Previous result: %s\", previousResult)\r\n\tt.Logf(\"New results: %v\", newResults)\r\n\r\n\tmergedResult, err := p.mergeResultsWithDB(previousResult, newResults)\r\n\r\n\tassert.NoError(t, err)\r\n\tt.Logf(\"Merged result: %s\", mergedResult)\r\n\r\n\texpectedResult := fmt.Sprintf(\"Entity1\\tType1\\tDescription1\\t%s\\t%s\\tSource1\\n\"+\r\n\t\t\"Entity2\\tType2\\tUpdated Description2\\t%s\\t%s\\tSource2\\n\"+\r\n\t\t\"Entity3\\tType3\\tDescription3\\t%s\\t%s\\tSource3\\n\"+\r\n\t\t\"Relation1\\tRelType\\tEntity1\\tEntity2\\tDescription\\t0.50\\tforward\\t%s\\t%s\\n\",\r\n\t\tnowStr, nowStr, nowStr, nowStr, nowStr, nowStr, nowStr, nowStr)\r\n\r\n\tassert.Equal(t, expectedResult, mergedResult)\r\n}\r\n",
    "size": 1858,
    "modTime": "2024-11-16T14:50:56.8805115+01:00",
    "path": "segmentation_test.go"
  },
  {
    "name": "string_utils.go",
    "content": "// string_utils.go\r\n\r\npackage pipeline\r\n\r\nimport (\r\n    \"strings\"\r\n    \"unicode\"\r\n)\r\n\r\n// truncateString tronque une chaîne à une longueur maximale donnée\r\nfunc truncateString(s string, maxLength int) string {\r\n    if len(s) \u003c= maxLength {\r\n        return s\r\n    }\r\n    return s[:maxLength] + \"...\"\r\n}\r\n\r\n// normalizeWord normalise un mot en le convertissant en minuscules et en ne gardant que les lettres, les chiffres et les apostrophes\r\nfunc normalizeWord(word string) string {\r\n    return strings.TrimSpace(strings.Map(func(r rune) rune {\r\n        if unicode.IsLetter(r) || unicode.IsNumber(r) || r == '\\'' {\r\n            return unicode.ToLower(r)\r\n        }\r\n        return ' '\r\n    }, word))\r\n}\r\n\r\n// generateArticleVariants génère des variantes d'un mot avec différents articles\r\nfunc generateArticleVariants(word string) []string {\r\n    variants := []string{word}\r\n    lowercaseWord := strings.ToLower(word)\r\n\r\n    // Ajouter des variantes avec et sans apostrophe\r\n    if !strings.HasPrefix(lowercaseWord, \"l'\") \u0026\u0026 !strings.HasPrefix(lowercaseWord, \"d'\") {\r\n        variants = append(variants, \"l'\"+lowercaseWord, \"d'\"+lowercaseWord, \"l \"+lowercaseWord, \"d \"+lowercaseWord)\r\n    }\r\n\r\n    // Ajouter une variante sans underscore si le mot en contient\r\n    if strings.Contains(word, \"_\") {\r\n        spaceVariant := strings.ReplaceAll(word, \"_\", \" \")\r\n        variants = append(variants, spaceVariant)\r\n        variants = append(variants, \"l'\"+spaceVariant, \"d'\"+spaceVariant, \"l \"+spaceVariant, \"d \"+spaceVariant)\r\n    }\r\n\r\n    return variants\r\n}\r\n\r\n// uniqueIntSlice supprime les doublons dans une slice d'entiers\r\nfunc uniqueIntSlice(intSlice []int) []int {\r\n    keys := make(map[int]bool)\r\n    list := []int{}\r\n    for _, entry := range intSlice {\r\n        if _, value := keys[entry]; !value {\r\n            keys[entry] = true\r\n            list = append(list, entry)\r\n        }\r\n    }\r\n    return list\r\n}\r\n\r\n// min retourne le minimum entre deux entiers\r\nfunc min(a, b int) int {\r\n    if a \u003c b {\r\n        return a\r\n    }\r\n    return b\r\n}\r\n\r\n// max retourne le maximum entre deux entiers\r\nfunc max(a, b int) int {\r\n    if a \u003e b {\r\n        return a\r\n    }\r\n    return b\r\n}",
    "size": 2184,
    "modTime": "2024-11-16T14:50:56.8815122+01:00",
    "path": "string_utils.go"
  }
]