[
  {
    "name": "expression.besoin.revise.md",
    "content": "# Créateur d'ontologie\r\n\r\n## Version\r\n\r\nVersion 0.2.1 Révision détaillée basée sur les retours d'implémentation\r\n\r\n## Aperçu du projet\r\n\r\nDévelopper un logiciel en Go qui, à partir de divers formats de documents (texte, PDF, Markdown, HTML, DOCX), crée une ontologie au format QuickStatement pour être compatible avec Wikibase. Le logiciel doit identifier et extraire chaque élément d'information du document d'entrée, tout en gérant efficacement les très grands documents.\r\n\r\n## Fonctionnalités principales\r\n\r\n### 1. Support Multi-format d'Entrée\r\n- Accepter les entrées en formats texte, PDF, Markdown, HTML et DOCX\r\n- Implémenter des analyseurs robustes pour chaque format supporté\r\n- Pour les PDFs, utiliser la bibliothèque github.com/ledongthuc/pdf pour l'extraction de texte et de métadonnées\r\n- Prévoir une phase d'évaluation approfondie des bibliothèques PDF pour s'assurer de leur compatibilité et fonctionnalité\r\n- Prise en charge de très grands documents (dépassant 120 000 tokens)\r\n- Extraction cohérente des métadonnées à travers les différents formats\r\n\r\n### 2. Génération d'une ontologie au format QuickStatement / Wikibase\r\n- Générer une sortie QuickStatement détaillée utilisant le vocabulaire Wikibase\r\n- Spécifier en détail le format exact attendu pour les sorties QuickStatement\r\n- Implémenter un traitement complexe des chaînes de caractères pour gérer correctement les caractères d'échappement, notamment les doubles backslashes et les '\\t'\r\n- Implémenter un système de nettoyage et de normalisation des entrées pour gérer les variations de format\r\n- Assurer une extraction complète des informations des documents d'entrée\r\n- Découper les documents larges en segments de maximum 4000 tokens\r\n- S'assurer que chaque élément de l'ontologie est unitaire entre les segments et que le tout est cohérent\r\n- Traiter plusieurs documents en utilisant la même ontologie\r\n- Le résultat de l'exécution du logiciel est un fichier ayant l'extension .tsv (tab separated value)\r\n- Ajouter des options d'export en formats RDF et OWL\r\n\r\n### 3. Segmentation et traitement du contenu\r\n- Implémenter une segmentation sophistiquée créant des segments cohérents tout en respectant les limites de tokens\r\n- Utiliser la bibliothèque tiktoken-go pour le comptage précis des tokens, en l'optimisant pour de grands volumes de texte\r\n- Optimiser la fonction de segmentation pour gérer efficacement de grands volumes de texte\r\n- Implémenter une gestion efficace de la mémoire pour les très grands documents\r\n- Assurer une intégration fluide entre le segmenter et le client LLM, en ajustant la taille des segments et la gestion du contexte\r\n\r\n### 4. Intégration LLM\r\n- Supporter plusieurs LLMs, au minimum OpenAI GPT-4, Claude 3.5 Sonnet, Ollama\r\n- Implémenter une gestion robuste des limites de taux des API LLM, incluant un système de \"token bucket\" dans le client Claude (claude.go)\r\n- Implémenter un backoff exponentiel avec un maximum de 5 tentatives pour les erreurs de limite de taux\r\n- Gérer les différences entre les APIs d'OpenAI, Claude, et Ollama avec des adaptateurs spécifiques\r\n- Optimiser les appels API aux LLMs pour réduire les coûts et les temps de traitement\r\n\r\n### 5. Système de Prompts\r\n- Implémenter un système de templates de prompts sophistiqué et flexible\r\n- Créer une structure PromptTemplate avec des méthodes pour formater les prompts\r\n- Créer des prompts spécifiques pour l'extraction d'entités et de relations\r\n- Assurer la compatibilité des prompts avec différents LLMs\r\n\r\n### 6. Système de Journalisation\r\n- Implémenter un système de journalisation polyvalent avec support pour les niveaux debug, info, warning et error\r\n- Ajouter un mode de débogage détaillé activable via une option --debug\r\n- Implémenter une structure de logger avec des niveaux de log et des méthodes associées (SetLevel, GetLevel)\r\n- Assurer que l'activation du mode debug n'affecte pas significativement les performances en mode normal\r\n- Exporter les logs vers des fichiers texte et les afficher sur la console\r\n- Implémenter un mode silencieux (--silent) pour désactiver la sortie console des logs\r\n\r\n### 7. Gestion de la Configuration\r\n- Utiliser YAML pour une configuration centralisée\r\n- Permettre des surcharges de paramètres par ligne de commande\r\n- Inclure des options de configuration pour les différents LLMs et leurs modèles spécifiques\r\n\r\n### 8. Architecture et Modularité\r\n- Utiliser Cobra pour la gestion des commandes CLI\r\n- Implémenter une architecture pipeline pour un traitement efficace des documents\r\n- Créer une couche d'abstraction pour les LLMs pour faciliter l'ajout futur de nouveaux modèles\r\n- Séparer le système de prompts en son propre module pour améliorer la modularité et la réutilisabilité\r\n- Prévoir une phase de prototype pour les composants critiques comme le segmenter et les clients LLM\r\n\r\n### 9. Gestion des Erreurs et Robustesse\r\n- Implémenter une gestion fine des erreurs pour les appels API aux LLMs, y compris la gestion des timeouts et des retries\r\n- Assurer une validation rigoureuse des entrées et des sorties à chaque étape du pipeline\r\n- Gérer de manière appropriée les erreurs spécifiques aux API LLM\r\n\r\n### 10. Tests et Validation\r\n- Implémenter des tests unitaires pour chaque composant\r\n- Créer des tests de bout en bout pour le pipeline complet\r\n- Ajouter des tests de performance et de charge pour valider le comportement avec de grands volumes de données\r\n- Inclure des tests spécifiques pour le mode de débogage et les nouvelles fonctionnalités de journalisation\r\n- Implémenter des tests de charge spécifiques pour vérifier le comportement du système sous des conditions de limite de taux\r\n\r\n### 11. Gestion du contexte\r\n- Implémenter une gestion sophistiquée du contexte entre les segments, en particulier pour l'interaction entre le segmenter et le convertisseur\r\n- Assurer que le contexte fourni aux LLMs est pertinent et ne dépasse pas les limites de tokens spécifiques à chaque modèle\r\n\r\n### 12. Optimisation de la mémoire\r\n- Implémenter des stratégies d'optimisation de la mémoire pour le traitement de très grands documents, en particulier dans le segmenter et le convertisseur\r\n- Utiliser des techniques de streaming et de buffering pour minimiser l'utilisation de la mémoire lors du traitement de grands fichiers\r\n\r\n## Exigences Détaillées\r\n\r\nPour tous les modules :\r\n- Limiter la taille d'un package à maximum 10 méthodes et découper le code logiciel finement\r\n- Optimiser le code pour la performance, particulièrement pour le traitement de grands documents\r\n- Assurer une gestion robuste des erreurs à travers l'application\r\n\r\n### 1. Analyse et Segmentation des Documents\r\n- Développer des modules séparés pour l'analyse de chaque format supporté\r\n- Implémenter un mécanisme de segmentation sophistiqué pour décomposer les grands documents en segments traitables et cohérents\r\n- Assurer une gestion robuste des erreurs pour les documents mal formés ou incomplets\r\n- Préserver la structure du document et le contexte à travers les segments\r\n- Implémenter un système de gestion des métadonnées du document cohérent à travers les différents formats\r\n\r\n### 2. Conversion QuickStatement TSV\r\n- Créer un mappage complet des éléments du document vers QuickStatement/Wikibase\r\n- Développer un moteur de conversion flexible capable de gérer diverses structures de documents\r\n- Implémenter un mécanisme pour lier les segments QuickStatement pour une représentation cohérente\r\n- Optimiser les requêtes aux LLM pour maximiser l'utilisation du contexte tout en respectant les limites de tokens\r\n- Implémenter un système de gestion des erreurs et de reconnexion robuste pour les appels API aux LLM\r\n\r\n### 3. Journalisation et Surveillance\r\n- Développer un module de journalisation centralisé supportant différents niveaux de log\r\n- Implémenter la rotation et l'archivage des logs pour les logs basés sur fichiers\r\n- Ajouter des métriques de performance spécifiques à la gestion documentaire\r\n- Assurer que l'activation du mode debug n'affecte pas significativement les performances en mode normal\r\n\r\n### 4. Client CLI\r\n- Développer une interface CLI conviviale en utilisant Cobra\r\n- Implémenter des commandes pour la conversion de fichiers uniques et le traitement par lots\r\n- Fournir des options pour la sélection du LLM à utiliser et la configuration des paramètres associés\r\n- Implémenter un mode interactif pour des interrogations à la volée d'un document sur la base d'une ontologie\r\n\r\n### 5. Système de Configuration\r\n- Développer un système de configuration basé sur YAML avec support pour les surcharges par ligne de commande\r\n- Inclure des options de réglage des performances pour le traitement parallèle et la gestion de la mémoire\r\n\r\n### 6. Internationalisation\r\n- S'assurer que tout le texte visible par l'utilisateur est en anglais\r\n- Concevoir le système pour supporter de futurs efforts de localisation\r\n- Implémenter un support pour les jeux de caractères internationaux dans le traitement des documents\r\n\r\n### 7. Sécurité et Confidentialité\r\n- Ajouter une option pour le chiffrement des données sensibles dans les logs et les fichiers de sortie\r\n- Implémenter un système basique de gestion des droits d'accès pour les différentes fonctionnalités\r\n\r\n### 8. Performance et Optimisation\r\n- Optimiser le traitement par lots des grands documents pour éviter les problèmes de mémoire\r\n- Implémenter des stratégies d'optimisation pour les appels API aux LLMs afin de réduire les coûts et les temps de traitement\r\n\r\n## Contraintes Techniques\r\n- Développer en langage de programmation Go\r\n- Suivre les meilleures pratiques et les modèles idiomatiques de Go\r\n- Utiliser les goroutines et les canaux pour le traitement concurrent lorsque c'est approprié\r\n- Assurer la compatibilité avec différents LLM et leurs limites de contexte spécifiques\r\n- Une méthode ne peut pas faire plus de 80 lignes\r\n- Un fichier de code source Go d'un package pas plus de 10 méthodes\r\n\r\n## Livrables\r\n1. Dépôt de code source avec des packages Go bien structurés\r\n2. Binaires exécutables pour les principaux systèmes d'exploitation (Windows, macOS, Linux)\r\n3. Suite de tests complète incluant des tests de performance et de stress\r\n4. Documentation utilisateur et technique avec des directives d'optimisation des performances\r\n5. Fichiers de configuration d'exemple\r\n6. README avec un guide de démarrage rapide et des instructions d'utilisation de base\r\n7. Licence GPL3",
    "size": 10649,
    "modTime": "2024-10-21T19:40:14.187428+02:00",
    "path": "expression.besoin.revise.md"
  },
  {
    "name": "planaction.revisite.md",
    "content": "# Plan d'action révisé pour le projet Ontology\r\n\r\n1. Configuration et structure de base\r\n   - Créer la structure du projet\r\n   - Implémenter le système de configuration YAML\r\n   - Mettre en place Cobra pour la gestion des commandes CLI\r\n   - Créer le package d'internationalisation (i18n)\r\n\r\n2. Système de journalisation\r\n   - Développer le module de journalisation avec différents niveaux de log\r\n   - Implémenter le mode de débogage détaillé (--debug)\r\n   - Ajouter le mode silencieux (--silent)\r\n   - Mettre en place la rotation et l'archivage des logs\r\n\r\n3. Gestion des erreurs et robustesse\r\n   - Créer un système centralisé de gestion des erreurs\r\n   - Implémenter la logique de retry avec backoff exponentiel\r\n   - Développer des mécanismes de validation pour les entrées et sorties\r\n\r\n4. Système de prompts\r\n   - Créer la structure PromptTemplate\r\n   - Implémenter les méthodes de formatage des prompts\r\n   - Développer des templates pour l'extraction d'entités et de relations\r\n\r\n5. Intégration LLM\r\n   - Créer une interface commune pour les clients LLM\r\n   - Implémenter les clients pour OpenAI, Claude, et Ollama\r\n   - Développer le système de \"token bucket\" pour la gestion des limites de taux\r\n   - Mettre en place les adaptateurs spécifiques pour chaque API LLM\r\n\r\n6. Analyse et segmentation des documents\r\n   - Implémenter le parsing pour chaque format de document supporté\r\n   - Développer le mécanisme de segmentation avec tiktoken-go\r\n   - Créer le système de gestion des métadonnées\r\n   - Optimiser la gestion de la mémoire pour les grands documents\r\n\r\n7. Conversion QuickStatement\r\n   - Développer le moteur de conversion vers QuickStatement\r\n   - Implémenter le traitement des caractères d'échappement\r\n   - Créer le système de nettoyage et normalisation des entrées\r\n   - Mettre en place le mécanisme de liaison des segments\r\n\r\n8. Gestion du contexte\r\n   - Implémenter la gestion du contexte entre les segments\r\n   - Développer le système d'optimisation du contexte pour les LLMs\r\n\r\n9. Pipeline de traitement principal\r\n   - Intégrer tous les composants développés dans un pipeline cohérent\r\n   - Implémenter le traitement par lots pour les grands documents\r\n   - Développer le système de traitement multi-passes\r\n\r\n10. Optimisation des performances\r\n    - Optimiser les appels API aux LLMs\r\n    - Implémenter des stratégies d'optimisation de la mémoire\r\n    - Utiliser des techniques de streaming et buffering pour les grands fichiers\r\n\r\n11. Fonctionnalités d'export\r\n    - Implémenter l'export en format RDF\r\n    - Développer l'export en format OWL\r\n\r\n12. Interface CLI complète\r\n    - Finaliser l'interface de ligne de commande\r\n    - Implémenter le mode interactif\r\n    - Ajouter toutes les options de configuration via les flags\r\n\r\n13. Sécurité et confidentialité\r\n    - Implémenter le chiffrement des données sensibles\r\n    - Développer le système basique de gestion des droits d'accès\r\n\r\n14. Tests et validation\r\n    - Créer des tests unitaires pour chaque composant\r\n    - Développer des tests d'intégration pour le pipeline complet\r\n    - Implémenter des tests de performance et de charge\r\n    - Créer des tests spécifiques pour le mode debug et les limites de taux\r\n\r\n15. Documentation et finalisation\r\n    - Rédiger la documentation utilisateur et technique\r\n    - Créer le README avec guide de démarrage rapide\r\n    - Préparer les fichiers de configuration d'exemple\r\n    - Générer les binaires pour différents systèmes d'exploitation",
    "size": 3557,
    "modTime": "2024-10-21T19:42:05.013252+02:00",
    "path": "planaction.revisite.md"
  }
]