[
  {
    "name": "expression.besoin.md",
    "content": "# Créateur d'ontologie\n\n## Version\n\nVersion 0.1.0 Brainstorm\n\n## Aperçu du projet\n\nDévelopper un logiciel en Go qui, à partir de divers formats de documents (texte, PDF, Markdown, HTML, DOCX), crée une ontologie au format QuickStatement pour être compatible avec Wikibase. \n\nLe logiciel identifiera et extraira chaque élément d'information du document d'entrée, aussi petit ou apparemment insignifiant soit-il, tout en gérant efficacement les très grands documents.\n\n## Fonctionnalités principales\n\n1. **Support Multi-format d'Entrée**\n   - Accepter les entrées en formats texte, PDF, Markdown, HTML et DOCX\n   - Implémenter des analyseurs robustes pour chaque format supporté\n   - Prise en charge de très grands documents (dépassant 120 000 tokens)\n\n2. **Génération d'une ontologie au format QuickStatement / Wikibase**\n   - Génération multi-passes : Le logiciel passe le nombre de fois indiqué en ligne de commande sur le document pour raffiner l'ontologie. La première passe est la constitution initiale de l'ontologie ; les passes suivantes visent à affiner les éléments. Le nombre est un paramètre de la ligne de commande.\n   - Générer une sortie QuickStatement détaillée utilisant le vocabulaire Wikibase\n   - Assurer une extraction complète des informations des documents d'entrée\n   - Découper les documents larges en segments de maximum 4000 tokens pour ne pas impacter les output contexte des LLMs et gérer la sortie pour respecter la limite de 4 000 tokens par segment\n   - S'assurer que chaque élément de l'ontologie est unitaire entre les segments et que le tout est cohérent. Chaque segment doit avoir la connaissance des éléments ontologiques du segment précédent pour que le LLM puisse s'appuyer dessus.\n   - Lorsque l'on appelle le LLM pour le traitement d'un nouveau segment, on commence par lui donner l'état de l'ontologie.\n   - Il doit être possible de traiter plusieurs documents en utilisant la même ontologie : donc on doit pouvoir donner une ontologie en ligne de commande pour que la première passe ne soit pas nécessaire.\n   - Il faut un mode batch où l'on donne un répertoire et tous les fichiers présents dans le répertoire et les sous-répertoires sont traités. Si une ontologie est donnée en paramètre de ligne de commande, c'est elle qui sera utilisée et qui sera enrichie avec l'ensemble des documents.\n   - Le résultat de l'exécution du logiciel est un fichier ayant l'extension .tsv (tab separated value) qui a le nom de l'ontologie. Le nom de l'ontologie doit être donné en ligne de commande par l'utilisateur. \n   - Ajouter des options d'export en formats RDF et OWL via des paramètres de ligne de commande\n   - Utiliser un comptage précis des tokens compatible avec les modèles GPT pour assurer une segmentation exacte et cohérente des documents.\n\n3. **Architecture Modulaire**\n   - Utiliser Cobra pour la gestion des commandes CLI\n   - Implémenter une architecture pipeline pour un traitement efficace des documents\n   - Le verbe pour créer l'ontologie est \"enrich\" par exemple : ontology enrich \u003cnom ontologie\u003e \u003cnom du fichier ou du répertoire\u003e \u003cflags\u003e\n   - On doit pouvoir supporter plusieurs LLMs, au minimum OpenAI GPT-4, Claude 3.5 Sonnet, Ollama\n  \n4. **Système de Journalisation**\n   - Implémenter un système de journalisation polyvalent avec support pour les niveaux debug, info, warning et error\n   - Exporter les logs vers des fichiers texte et les afficher sur la console pour le serveur et le CLI\n   - Implémenter un mode silencieux (--silent) pour désactiver la sortie console des logs\n   - Implémenter un mode debug (--debug) pour une journalisation très détaillée\n   - Tous les output console doivent passer par le système de journalisation\n   - Chaque étape doit être journalisée\n\n5. **Gestion de la Configuration**\n   - Utiliser YAML pour une configuration centralisée\n   - Permettre des surcharges de paramètres par ligne de commande\n\n6. **Versionnage**\n   - Implémenter un suivi de version commençant à 0.1.0\n\n## Exigences Détaillées\n\nPour tous les modules : \n\n- Limiter la taille d'un package à maximum 10 méthodes et découper le code logiciel finement\n\n### 1. Analyse et Segmentation des Documents\n\n- Développer des modules séparés pour l'analyse de chaque format supporté (texte, PDF, Markdown, HTML, DOCX)\n- Implémenter un mécanisme de segmentation pour décomposer les grands documents en segments traitables\n- Assurer une gestion robuste des erreurs pour les documents mal formés ou incomplets via le système de journalisation \n- Implémenter une interface commune pour tous les analyseurs afin de standardiser le processus d'extraction\n- Préserver la structure du document et le contexte à travers les segments\n- Implémenter un système de gestion des métadonnées du document (auteur, date de création, version, etc.). Les métadonnées doivent pouvoir être données en paramètre de ligne de commande.\n\n### 2. Conversion QuickStatement TSV\n\n- Créer un mappage complet des éléments du document vers QuickStatement/Wikibase\n- Vérifier qu'il n'y a pas de concepts qui sont similaires ou il y ait peu de nuances ou alors que la nuance puisse être un paramètre d'un concept.\n- Développer un moteur de conversion flexible capable de gérer diverses structures de documents\n- S'assurer que chaque segment de sortie QuickStatement TSV respecte la limite de 4 000 tokens\n- Implémenter un mécanisme pour lier les segments QuickStatement pour une représentation cohérente\n- Intégrer des clients API pour différents LLM externes :\n  - Claude (Anthropic)\n  - GPT (OpenAI)\n  - Ollama (pour les modèles locaux)\n- Implémenter une interface commune `TranslationClient` pour tous les clients LLM\n- Permettre la sélection du LLM à utiliser via la configuration ou les options de ligne de commande\n- Ajouter une option `-i` pour enrichir les instructions de conversion envoyées au LLM\n- Optimiser les requêtes aux LLM pour maximiser l'utilisation du contexte tout en respectant les limites de tokens\n- Implémenter un système de gestion des erreurs et de reconnexion pour les appels API aux LLM : si un LLM ne répond pas dans le timeout, réessayer au bout de 20s en lui laissant plus de temps pour répondre par tranche de 20 secondes ; maximum 5 essais avant échec.\n\n### 3. Journalisation et Surveillance\n\n- Développer un module de journalisation centralisé supportant la sortie vers fichiers et console\n- Implémenter la rotation et l'archivage des logs pour les logs basés sur fichiers\n- Intégrer la journalisation dans toute l'application pour un suivi complet des opérations\n- Implémenter la surveillance et le reporting des performances pour les tâches de traitement à grande échelle\n- Ajouter des métriques de performance spécifiques à la gestion documentaire (temps de traitement par page, taux d'extraction, etc.)\n- La dernière ligne du terminal doit afficher la progression de l'analyse d'un document et indiquer la progression du nombre total de documents. La console s'affiche au-dessus et n'empiète pas.\n- Afficher le temps global écoulé depuis le début du traitement et le temps passé sur chaque chunk (segment) traité\n\n### 4. Client CLI\n\n- Développer une interface CLI conviviale en utilisant Cobra\n- Implémenter des commandes pour :\n  - La conversion de fichiers uniques\n  - Le traitement par lots de plusieurs fichiers\n  - La gestion de la configuration\n  - Le contrôle du niveau de log\n  - Le suivi de la progression pour le traitement de grands documents\n  - La sélection du LLM à utiliser\n- Ajouter une option `-i` pour spécifier des instructions supplémentaires pour la conversion\n- Fournir une aide détaillée et des informations d'utilisation pour chaque commande\n- Implémenter un mode interactif pour des interrogations à la volée d'un document sur la base d'une ontologie\n\n### 5. Système de Configuration\n\n- Développer un système de configuration basé sur YAML\n- Implémenter le chargement de fichiers de configuration avec des surcharges spécifiques à l'environnement\n- Permettre des surcharges de paramètres de configuration par ligne de commande\n- Inclure des options de réglage des performances pour le traitement parallèle et la gestion de la mémoire\n\n### 6. Documentation\n\n- Créer une documentation utilisateur détaillée incluant des guides d'installation, de configuration et d'utilisation\n- Développer une documentation technique pour l'utilisation et l'intégration de l'API\n- Fournir des exemples et des meilleures pratiques pour une utilisation efficace de l'outil\n- Inclure des directives pour l'optimisation des performances avec de grands documents\n\n### 7. Internationalisation\n\n- S'assurer que tout le texte visible par l'utilisateur est en anglais\n- Concevoir le système pour supporter de futurs efforts de localisation\n- Implémenter un support pour les jeux de caractères internationaux dans le traitement des documents\n- Documenter le code produit\n\n### 8. Sécurité et Confidentialité\n\n- Ajouter une option pour le chiffrement des données sensibles dans les logs et les fichiers de sortie\n- Implémenter un système basique de gestion des droits d'accès pour les différentes fonctionnalités\n\n### 9. Interopérabilité\n\n- Concevoir une API simple ou des hooks pour faciliter l'intégration future avec d'autres outils ou workflows\n\n## Contraintes Techniques\n\n- Développer en langage de programmation Go\n- S'assurer qu'aucun fichier ne dépasse 3000 tokens\n- Suivre les meilleures pratiques et les modèles idiomatiques de Go\n- Utiliser les goroutines et les canaux pour le traitement concurrent lorsque c'est approprié\n- Assurer la compatibilité avec différents LLM et leurs limites de contexte spécifiques (par exemple, limiter à environ 200 tokens par batch pour Ollama avec des modèles comme llama3.2)\n- Une méthode ne peut pas faire plus de 80 lignes\n- Un fichier de code source Go d'un package pas plus de 10 méthodes\n\n## Livrables\n\n1. Dépôt de code source avec des packages Go bien structurés\n2. Binaires exécutables pour les principaux systèmes d'exploitation (Windows, macOS, Linux)\n3. Suite de tests complète incluant des tests de performance et de stress\n4. Documentation utilisateur et technique avec des directives d'optimisation des performances\n5. Fichiers de configuration d'exemple\n6. README avec un guide de démarrage rapide et des instructions d'utilisation de base\n7. Licence GPL3",
    "size": 10484,
    "modTime": "2024-10-20T12:37:04.5069168+02:00",
    "path": "expression.besoin.md"
  },
  {
    "name": "planaction.md",
    "content": "# Plan d'action détaillé pour le développement d'Ontology\r\n\r\n## Directives générales de développement\r\n\r\nCes directives s'appliquent à toutes les étapes du développement et doivent être suivies par tous les exécutants :\r\n\r\n1. Langage et version : Développer exclusivement en Go, en utilisant la dernière version stable.\r\n\r\n2. Journalisation : \r\n   - Toutes les actions et événements significatifs doivent être logués en utilisant le package `internal/logger`.\r\n   - Implémenter les niveaux de log : debug, info, warning, et error.\r\n   - Supporter l'export des logs vers des fichiers texte et l'affichage sur la console.\r\n   - Implémenter un mode silencieux (--silent) et un mode debug (--debug).\r\n\r\n3. Taille et structure du code :\r\n   - Aucun fichier de code source ne doit dépasser 3000 tokens.\r\n   - Chaque package ne doit pas contenir plus de 10 méthodes exportées.\r\n   - La taille totale d'un package ne doit pas dépasser 4000 tokens.\r\n   - Aucune méthode ne doit dépasser 80 lignes de code.\r\n\r\n4. Architecture :\r\n   - Utiliser Cobra pour la gestion des commandes CLI.\r\n   - Implémenter une architecture pipeline pour un traitement efficace des documents.\r\n\r\n5. Gestion des documents :\r\n   - Supporter les formats : texte, PDF, Markdown, HTML, DOCX.\r\n   - Implémenter la segmentation des documents en chunks de maximum 4000 tokens.\r\n   - Gérer efficacement les très grands documents (dépassant 120 000 tokens).\r\n   - Préserver la structure et le contexte du document à travers les segments.\r\n\r\n6. Intégration LLM :\r\n   - Supporter au minimum OpenAI GPT-4, Claude 3.5 Sonnet, et Ollama.\r\n   - Lors de l'interaction avec les LLM, respecter les limites de contexte spécifiques à chaque modèle.\r\n   - Pour Ollama avec des modèles comme llama3.2, limiter à environ 200 tokens par batch.\r\n   - Implémenter un système de gestion des erreurs et de reconnexion pour les appels API aux LLM.\r\n\r\n7. Gestion de l'ontologie :\r\n   - Générer une sortie au format QuickStatement TSV pour Wikibase.\r\n   - Implémenter des options d'export en formats RDF et OWL.\r\n   - Assurer la cohérence de l'ontologie entre les segments et les passes multiples.\r\n\r\n8. Configuration :\r\n   - Utiliser YAML pour la configuration centralisée.\r\n   - Permettre des surcharges de paramètres par ligne de commande.\r\n   - Toutes les valeurs configurables doivent être définies dans le package `internal/config`.\r\n\r\n9. Documentation :\r\n   - Chaque fonction, méthode et type exporté doit avoir un commentaire de documentation conforme aux standards GoDoc.\r\n   - Créer une documentation utilisateur détaillée incluant des guides d'installation, de configuration et d'utilisation.\r\n\r\n10. Tests et qualité :\r\n    - Chaque package doit avoir une couverture de tests d'au moins 80%.\r\n    - Inclure des tests unitaires, des tests d'intégration, et des tests de performance.\r\n\r\n11. Gestion des erreurs :\r\n    - Toutes les erreurs doivent être gérées et propagées de manière appropriée.\r\n    - Utiliser des error wrapping lorsque c'est pertinent.\r\n\r\n12. Concurrence :\r\n    - Utiliser les goroutines et les canaux de manière appropriée pour le traitement parallèle.\r\n    - Veiller à éviter les race conditions.\r\n\r\n13. Internationalisation :\r\n    - Tous les messages visibles par l'utilisateur doivent être en anglais.\r\n    - Définir les messages dans le package `internal/i18n`.\r\n    - Concevoir le système pour supporter de futurs efforts de localisation.\r\n\r\n14. Performance :\r\n    - Optimiser le code pour la performance, en particulier pour le traitement de grands documents.\r\n    - Utiliser le profilage Go pour identifier les goulots d'étranglement.\r\n\r\n15. Sécurité :\r\n    - Implémenter des options pour le chiffrement des données sensibles dans les logs et les fichiers de sortie.\r\n    - Mettre en place un système basique de gestion des droits d'accès.\r\n\r\n16. Versionnage :\r\n    - Implémenter un suivi de version commençant à 0.1.0.\r\n    - Utiliser les tags Git pour le versioning.\r\n\r\n17. Compilation et distribution :\r\n    - Fournir des binaires exécutables pour Windows, macOS, et Linux.\r\n    - Utiliser un Makefile pour automatiser les tâches de build, test, et release.\r\n\r\n## Étapes de développement\r\n\r\n### 1. Configuration initiale du projet\r\n\r\nInstruction : \"Créez la structure exacte suivante pour le projet Go :\r\n\r\n/Ontology /cmd /ontology main.go /internal /parser /segmenter /converter /llm /logger /config /pipeline /i18n /pkg go.mod Makefile README.md\r\n\r\nInitialisez le module Go avec 'go mod init github.com/chrlesur/Ontology'. Dans le fichier main.go, importez uniquement le package 'cmd/ontology' et appelez la fonction Run().\"\r\n\r\n### 2. Mise en place de Cobra pour le CLI\r\n\r\nInstruction : \"Dans cmd/ontology/root.go, créez la commande racine 'ontology' avec Cobra. Définissez les flags globaux suivants : --config (string), --debug (bool), --silent (bool). Dans cmd/ontology/enrich.go, créez la sous-commande 'enrich' avec les flags : --input (string), --output (string), --format (string), --llm (string), --llm-model (string), --passes (int), --rdf (bool), --owl (bool), --recursive (bool). La fonction Run() de 'enrich' doit appeler une fonction du package 'pipeline' nommée ExecutePipeline().\"\r\n\r\n### 3. Développement des analyseurs de documents\r\n\r\nInstruction : \"Dans internal/parser, créez une interface Parser avec une seule méthode Parse(path string) ([]byte, error). Implémentez cette interface pour chaque format (text.go, pdf.go, markdown.go, html.go, docx.go). Chaque implémentation doit utiliser uniquement la bibliothèque standard Go ou une bibliothèque tierce spécifiée (par exemple, 'github.com/unidoc/unipdf/v3' pour PDF). Ajoutez une fonction GetMetadata() map[string]string à chaque parser pour extraire les métadonnées du document. Créez une fonction ParseDirectory(path string, recursive bool) ([][]byte, error) qui parcourt un répertoire (et ses sous-répertoires si recursive est true) et parse tous les fichiers supportés.\"\r\n\r\n### 4. Implémentation de la segmentation des documents\r\n\r\nInstruction : \"Dans internal/segmenter/segmenter.go, créez une fonction Segment(content []byte, maxTokens int) ([][]byte, error) qui divise le contenu en segments de maxTokens. Utilisez un algorithme de tokenization précis compatible avec les modèles GPT (par exemple, en utilisant la bibliothèque 'github.com/pkoukk/tiktoken-go') pour le comptage des tokens. Assurez-vous que chaque segment se termine par une phrase complète. Implémentez une fonction GetContext(segments [][]byte, currentIndex int) string qui retourne le contexte des segments précédents. Créez une fonction CountTokens(content []byte) int qui retourne le nombre exact de tokens dans un contenu donné. Ajoutez une fonction pour calibrer le comptage des tokens en fonction du modèle LLM spécifique utilisé.\"\r\n\r\n### 5. Création du moteur de conversion QuickStatement\r\n\r\nInstruction : \"Dans internal/converter/quickstatement.go, implémentez une fonction Convert(segment []byte, context string, ontology string) (string, error) qui transforme un segment en format QuickStatement TSV. Utilisez une table de mapping prédéfinie pour les éléments Wikibase. La sortie doit être une chaîne formatée en TSV. Implémentez également des fonctions ConvertToRDF(quickstatement string) (string, error) et ConvertToOWL(quickstatement string) (string, error) pour les exports additionnels.\"\r\n\r\n### 6. Intégration des clients LLM\r\n\r\nInstruction : \"Dans internal/llm, créez une interface Client avec une méthode Translate(prompt string, context string) (string, error). Implémentez cette interface pour chaque LLM supporté (openai.go, claude.go, ollama.go). Pour chaque implémentation, ajoutez un paramètre 'model' à la méthode de création du client (par exemple, NewOpenAIClient(apiKey string, model string) *OpenAIClient). Supportez au minimum les modèles suivants :\r\n- OpenAI : 'gpt-3.5-turbo', 'gpt-4'\r\n- Claude : 'claude-3-opus-20240229', 'claude-3-sonnet-20240229'\r\n- Ollama : 'llama2', 'llama2:13b', 'llama2:70b'\r\nUtilisez les SDK officiels ou des requêtes HTTP simples pour l'interaction avec les API. Implémentez un système de retry avec backoff exponentiel en cas d'erreur, avec un maximum de 5 tentatives.\"\r\n\r\n### 7. Développement du système de journalisation\r\n\r\nInstruction : \"Dans internal/logger/logger.go, implémentez des fonctions Debug(), Info(), Warning(), et Error() qui écrivent dans un fichier de log et sur la console. Utilisez le package 'log' de la bibliothèque standard. Implémentez la rotation des logs avec un fichier de 10MB maximum. Ajoutez une fonction UpdateProgress(current, total int) qui met à jour la dernière ligne de la console avec la progression actuelle.\"\r\n\r\n### 8. Implémentation du système de configuration\r\n\r\nInstruction : \"Dans internal/config/config.go, créez une structure Config qui correspond exactement aux flags CLI, y compris les nouveaux flags --llm-model et --recursive. Implémentez une fonction LoadConfig(path string) (Config, error) qui charge un fichier YAML. Utilisez le package 'gopkg.in/yaml.v2' pour le parsing YAML. Ajoutez une fonction ValidateConfig(config Config) error pour vérifier la validité de la configuration, y compris la validation du modèle LLM spécifié.\"\r\n\r\n### 9. Développement du pipeline de traitement principal\r\n\r\nInstruction : \"Dans internal/pipeline/pipeline.go, créez une fonction ExecutePipeline(config Config) error qui orchestre le flux de travail complet. Cette fonction doit gérer le traitement d'un seul fichier ou d'un répertoire entier selon la valeur de config.Input. Si c'est un répertoire, utilisez la fonction ParseDirectory du package parser. Pour chaque document, appelez séquentiellement les fonctions des autres packages dans cet ordre : parser, segmenter, converter, llm. Utilisez un WaitGroup pour le traitement parallèle des segments. Implémentez une boucle pour les passes multiples d'enrichissement de l'ontologie. Assurez-vous que le LLM client est créé avec le modèle spécifié dans config.LLMModel.\"\r\n\r\n### 10. Implémentation des fonctionnalités de sécurité\r\n\r\nInstruction : \"Dans internal/logger/logger.go, ajoutez une fonction SetEncryption(key string) qui active le chiffrement AES pour les logs. Dans internal/config/config.go, ajoutez un champ 'AccessLevel' à la structure Config et implémentez une fonction CheckAccess(level string) bool.\"\r\n\r\n### 11. Développement des tests unitaires et d'intégration\r\n\r\nInstruction : \"Pour chaque package, créez un fichier *_test.go avec des tests unitaires pour chaque fonction publique. Utilisez le package 'testing' de la bibliothèque standard. Dans le dossier racine, créez un fichier integration_test.go avec un test qui exécute le pipeline complet sur un petit document d'exemple. Ajoutez des benchmarks pour mesurer les performances de chaque composant.\"\r\n\r\n### 12. Création de la documentation\r\n\r\nInstruction : \"Ajoutez des commentaires de documentation Go à toutes les fonctions et structures publiques. Dans le README.md, incluez les sections suivantes : Installation, Configuration, Utilisation, Exemples, Contribution, Licence. Utilisez des blocs de code Markdown pour les exemples de commandes CLI. Générez la documentation GoDoc pour tous les packages.\"\r\n\r\n### 13. Mise en place du système de build et de release\r\n\r\nInstruction : \"Dans le Makefile, définissez les cibles : build, test, lint, clean. La cible build doit compiler des binaires pour Windows, macOS et Linux en utilisant la commande 'go build' avec les flags appropriés pour chaque OS. Utilisez les tags Git pour le versioning, en commençant par v0.1.0. Ajoutez une cible 'release' qui crée une archive ZIP contenant les binaires et la documentation.\"\r\n\r\n### 14. Internationalisation\r\n\r\nInstruction : \"Dans internal/i18n/messages.go, définissez toutes les chaînes de caractères visibles par l'utilisateur en anglais. Utilisez ces constantes partout dans le code au lieu de chaînes de caractères codées en dur.\"\r\n\r\n### 15. Finalisation et préparation à la release\r\n\r\nInstruction : \"Exécutez 'go fmt' et 'go vet' sur tout le code. Vérifiez que chaque fichier ne dépasse pas 3000 tokens, chaque fonction 80 lignes, et chaque package 10 méthodes publiques. Créez un tag Git v0.1.0 et poussez-le sur le dépôt distant. Générez un changelog détaillé des fonctionnalités implémentées.\"\r\n\r\n## Vérification finale\r\n\r\nAprès avoir relu attentivement l'expression de besoins et ce plan d'action, je confirme que tous les éléments essentiels ont été couverts. Le plan d'action respecte les exigences initiales, intègre les modifications discutées (comme le traitement récursif des répertoires et la sélection des modèles LLM spécifiques), et couvre tous les aspects techniques et fonctionnels demandés.\r\n\r\nLes points clés tels que la gestion des formats de documents, la segmentation, l'interaction avec différents LLM, la génération d'ontologies au format QuickStatement, les exports RDF et OWL, la gestion des métadonnées, et les contraintes techniques (taille des fichiers, structure des packages, etc.) sont tous adressés.\r\n\r\nLe plan respecte également les exigences en termes de journalisation, configuration, tests, documentation, et internationalisation. Les aspects de sécurité, de performance et de distribution sont également pris en compte.\r\n\r\nSi des ajustements ou des précisions supplémentaires sont nécessaires, n'hésitez pas à le mentionner.",
    "size": 13542,
    "modTime": "2024-10-20T12:37:51.7756116+02:00",
    "path": "planaction.md"
  },
  {
    "name": "step1.md",
    "content": "En tant que développeur Go expérimenté, votre tâche est de mettre en place Cobra pour le CLI du projet Ontology. Voici vos directives :\r\n\r\nDirectives générales (à suivre pour toutes les étapes du projet) :\r\n1. Utilisez exclusivement Go dans sa dernière version stable.\r\n2. Assurez-vous qu'aucun fichier de code source ne dépasse 3000 tokens.\r\n3. Limitez chaque package à un maximum de 10 méthodes exportées.\r\n4. Aucune méthode ne doit dépasser 80 lignes de code.\r\n5. Suivez les meilleures pratiques et les modèles idiomatiques de Go.\r\n6. Tous les messages visibles par l'utilisateur doivent être en anglais.\r\n7. Chaque fonction, méthode et type exporté doit avoir un commentaire de documentation conforme aux standards GoDoc.\r\n\r\nInstructions spécifiques pour l'étape 2 - Mise en place de Cobra pour le CLI :\r\n\r\n1. Installez Cobra en utilisant la commande : go get -u github.com/spf13/cobra@latest\r\n\r\n2. Dans cmd/ontology/root.go, créez la commande racine 'ontology' avec Cobra. Définissez les flags globaux suivants :\r\n   - --config (string)\r\n   - --debug (bool)\r\n   - --silent (bool)\r\n\r\n3. Dans cmd/ontology/enrich.go, créez la sous-commande 'enrich' avec les flags suivants :\r\n   - --input (string)\r\n   - --output (string)\r\n   - --format (string)\r\n   - --llm (string)\r\n   - --llm-model (string)\r\n   - --passes (int)\r\n   - --rdf (bool)\r\n   - --owl (bool)\r\n   - --recursive (bool)\r\n\r\n4. La fonction Run() de 'enrich' doit appeler une fonction du package 'pipeline' nommée ExecutePipeline(). Pour l'instant, cette fonction peut être un placeholder qui affiche simplement un message indiquant que le pipeline sera exécuté ici.\r\n\r\n5. Assurez-vous que la commande racine et la sous-commande sont correctement liées.\r\n\r\n6. Mettez à jour la fonction Run() dans cmd/ontology/root.go pour exécuter la commande racine.\r\n\r\n7. Ajoutez des commentaires de documentation appropriés pour chaque commande et flag.\r\n\r\n8. Assurez-vous que l'aide générée par Cobra (accessible via --help) est claire et informative.\r\n\r\nVeillez à ce que le code soit bien structuré, lisible, et conforme aux meilleures pratiques de Go et de Cobra. Une fois terminé, vérifiez que les commandes et les flags fonctionnent correctement en exécutant le binaire avec différentes options.",
    "size": 2286,
    "modTime": "2024-10-20T12:28:15.459367+02:00",
    "path": "step1.md"
  },
  {
    "name": "step2.md",
    "content": "En tant que développeur Go expérimenté, votre tâche est de mettre en place Cobra pour le CLI du projet Ontology. Voici vos directives :\r\n\r\nDirectives générales (à suivre pour toutes les étapes du projet) :\r\n1. Utilisez exclusivement Go dans sa dernière version stable.\r\n2. Assurez-vous qu'aucun fichier de code source ne dépasse 3000 tokens.\r\n3. Limitez chaque package à un maximum de 10 méthodes exportées.\r\n4. Aucune méthode ne doit dépasser 80 lignes de code.\r\n5. Suivez les meilleures pratiques et les modèles idiomatiques de Go.\r\n6. Tous les messages visibles par l'utilisateur doivent être en anglais.\r\n7. Chaque fonction, méthode et type exporté doit avoir un commentaire de documentation conforme aux standards GoDoc.\r\n8. Utilisez le package 'internal/logger' pour toute journalisation. Implémentez les niveaux de log : debug, info, warning, et error.\r\n9. Toutes les valeurs configurables doivent être définies dans le package 'internal/config'.\r\n10. Gérez toutes les erreurs de manière appropriée, en utilisant error wrapping lorsque c'est pertinent.\r\n11. Pour les messages utilisateur, utilisez les constantes définies dans le package 'internal/i18n'.\r\n12. Assurez-vous que le code est prêt pour de futurs efforts de localisation.\r\n13. Optimisez le code pour la performance, particulièrement pour le traitement de grands documents.\r\n14. Implémentez des tests unitaires pour chaque nouvelle fonction ou méthode.\r\n15. Veillez à ce que le code soit sécurisé, en particulier lors du traitement des entrées utilisateur.\r\n\r\nInstructions spécifiques pour l'étape 2 - Mise en place de Cobra pour le CLI :\r\n\r\n1. Installez Cobra en utilisant la commande : go get -u github.com/spf13/cobra@latest\r\n\r\n2. Dans cmd/ontology/root.go, créez la commande racine 'ontology' avec Cobra. Définissez les flags globaux suivants :\r\n   - --config (string)\r\n   - --debug (bool)\r\n   - --silent (bool)\r\n\r\n3. Dans cmd/ontology/enrich.go, créez la sous-commande 'enrich' avec les flags suivants :\r\n   - --input (string)\r\n   - --output (string)\r\n   - --format (string)\r\n   - --llm (string)\r\n   - --llm-model (string)\r\n   - --passes (int)\r\n   - --rdf (bool)\r\n   - --owl (bool)\r\n   - --recursive (bool)\r\n\r\n4. La fonction Run() de 'enrich' doit appeler une fonction du package 'pipeline' nommée ExecutePipeline(). Pour l'instant, cette fonction peut être un placeholder qui affiche simplement un message indiquant que le pipeline sera exécuté ici.\r\n\r\n5. Assurez-vous que la commande racine et la sous-commande sont correctement liées.\r\n\r\n6. Mettez à jour la fonction Run() dans cmd/ontology/root.go pour exécuter la commande racine.\r\n\r\n7. Ajoutez des commentaires de documentation appropriés pour chaque commande et flag.\r\n\r\n8. Assurez-vous que l'aide générée par Cobra (accessible via --help) est claire et informative.\r\n\r\n9. Implémentez une gestion basique des erreurs pour les entrées utilisateur invalides.\r\n\r\n10. Utilisez le package 'internal/logger' pour journaliser les actions importantes (par exemple, le démarrage de la commande, la validation des flags).\r\n\r\n11. Définissez les messages utilisateur dans 'internal/i18n' et utilisez-les dans vos commandes.\r\n\r\n12. Créez des tests unitaires pour vos commandes dans des fichiers *_test.go appropriés.\r\n\r\nVeillez à ce que le code soit bien structuré, lisible, et conforme aux meilleures pratiques de Go et de Cobra. Une fois terminé, vérifiez que les commandes et les flags fonctionnent correctement en exécutant le binaire avec différentes options.",
    "size": 3543,
    "modTime": "2024-10-20T12:28:27.5415164+02:00",
    "path": "step2.md"
  },
  {
    "name": "step3.md",
    "content": "En tant que développeur Go expérimenté, votre tâche est de développer les analyseurs de documents pour le projet Ontology. Voici vos directives :\r\n\r\nDirectives générales (à suivre pour toutes les étapes du projet) :\r\n1. Utilisez exclusivement Go dans sa dernière version stable.\r\n2. Assurez-vous qu'aucun fichier de code source ne dépasse 3000 tokens.\r\n3. Limitez chaque package à un maximum de 10 méthodes exportées.\r\n4. Aucune méthode ne doit dépasser 80 lignes de code.\r\n5. Suivez les meilleures pratiques et les modèles idiomatiques de Go.\r\n6. Tous les messages visibles par l'utilisateur doivent être en anglais.\r\n7. Chaque fonction, méthode et type exporté doit avoir un commentaire de documentation conforme aux standards GoDoc.\r\n8. Utilisez le package 'internal/logger' pour toute journalisation. Implémentez les niveaux de log : debug, info, warning, et error.\r\n9. Toutes les valeurs configurables doivent être définies dans le package 'internal/config'.\r\n10. Gérez toutes les erreurs de manière appropriée, en utilisant error wrapping lorsque c'est pertinent.\r\n11. Pour les messages utilisateur, utilisez les constantes définies dans le package 'internal/i18n'.\r\n12. Assurez-vous que le code est prêt pour de futurs efforts de localisation.\r\n13. Optimisez le code pour la performance, particulièrement pour le traitement de grands documents.\r\n14. Implémentez des tests unitaires pour chaque nouvelle fonction ou méthode.\r\n15. Veillez à ce que le code soit sécurisé, en particulier lors du traitement des entrées utilisateur.\r\n\r\nInstructions spécifiques pour l'étape 3 - Développement des analyseurs de documents :\r\n\r\n1. Dans internal/parser, créez une interface Parser avec une seule méthode Parse(path string) ([]byte, error).\r\n\r\n2. Implémentez cette interface pour chaque format supporté dans des fichiers séparés :\r\n   - text.go\r\n   - pdf.go\r\n   - markdown.go\r\n   - html.go\r\n   - docx.go\r\n\r\n3. Pour chaque implémentation, utilisez uniquement la bibliothèque standard Go ou une bibliothèque tierce spécifiée. Pour PDF, utilisez 'github.com/unidoc/unipdf/v3'.\r\n\r\n4. Ajoutez une fonction GetMetadata() map[string]string à chaque parser pour extraire les métadonnées du document.\r\n\r\n5. Créez une fonction ParseDirectory(path string, recursive bool) ([][]byte, error) qui parcourt un répertoire (et ses sous-répertoires si recursive est true) et parse tous les fichiers supportés.\r\n\r\n6. Implémentez une gestion robuste des erreurs pour les documents mal formés ou incomplets, en utilisant le système de journalisation.\r\n\r\n7. Optimisez les parsers pour gérer efficacement les très grands documents (dépassant 120 000 tokens).\r\n\r\n8. Assurez-vous que chaque parser préserve la structure du document autant que possible.\r\n\r\n9. Implémentez des tests unitaires pour chaque parser et la fonction ParseDirectory.\r\n\r\n10. Utilisez des goroutines et des canaux de manière appropriée pour le traitement parallèle dans ParseDirectory, si cela peut améliorer les performances.\r\n\r\n11. Ajoutez des logs appropriés à chaque étape du parsing, en utilisant les niveaux de log appropriés.\r\n\r\n12. Créez une factory function GetParser(format string) (Parser, error) qui retourne le parser approprié basé sur le format spécifié.\r\n\r\n13. Assurez-vous que tous les messages d'erreur et les logs sont définis dans internal/i18n et utilisés de manière cohérente.\r\n\r\n14. Documentez chaque fonction et méthode avec des commentaires GoDoc détaillés, y compris des exemples d'utilisation si nécessaire.\r\n\r\nVeillez à ce que le code soit bien structuré, performant et robuste. Une fois terminé, assurez-vous que tous les parsers fonctionnent correctement avec différents types et tailles de documents.",
    "size": 3764,
    "modTime": "2024-10-20T12:28:41.1053785+02:00",
    "path": "step3.md"
  },
  {
    "name": "step4.md",
    "content": "En tant que développeur Go expérimenté, votre tâche est d'implémenter la segmentation des documents pour le projet Ontology. Voici vos directives :\r\n\r\nDirectives générales (à suivre pour toutes les étapes du projet) :\r\n1. Utilisez exclusivement Go dans sa dernière version stable.\r\n2. Assurez-vous qu'aucun fichier de code source ne dépasse 3000 tokens.\r\n3. Limitez chaque package à un maximum de 10 méthodes exportées.\r\n4. Aucune méthode ne doit dépasser 80 lignes de code.\r\n5. Suivez les meilleures pratiques et les modèles idiomatiques de Go.\r\n6. Tous les messages visibles par l'utilisateur doivent être en anglais.\r\n7. Chaque fonction, méthode et type exporté doit avoir un commentaire de documentation conforme aux standards GoDoc.\r\n8. Utilisez le package 'internal/logger' pour toute journalisation. Implémentez les niveaux de log : debug, info, warning, et error.\r\n9. Toutes les valeurs configurables doivent être définies dans le package 'internal/config'.\r\n10. Gérez toutes les erreurs de manière appropriée, en utilisant error wrapping lorsque c'est pertinent.\r\n11. Pour les messages utilisateur, utilisez les constantes définies dans le package 'internal/i18n'.\r\n12. Assurez-vous que le code est prêt pour de futurs efforts de localisation.\r\n13. Optimisez le code pour la performance, particulièrement pour le traitement de grands documents.\r\n14. Implémentez des tests unitaires pour chaque nouvelle fonction ou méthode.\r\n15. Veillez à ce que le code soit sécurisé, en particulier lors du traitement des entrées utilisateur.\r\n\r\nEn tant que développeur Go expérimenté, votre tâche est d'implémenter la segmentation des documents pour le projet Ontology. Voici vos directives :\r\n\r\nInstructions spécifiques pour l'étape 4 - Implémentation de la segmentation des documents :\r\n\r\n1. Dans internal/segmenter/segmenter.go, créez une fonction Segment(content []byte, maxTokens int) ([][]byte, error) qui divise le contenu en segments de maxTokens.\r\n\r\n2. Implémentez un comptage précis des tokens en utilisant l'algorithme de tokenization GPT. Vous pouvez utiliser une bibliothèque existante comme \"github.com/pkoukk/tiktoken-go\" pour une tokenization précise compatible avec les modèles GPT.\r\n\r\n3. Assurez-vous que chaque segment se termine par une phrase complète. Ne coupez jamais au milieu d'une phrase.\r\n\r\n4. Implémentez une fonction GetContext(segments [][]byte, currentIndex int) string qui retourne le contexte des segments précédents. Cette fonction devrait retourner un résumé ou les dernières phrases des segments précédents, sans dépasser une limite de tokens définie (par exemple, 500 tokens).\r\n\r\n5. Créez une fonction CountTokens(content []byte) int qui retourne le nombre exact de tokens dans un contenu donné, en utilisant l'algorithme de tokenization GPT.\r\n\r\n6. Implémentez une gestion efficace de la mémoire pour traiter de très grands documents (dépassant 120 000 tokens).\r\n\r\n7. Ajoutez des logs appropriés à chaque étape de la segmentation, en utilisant les niveaux de log appropriés.\r\n\r\n8. Créez des tests unitaires pour toutes les fonctions, y compris des cas de test pour des documents de différentes tailles et structures, en vérifiant la précision du comptage des tokens.\r\n\r\n9. Optimisez les performances en utilisant des techniques comme le buffering et le streaming pour les grands documents, tout en maintenant la précision du comptage des tokens.\r\n\r\n10. Assurez-vous que la segmentation préserve les métadonnées importantes du document original (par exemple, les en-têtes de section).\r\n\r\n11. Implémentez une fonction MergeSegments(segments [][]byte) []byte pour reconstituer le document original si nécessaire.\r\n\r\n12. Documentez chaque fonction avec des commentaires GoDoc détaillés, y compris des exemples d'utilisation et des explications sur la méthode de tokenization utilisée.\r\n\r\n13. Créez une structure de configuration dans internal/config pour stocker les paramètres de segmentation (comme maxTokens, contextSize, etc.).\r\n\r\n14. Assurez-vous que tous les messages d'erreur et les logs sont définis dans internal/i18n et utilisés de manière cohérente.\r\n\r\n15. Implémentez une gestion des erreurs robuste, en particulier pour les cas où le document ne peut pas être segmenté correctement.\r\n\r\n16. Ajoutez une fonction pour calibrer le comptage des tokens en fonction du modèle LLM spécifique utilisé (par exemple, GPT-3.5, GPT-4, etc.), car différents modèles peuvent avoir des méthodes de tokenization légèrement différentes.\r\n\r\nVeillez à ce que le code soit efficace, bien structuré et capable de gérer une variété de formats de documents et de tailles, tout en assurant un comptage précis des tokens. Une fois terminé, testez rigoureusement la segmentation avec différents types de documents pour assurer sa fiabilité, sa précision et sa performance.",
    "size": 4901,
    "modTime": "2024-10-20T12:35:58.2436626+02:00",
    "path": "step4.md"
  }
]